{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SaaS LiteLLM","text":"<p>A production-ready platform built on top of LiteLLM with job-based cost tracking for multi-tenant SaaS applications. Build your LLM-powered SaaS without exposing infrastructure complexity to your customers.</p>"},{"location":"#what-is-saas-litellm","title":"What is SaaS LiteLLM?","text":"<p>SaaS LiteLLM is a complete SaaS wrapper built on top of LiteLLM that provides:</p> <p>Built on LiteLLM</p> <p>SaaS LiteLLM leverages LiteLLM as its foundation for unified LLM API access. LiteLLM provides the core routing to 100+ LLM providers (OpenAI, Anthropic, Google, Azure, etc.), while SaaS LiteLLM adds the job-based cost tracking, multi-tenancy, and SaaS-ready features on top.</p> <p>Core Features:</p> <ul> <li>Job-Based Cost Tracking - Group multiple LLM calls into business operations and track true costs</li> <li>Multi-Tenant Architecture - Isolated teams with independent budgets and access controls</li> <li>Hidden Complexity - Teams interact with a clean API, never seeing models, pricing, or infrastructure</li> <li>Flexible Pricing - Set your own pricing strategy while tracking actual provider costs</li> <li>Production-Ready - Deploy to Railway, includes PostgreSQL, Redis, rate limiting, and monitoring</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#saas-ready-architecture","title":"\ud83c\udfaf SaaS-Ready Architecture","text":"<ul> <li>Job-Based Tracking - Group multiple LLM calls into business operations</li> <li>Hidden Complexity - Teams never see models, pricing, or LiteLLM</li> <li>Cost Aggregation - Track true costs per job, not per API call</li> <li>Usage Analytics - Detailed insights per team and job type</li> </ul>"},{"location":"#business-features","title":"\ud83d\udcb0 Business Features","text":"<ul> <li>Cost Transparency - See actual LiteLLM costs vs. customer pricing</li> <li>Flexible Pricing - Flat rate, tiered, or markup-based pricing</li> <li>Budget Controls - Per-team credit allocation with suspend/pause capabilities</li> <li>Profit Tracking - Calculate margins per job/team</li> </ul>"},{"location":"#technical-features","title":"\ud83d\udd27 Technical Features","text":"<ul> <li>\ud83d\ude80 Deploy to Railway with Docker</li> <li>\ud83d\udc18 PostgreSQL database with job tracking schema</li> <li>\ud83d\udc65 Team and organization management</li> <li>\ud83d\udd11 Virtual API key generation (hidden from teams)</li> <li>\ud83d\udd04 Multiple LLM providers (OpenAI, Anthropic, etc.)</li> <li>\u26a1 Redis caching for performance and cost savings</li> <li>\ud83d\udcca Rate limiting per team (TPM/RPM)</li> <li>\ud83c\udf9b\ufe0f Admin dashboard for team management</li> <li>\ud83c\udf0a Server-Sent Events (SSE) streaming support</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Getting Started</p> <p>New to SaaS LiteLLM? Start here for installation and setup.</p> <p> Quickstart Guide</p> </li> <li> <p> Admin Dashboard</p> <p>Learn how to manage organizations, teams, and model access.</p> <p> Dashboard Guide</p> </li> <li> <p> Integration Guide</p> <p>Integrate the SaaS API into your application.</p> <p> Integration Docs</p> </li> <li> <p> API Reference</p> <p>Complete API documentation with interactive examples.</p> <p> API Docs |  ReDoc</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Here's a simple example of using the job-based API:</p> PythonJavaScriptcURL <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\n\n# 1. Create job for tracking\njob = requests.post(f\"{API}/jobs/create\", json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"document_analysis\",\n    \"metadata\": {\"document_id\": \"doc_123\"}\n}).json()\n\njob_id = job[\"job_id\"]\n\n# 2. Make LLM call\nresponse = requests.post(f\"{API}/jobs/{job_id}/llm-call\", json={\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Analyze this document...\"}\n    ]\n}).json()\n\n# 3. Complete job\nresult = requests.post(f\"{API}/jobs/{job_id}/complete\", json={\n    \"status\": \"completed\"\n}).json()\n\nprint(f\"Total cost: ${result['costs']['total_cost_usd']}\")\n</code></pre> <pre><code>const API = \"http://localhost:8003/api\";\n\n// 1. Create job for tracking\nconst jobResponse = await fetch(`${API}/jobs/create`, {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({\n    team_id: \"acme-corp\",\n    job_type: \"document_analysis\",\n    metadata: { document_id: \"doc_123\" }\n  })\n});\nconst job = await jobResponse.json();\nconst jobId = job.job_id;\n\n// 2. Make LLM call\nconst llmResponse = await fetch(`${API}/jobs/${jobId}/llm-call`, {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({\n    messages: [\n      { role: \"user\", content: \"Analyze this document...\" }\n    ]\n  })\n});\n\n// 3. Complete job\nconst result = await fetch(`${API}/jobs/${jobId}/complete`, {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({ status: \"completed\" })\n}).json();\n\nconsole.log(`Total cost: $${result.costs.total_cost_usd}`);\n</code></pre> <pre><code># 1. Create job\nJOB_ID=$(curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"team_id\":\"acme-corp\",\"job_type\":\"document_analysis\"}' \\\n  | jq -r '.job_id')\n\n# 2. Make LLM call\ncurl -X POST http://localhost:8003/api/jobs/$JOB_ID/llm-call \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"messages\":[{\"role\":\"user\",\"content\":\"Analyze this document...\"}]}'\n\n# 3. Complete job\ncurl -X POST http://localhost:8003/api/jobs/$JOB_ID/complete \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\":\"completed\"}'\n</code></pre> <p> See more examples</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    A[Your SaaS Application] --&gt; B[SaaS API :8003]\n    B --&gt; C[LiteLLM Proxy :8002]\n    C --&gt; D[PostgreSQL Database]\n    C --&gt; E[Redis Cache]\n    C --&gt; F[OpenAI]\n    C --&gt; G[Anthropic]\n    C --&gt; H[Other Providers]\n\n    B -.Job Tracking.-&gt; D\n\n    style B fill:#4CAF50\n    style C fill:#2196F3\n    style D fill:#FF9800\n    style E fill:#F44336</code></pre> <p>Why This Architecture?</p> <ol> <li>Hidden Complexity - Teams interact with your SaaS API, not LiteLLM</li> <li>Job-Based Costs - Track costs for business operations, not individual API calls</li> <li>Flexible Pricing - Charge what you want, track actual costs internally</li> <li>Multi-Tenant - Isolated teams with their own budgets and limits</li> </ol> <p> Learn more about the architecture</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#for-admin-users","title":"For Admin Users","text":"<ul> <li>Admin Dashboard Guide - Manage organizations, teams, and model access</li> <li>Organizations - Create and manage organizations</li> <li>Teams - Team management and virtual keys</li> <li>Model Access Groups - Control model access</li> <li>Credits - Allocate and monitor credits</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<ul> <li>Integration Overview - How to integrate the API</li> <li>Job Workflow - Understanding job-based tracking</li> <li>Streaming - Server-Sent Events (SSE) streaming</li> <li>Typed Client - Type-safe Python client</li> <li>Error Handling - Handle errors gracefully</li> </ul>"},{"location":"#for-system-administrators","title":"For System Administrators","text":"<ul> <li>Local Development - Docker Compose setup</li> <li>Railway Deployment - Deploy to Railway</li> <li>Environment Variables - Configuration guide</li> <li>Testing - Run integration tests</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Interactive API Docs: ReDoc | Swagger UI</li> <li>Admin Dashboard: http://localhost:3002</li> <li>LiteLLM Documentation: docs.litellm.ai</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>Need Help?</p> <ul> <li>Check the Troubleshooting Guide</li> <li>Review Common Errors</li> <li>See Examples for working code</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to get started?</p> <ol> <li> Follow the Quickstart Guide</li> <li> Learn about the Architecture</li> <li> Try the Examples</li> <li> Integrate into Your App</li> </ol>"},{"location":"INTEGRATION_TEST_RESULTS/","title":"Integration Test Results","text":"<p>Date: 2025-10-16 Status: \u2705 ALL TESTS PASSED (12/12)</p>"},{"location":"INTEGRATION_TEST_RESULTS/#test-summary","title":"Test Summary","text":"<p>Comprehensive integration testing of JWT authentication and dual auth system.</p>"},{"location":"INTEGRATION_TEST_RESULTS/#test-results","title":"Test Results","text":"# Test Name Status Description 1 Health Check \u2705 PASS API server health endpoint responding 2 Setup/Login \u2705 PASS Owner account creation and login flow 3 JWT Authentication \u2705 PASS JWT Bearer token authentication working 4 Legacy X-Admin-Key Auth \u2705 PASS Backward compatible API key auth working 5 Dual Auth - Organizations \u2705 PASS Both JWT and X-Admin-Key work on endpoints 6 Create Organization \u2705 PASS Organization creation with JWT auth 7 Create Team \u2705 PASS Team creation with JWT auth 8 List Admin Users \u2705 PASS User management endpoints working 9 Create Admin User \u2705 PASS Creating additional admin users 10 View Audit Logs \u2705 PASS Audit logging system working 11 Unauthorized Access Protection \u2705 PASS Security - rejecting invalid/missing auth 12 Credits Management \u2705 PASS Credit allocation with both auth methods <p>Results: 12/12 tests passed (100%)</p>"},{"location":"INTEGRATION_TEST_RESULTS/#what-was-tested","title":"What Was Tested","text":""},{"location":"INTEGRATION_TEST_RESULTS/#1-jwt-authentication-system","title":"1. JWT Authentication System \u2705","text":"<p>Setup Flow: - \u2705 Setup status detection (needs_setup endpoint) - \u2705 First-time owner account creation - \u2705 Email/password validation - \u2705 JWT token generation - \u2705 Session creation in database</p> <p>Login Flow: - \u2705 Email/password authentication - \u2705 JWT token return - \u2705 User information in response - \u2705 Session tracking</p> <p>Authenticated Requests: - \u2705 Bearer token in Authorization header - \u2705 Token validation - \u2705 Session verification - \u2705 User retrieval from token</p>"},{"location":"INTEGRATION_TEST_RESULTS/#2-legacy-x-admin-key-authentication","title":"2. Legacy X-Admin-Key Authentication \u2705","text":"<p>Backward Compatibility: - \u2705 X-Admin-Key header recognition - \u2705 MASTER_KEY validation - \u2705 Access to all management endpoints - \u2705 Works alongside JWT (dual auth)</p>"},{"location":"INTEGRATION_TEST_RESULTS/#3-dual-authentication-support","title":"3. Dual Authentication Support \u2705","text":"<p>Management Endpoints: - \u2705 Organizations API (both JWT and X-Admin-Key) - \u2705 Teams API (both JWT and X-Admin-Key) - \u2705 Model Groups API (both JWT and X-Admin-Key) - \u2705 Credits API (both JWT and X-Admin-Key)</p> <p>JWT-Only Endpoints: - \u2705 Admin Users API (requires JWT) - \u2705 Audit Logs API (requires JWT) - \u2705 User profile endpoints (requires JWT)</p>"},{"location":"INTEGRATION_TEST_RESULTS/#4-role-based-access-control","title":"4. Role-Based Access Control \u2705","text":"<p>User Roles: - \u2705 Owner role created via setup - \u2705 Admin role created by owner - \u2705 Role validation on endpoints - \u2705 Permission checks working</p> <p>Role Restrictions: - \u2705 Admin users listing (owner/admin only) - \u2705 User creation (owner/admin only) - \u2705 Audit logs (owner/admin only)</p>"},{"location":"INTEGRATION_TEST_RESULTS/#5-security-features","title":"5. Security Features \u2705","text":"<p>Authentication Security: - \u2705 Invalid tokens rejected (401) - \u2705 Missing authentication rejected (401) - \u2705 Expired tokens rejected - \u2705 Session validation</p> <p>Password Security: - \u2705 Password hashing (bcrypt) - \u2705 Minimum 8 characters enforced - \u2705 Secure storage</p> <p>Session Management: - \u2705 Token hashing in database (SHA256) - \u2705 Session expiration (24 hours) - \u2705 Session revocation on logout - \u2705 IP and user agent tracking</p>"},{"location":"INTEGRATION_TEST_RESULTS/#6-audit-logging","title":"6. Audit Logging \u2705","text":"<p>Logged Actions: - \u2705 setup_owner - \u2705 login - \u2705 logout - \u2705 created_user - \u2705 updated_user - \u2705 deleted_user - \u2705 changed_password</p> <p>Audit Log Data: - \u2705 User ID tracking - \u2705 Action type - \u2705 Resource type and ID - \u2705 Action details (JSON) - \u2705 IP address - \u2705 Timestamp</p>"},{"location":"INTEGRATION_TEST_RESULTS/#7-api-endpoints","title":"7. API Endpoints \u2705","text":"<p>Organizations: - \u2705 List organizations - \u2705 Create organization - \u2705 Get organization details - \u2705 List organization teams - \u2705 Get organization usage</p> <p>Teams: - \u2705 List teams - \u2705 Create team - \u2705 Get team details - \u2705 Assign model groups</p> <p>Admin Users: - \u2705 Setup status check - \u2705 Setup owner account - \u2705 Login - \u2705 Logout - \u2705 Get current user (/me) - \u2705 List users - \u2705 Create user - \u2705 Update user - \u2705 Delete user - \u2705 Change password - \u2705 View audit logs</p> <p>Credits: - \u2705 Get balance - \u2705 Add credits (admin only) - \u2705 View transactions</p>"},{"location":"INTEGRATION_TEST_RESULTS/#8-frontend-integration","title":"8. Frontend Integration \u2705","text":"<p>Dashboard: - \u2705 Next.js server running on port 3002 - \u2705 Frontend can reach API on port 8004 - \u2705 Login flow works - \u2705 Authenticated requests work - \u2705 JWT token storage in localStorage</p>"},{"location":"INTEGRATION_TEST_RESULTS/#9-port-configuration","title":"9. Port Configuration \u2705","text":"<p>Non-conflicting Ports: - \u2705 PostgreSQL: 5433 - \u2705 Redis: 6381 - \u2705 SaaS API: 8004 - \u2705 Next.js Dashboard: 3002</p>"},{"location":"INTEGRATION_TEST_RESULTS/#10-database","title":"10. Database \u2705","text":"<p>Tables: - \u2705 admin_users (user accounts) - \u2705 admin_sessions (JWT sessions) - \u2705 admin_audit_log (action logging)</p> <p>Data Integrity: - \u2705 Foreign key constraints - \u2705 Unique constraints (email) - \u2705 Session cleanup - \u2705 Audit log retention</p>"},{"location":"INTEGRATION_TEST_RESULTS/#test-execution","title":"Test Execution","text":""},{"location":"INTEGRATION_TEST_RESULTS/#command","title":"Command:","text":"<pre><code>python3 scripts/test_jwt_integration.py\n</code></pre>"},{"location":"INTEGRATION_TEST_RESULTS/#test-user-created","title":"Test User Created:","text":"<ul> <li>Email: test-owner@example.com</li> <li>Password: TestPassword123!</li> <li>Role: owner</li> <li>User ID: ae89197e-e24a-4e55-a9e2-70ba9a273730</li> </ul>"},{"location":"INTEGRATION_TEST_RESULTS/#additional-users-created","title":"Additional Users Created:","text":"<ul> <li>Email: test-admin@example.com</li> <li>Role: admin</li> <li>User ID: f1e753ec-7e92-4767-8de8-e9a5bed22840</li> </ul>"},{"location":"INTEGRATION_TEST_RESULTS/#test-resources-created","title":"Test Resources Created:","text":"<ul> <li>Organization: test_org_integration</li> <li>Team: test_team_integration</li> <li>Credits allocated: 175 (100 + 50 + 25)</li> </ul>"},{"location":"INTEGRATION_TEST_RESULTS/#authentication-examples","title":"Authentication Examples","text":""},{"location":"INTEGRATION_TEST_RESULTS/#jwt-authentication","title":"JWT Authentication","text":"<pre><code># 1. Login\ncurl -X POST http://localhost:8004/api/admin-users/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"test-owner@example.com\",\n    \"password\": \"TestPassword123!\"\n  }'\n\n# Returns:\n# {\n#   \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n#   \"token_type\": \"bearer\",\n#   \"user\": {...}\n# }\n\n# 2. Use token for authenticated requests\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users/me\n</code></pre>"},{"location":"INTEGRATION_TEST_RESULTS/#legacy-x-admin-key-authentication","title":"Legacy X-Admin-Key Authentication","text":"<pre><code># Still works for backward compatibility\ncurl -H \"X-Admin-Key: sk-admin-local-dev-change-in-production\" \\\n  http://localhost:8004/api/organizations\n</code></pre>"},{"location":"INTEGRATION_TEST_RESULTS/#security-validation","title":"Security Validation","text":""},{"location":"INTEGRATION_TEST_RESULTS/#unauthorized-access-protection","title":"Unauthorized Access Protection","text":"<pre><code># No auth - REJECTED \u2705\ncurl http://localhost:8004/api/admin-users\n# Returns: 401 Unauthorized\n\n# Invalid token - REJECTED \u2705\ncurl -H \"Authorization: Bearer invalid-token\" \\\n  http://localhost:8004/api/admin-users\n# Returns: 401 Unauthorized\n\n# Valid token - ACCEPTED \u2705\ncurl -H \"Authorization: Bearer $VALID_TOKEN\" \\\n  http://localhost:8004/api/admin-users\n# Returns: 200 OK with user list\n</code></pre>"},{"location":"INTEGRATION_TEST_RESULTS/#performance","title":"Performance","text":"<ul> <li>Test Duration: ~2 seconds</li> <li>API Response Times: &lt; 100ms average</li> <li>Database Queries: Optimized with proper indexing</li> <li>Memory Usage: Stable throughout tests</li> </ul>"},{"location":"INTEGRATION_TEST_RESULTS/#recommendations","title":"Recommendations","text":""},{"location":"INTEGRATION_TEST_RESULTS/#for-production","title":"For Production:","text":"<ol> <li>\u2705 DONE: JWT authentication implemented</li> <li>\u2705 DONE: Role-based access control</li> <li>\u2705 DONE: Audit logging</li> <li>\u2705 DONE: Session management</li> <li>\u2705 DONE: Dual auth for backward compatibility</li> </ol>"},{"location":"INTEGRATION_TEST_RESULTS/#for-future-enhancements","title":"For Future Enhancements:","text":"<ol> <li>Token Refresh: Implement refresh tokens for longer sessions</li> <li>Rate Limiting: Add rate limiting to login endpoint</li> <li>2FA: Consider two-factor authentication for owners</li> <li>Session Management UI: Frontend for viewing/revoking sessions</li> <li>Audit Log Viewer: Dashboard page for audit logs</li> <li>Email Verification: Require email verification on signup</li> </ol>"},{"location":"INTEGRATION_TEST_RESULTS/#conclusion","title":"Conclusion","text":"<p>\u2705 All integration tests passed successfully!</p> <p>The JWT authentication system is fully functional with: - Secure email/password login - Role-based access control (owner, admin, user) - Session tracking and management - Complete audit logging - Backward compatible legacy auth - Frontend integration working</p> <p>The system is ready for production use with proper security measures in place.</p> <p>Test Script: <code>scripts/test_jwt_integration.py</code> Test Date: 2025-10-16 Tested By: Integration Test Suite Environment: Local Development (ports: 5433, 6381, 8004, 3002)</p>"},{"location":"environment-variables/","title":"Environment Variables Reference","text":"<p>Complete guide to all environment variables needed for SaasLiteLLM deployment.</p>"},{"location":"environment-variables/#quick-reference","title":"Quick Reference","text":"Service Env File Port Key Variables SaaS API <code>.env</code> (root) 8003 <code>DATABASE_URL</code>, <code>MASTER_KEY</code>, <code>LITELLM_MASTER_KEY</code>, <code>LITELLM_PROXY_URL</code> LiteLLM Proxy <code>.env</code> (root) 8002 <code>DATABASE_URL</code>, <code>LITELLM_MASTER_KEY</code>, <code>REDIS_*</code>, <code>STORE_MODEL_IN_DB</code> Admin Panel <code>admin-panel/.env.local</code> 3002 <code>NEXT_PUBLIC_API_URL</code>"},{"location":"environment-variables/#1-saas-api-environment-variables","title":"1. SaaS API Environment Variables","text":"<p>File: <code>.env</code> in project root</p>"},{"location":"environment-variables/#required","title":"Required","text":"<pre><code># Database Connection\nDATABASE_URL=postgresql://username:password@host:port/database\n\n# Admin Authentication (for SaaS API management endpoints)\n# Use this key with X-Admin-Key header to access admin endpoints\nMASTER_KEY=sk-admin-your-super-secure-admin-key-here\n\n# LiteLLM Connection\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-litellm-key-here\nLITELLM_PROXY_URL=http://localhost:8002\n</code></pre>"},{"location":"environment-variables/#optional-provider-api-keys","title":"Optional (Provider API Keys)","text":"<pre><code># LLM Provider API Keys (optional if configured via LiteLLM UI)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n# Add other providers as needed\n</code></pre>"},{"location":"environment-variables/#optional-redis-caching","title":"Optional (Redis Caching)","text":"<pre><code># Redis Configuration (optional for SaaS API, but recommended)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"environment-variables/#optional-server-configuration","title":"Optional (Server Configuration)","text":"<pre><code># Server Settings\nHOST=0.0.0.0\nPORT=8000\nWORKERS=1\nENVIRONMENT=development\nDEBUG=false\n</code></pre>"},{"location":"environment-variables/#2-litellm-proxy-environment-variables","title":"2. LiteLLM Proxy Environment Variables","text":"<p>File: <code>.env</code> in project root (SAME FILE as SaaS API)</p>"},{"location":"environment-variables/#required_1","title":"Required","text":"<pre><code># Database Connection (for storing models, teams, keys)\nDATABASE_URL=postgresql://username:password@host:port/database\n\n# LiteLLM Authentication\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-litellm-key-here\n\n# Enable database storage for models/teams/keys\nSTORE_MODEL_IN_DB=True\n\n# Redis Configuration (REQUIRED for LiteLLM caching and rate limiting)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"environment-variables/#optional-provider-api-keys_1","title":"Optional (Provider API Keys)","text":"<pre><code># LLM Provider API Keys\n# Can be configured here OR via LiteLLM UI at http://localhost:8002/ui\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nAZURE_API_KEY=...\nAZURE_API_BASE=https://...\nAZURE_API_VERSION=2024-02-15-preview\n# Add other providers as needed\n</code></pre>"},{"location":"environment-variables/#3-admin-panel-environment-variables","title":"3. Admin Panel Environment Variables","text":"<p>File: <code>admin-panel/.env.local</code></p>"},{"location":"environment-variables/#required_2","title":"Required","text":"<pre><code># SaaS API URL\n# Points to where your SaaS API is running\nNEXT_PUBLIC_API_URL=http://localhost:8003\n</code></pre> <p>Note: The MASTER_KEY is NOT stored in the admin panel's <code>.env.local</code> file. Users enter it in the login page, and it's validated against the SaaS API.</p>"},{"location":"environment-variables/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"environment-variables/#local-development","title":"Local Development","text":"<p>SaaS API &amp; LiteLLM (<code>.env</code>): <pre><code>DATABASE_URL=postgresql://litellm_user:litellm_password@localhost:5432/litellm\nMASTER_KEY=sk-admin-local-dev-change-in-production\nLITELLM_MASTER_KEY=sk-local-dev-master-key-change-me\nLITELLM_PROXY_URL=http://localhost:8002\nSTORE_MODEL_IN_DB=True\n\n# Redis (from Docker Compose)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n\n# Provider keys (optional)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Note: If you have multiple Redis instances running locally, adjust <code>REDIS_PORT</code> to match your instance (e.g., 6380). For Docker Compose users, Redis is exposed on port 6380 on the host but uses 6379 internally.</p> <p>Admin Panel (<code>admin-panel/.env.local</code>): <pre><code>NEXT_PUBLIC_API_URL=http://localhost:8003\n</code></pre></p>"},{"location":"environment-variables/#railway-deployment","title":"Railway Deployment","text":"<p>SaaS API Service: <pre><code>DATABASE_URL=${{Postgres.DATABASE_URL}}\nMASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n\n# Redis (if using Railway Redis)\nREDIS_HOST=${{Redis.REDISHOST}}\nREDIS_PORT=${{Redis.REDISPORT}}\nREDIS_PASSWORD=${{Redis.REDIS_PASSWORD}}\nREDIS_URL=${{Redis.REDIS_URL}}\n\n# Provider keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>LiteLLM Proxy Service: <pre><code>DATABASE_URL=${{Postgres.DATABASE_URL}}\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\nSTORE_MODEL_IN_DB=True\n\n# Redis (REQUIRED for LiteLLM)\nREDIS_HOST=${{Redis.REDISHOST}}\nREDIS_PORT=${{Redis.REDISPORT}}\nREDIS_PASSWORD=${{Redis.REDIS_PASSWORD}}\nREDIS_URL=${{Redis.REDIS_URL}}\n\n# Provider keys (can also be added via UI)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Admin Panel Service: <pre><code>NEXT_PUBLIC_API_URL=https://your-saas-api.railway.app\n</code></pre></p>"},{"location":"environment-variables/#key-concepts","title":"Key Concepts","text":""},{"location":"environment-variables/#1-two-master-keys","title":"1. Two Master Keys","text":"<p>There are TWO different master keys:</p> Key Purpose Used By <code>MASTER_KEY</code> SaaS API admin authentication Admin Panel, API management tools <code>LITELLM_MASTER_KEY</code> LiteLLM proxy authentication SaaS API \u2194 LiteLLM communication, LiteLLM UI access <p>IMPORTANT: These must be DIFFERENT keys for security.</p>"},{"location":"environment-variables/#2-shared-database","title":"2. Shared Database","text":"<p>Both SaaS API and LiteLLM Proxy can share the SAME PostgreSQL database: - SaaS API uses it for: organizations, teams, credits, jobs - LiteLLM uses it for: models, virtual keys, budgets (when <code>STORE_MODEL_IN_DB=True</code>)</p> <p>They use different tables, so there's no conflict.</p>"},{"location":"environment-variables/#3-redis-configuration","title":"3. Redis Configuration","text":"<p>Redis serves different purposes:</p> <p>For SaaS API (optional): - Response caching - Session management - Performance optimization</p> <p>For LiteLLM (REQUIRED): - Request caching - Rate limiting - Token counting - Budget tracking</p>"},{"location":"environment-variables/#4-store_model_in_db-flag","title":"4. STORE_MODEL_IN_DB Flag","text":"<p>When <code>STORE_MODEL_IN_DB=True</code>: - Models are stored in PostgreSQL (not YAML config) - Teams/keys are persisted in database - Changes via LiteLLM UI are permanent - Enables dynamic model management</p> <p>This is REQUIRED for production deployments.</p>"},{"location":"environment-variables/#generating-secure-keys","title":"Generating Secure Keys","text":"<p>For production, generate strong random keys:</p> <pre><code># Generate MASTER_KEY\nopenssl rand -hex 32\n\n# Generate LITELLM_MASTER_KEY\nopenssl rand -hex 32\n</code></pre> <p>Then format them with proper prefixes: <pre><code>MASTER_KEY=sk-admin-&lt;generated-hex&gt;\nLITELLM_MASTER_KEY=sk-litellm-&lt;generated-hex&gt;\n</code></pre></p>"},{"location":"environment-variables/#validation-checklist","title":"Validation Checklist","text":"<p>Before deploying, verify:</p>"},{"location":"environment-variables/#saas-api","title":"SaaS API","text":"<ul> <li> <code>DATABASE_URL</code> connects successfully</li> <li> <code>MASTER_KEY</code> is secure and unique</li> <li> <code>LITELLM_MASTER_KEY</code> matches LiteLLM proxy</li> <li> <code>LITELLM_PROXY_URL</code> is correct for environment</li> <li> Provider API keys are valid (if provided)</li> </ul>"},{"location":"environment-variables/#litellm-proxy","title":"LiteLLM Proxy","text":"<ul> <li> <code>DATABASE_URL</code> connects successfully</li> <li> <code>LITELLM_MASTER_KEY</code> is secure and unique</li> <li> <code>STORE_MODEL_IN_DB=True</code> is set</li> <li> Redis connection works (<code>REDIS_HOST</code>, <code>REDIS_PORT</code>)</li> <li> Can access UI at <code>http://localhost:8002/ui</code></li> </ul>"},{"location":"environment-variables/#admin-panel","title":"Admin Panel","text":"<ul> <li> <code>NEXT_PUBLIC_API_URL</code> points to correct SaaS API</li> <li> Can log in with <code>MASTER_KEY</code></li> <li> All API requests include <code>X-Admin-Key</code> header</li> </ul>"},{"location":"environment-variables/#troubleshooting","title":"Troubleshooting","text":""},{"location":"environment-variables/#failed-to-connect-to-litellm","title":"\"Failed to connect to LiteLLM\"","text":"<ul> <li>Check <code>LITELLM_PROXY_URL</code> is correct</li> <li>Verify <code>LITELLM_MASTER_KEY</code> matches in both services</li> <li>Ensure LiteLLM proxy is running</li> </ul>"},{"location":"environment-variables/#401-unauthorized-from-saas-api","title":"\"401 Unauthorized\" from SaaS API","text":"<ul> <li>Verify <code>MASTER_KEY</code> is correct</li> <li>Check <code>X-Admin-Key</code> header is being sent</li> <li>Ensure admin panel has correct API URL</li> </ul>"},{"location":"environment-variables/#models-not-persisting-in-litellm","title":"\"Models not persisting\" in LiteLLM","text":"<ul> <li>Verify <code>STORE_MODEL_IN_DB=True</code> is set</li> <li>Check <code>DATABASE_URL</code> is correct</li> <li>Restart LiteLLM proxy after changing this setting</li> </ul>"},{"location":"environment-variables/#redis-connection-errors","title":"Redis connection errors","text":"<ul> <li>Verify Redis is running</li> <li>Check <code>REDIS_HOST</code> and <code>REDIS_PORT</code></li> <li>For Railway, ensure Redis service is linked</li> </ul>"},{"location":"environment-variables/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit keys to git</li> <li>Use <code>.env</code> files (in <code>.gitignore</code>)</li> <li> <p>Use Railway's built-in secret management</p> </li> <li> <p>Use different keys for different environments</p> </li> <li>Development keys should differ from production</li> <li> <p>Each Railway environment should have unique keys</p> </li> <li> <p>Rotate keys regularly</p> </li> <li>Change <code>MASTER_KEY</code> quarterly</li> <li>Change <code>LITELLM_MASTER_KEY</code> quarterly</li> <li> <p>Update provider API keys as needed</p> </li> <li> <p>Limit key access</p> </li> <li>Only share <code>MASTER_KEY</code> with admins</li> <li>End users should NEVER see <code>MASTER_KEY</code> or <code>LITELLM_MASTER_KEY</code></li> <li> <p>Use virtual keys for team access</p> </li> <li> <p>Monitor for unauthorized access</p> </li> <li>Check logs for 401 errors</li> <li>Monitor unusual API usage patterns</li> <li>Set up alerts for multiple failed auth attempts</li> </ol>"},{"location":"integration-flow/","title":"Complete Integration Flow","text":"<p>This guide explains how all components of SaasLiteLLM work together, from admin setup to end-user requests.</p>"},{"location":"integration-flow/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        ADMIN SETUP PHASE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  1. Admin \u2192 LiteLLM UI (Port 8002)                              \u2502\n\u2502     - Add provider credentials (OpenAI, Anthropic, etc.)        \u2502\n\u2502     - Configure models (gpt-4, claude-3-sonnet, etc.)           \u2502\n\u2502     - Test models in playground                                  \u2502\n\u2502                                                                  \u2502\n\u2502  2. Admin \u2192 SaaS Admin Panel (Port 3000)                        \u2502\n\u2502     - Create organizations                                       \u2502\n\u2502     - Create model groups (referencing LiteLLM models)          \u2502\n\u2502     - Create teams with budgets                                  \u2502\n\u2502     - Assign model groups to teams                               \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     END USER REQUEST PHASE                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  Client Application                                              \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (1) Uses team's virtual_key                           \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  SaaS API (Port 8003)                                           \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (2) Validates key, checks credits, tracks job         \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  LiteLLM Proxy (Port 8002)                                      \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (3) Routes to provider, tracks costs                  \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  LLM Provider (OpenAI, Anthropic, etc.)                         \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (4) Returns completion                                 \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  Response flows back to client                                   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integration-flow/#authentication-keys","title":"Authentication Keys","text":"<p>Understanding the different keys is crucial:</p>"},{"location":"integration-flow/#1-master_key-saas-api-admin-key","title":"1. MASTER_KEY (SaaS API Admin Key)","text":"<p>Purpose: Admin access to SaaS API management endpoints</p> <p>Used for: - Creating organizations - Creating teams - Creating model groups - Adding credits - Viewing usage reports</p> <p>Used by: System administrators via admin panel or API calls</p> <p>Configured in: <code>saas-api/.env</code> \u2192 <code>MASTER_KEY</code></p> <p>Example: <pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"X-Admin-Key: sk-admin-your-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Acme Corp\", \"litellm_organization_id\": \"acme\"}'\n</code></pre></p> <p>Security: - \u26a0\ufe0f Never expose to end users - Store securely (environment variables, secrets manager) - Rotate quarterly - Different from LITELLM_MASTER_KEY</p>"},{"location":"integration-flow/#2-litellm_master_key-litellm-admin-key","title":"2. LITELLM_MASTER_KEY (LiteLLM Admin Key)","text":"<p>Purpose: Admin access to LiteLLM proxy management</p> <p>Used for: - Accessing LiteLLM UI - SaaS API creating virtual keys for teams - Managing models and credentials in LiteLLM - Direct LiteLLM API access (admin only)</p> <p>Used by: - Admins accessing LiteLLM UI - SaaS API (programmatically)</p> <p>Configured in: Both <code>saas-api/.env</code> and <code>litellm/.env</code> \u2192 <code>LITELLM_MASTER_KEY</code></p> <p>Example: <pre><code># Access LiteLLM UI\nhttp://localhost:8002/ui\n# Login with: LITELLM_MASTER_KEY value\n\n# Direct API call\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer sk-litellm-your-master-key\" \\\n  -d '{\"model\": \"gpt-3.5-turbo\", \"messages\": [...]}'\n</code></pre></p> <p>Security: - \u26a0\ufe0f Never expose to end users - Must be the same in both SaaS API and LiteLLM .env files - Used by SaaS API to create team virtual keys - Rotate quarterly</p>"},{"location":"integration-flow/#3-virtual-keys-team-keys","title":"3. Virtual Keys (Team Keys)","text":"<p>Purpose: Team-specific access to LiteLLM with budget/model controls</p> <p>Used for: - End users making LLM requests - Enforcing budget limits per team - Restricting model access per team - Tracking usage per team</p> <p>Created by: SaaS API automatically when creating a team</p> <p>Used by: End-user applications (your SaaS customers)</p> <p>Example: <pre><code># Your customer's application uses their team's virtual key\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer sk-litellm-virtual-key-team-abc-123\" \\\n  -d '{\"model\": \"gpt-3.5-turbo\", \"messages\": [...]}'\n</code></pre></p> <p>Security: - \u2705 Safe to give to end users - Budget-limited (can't overspend) - Model-limited (only assigned models) - Rate-limited (RPM/TPM limits) - Tracked per team</p>"},{"location":"integration-flow/#setup-flow-admin-perspective","title":"Setup Flow (Admin Perspective)","text":""},{"location":"integration-flow/#phase-1-infrastructure-setup","title":"Phase 1: Infrastructure Setup","text":"<ol> <li>Deploy services (Docker Compose, Railway, etc.)</li> <li>PostgreSQL database</li> <li>Redis cache</li> <li>LiteLLM proxy</li> <li>SaaS API</li> <li> <p>Admin panel</p> </li> <li> <p>Configure environment variables:    <pre><code># Generate strong keys\nMASTER_KEY=$(openssl rand -hex 32)\nLITELLM_MASTER_KEY=$(openssl rand -hex 32)\n\n# Set in .env files\n# saas-api/.env\nMASTER_KEY=sk-admin-...\nLITELLM_MASTER_KEY=sk-litellm-...\n\n# litellm/.env\nLITELLM_MASTER_KEY=sk-litellm-...  # Same as SaaS API\n</code></pre></p> </li> <li> <p>Verify services are running:    <pre><code>docker ps\n# Should see: postgres, redis, litellm-proxy, saas-api, admin-panel\n</code></pre></p> </li> </ol>"},{"location":"integration-flow/#phase-2-litellm-configuration-critical","title":"Phase 2: LiteLLM Configuration (CRITICAL)","text":"<p>This must be done BEFORE creating teams in SaaS API</p> <ol> <li>Access LiteLLM UI: http://localhost:8002/ui</li> <li> <p>Login with <code>LITELLM_MASTER_KEY</code></p> </li> <li> <p>Add Provider Credentials:</p> </li> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li> <p>For each provider you want to use:      <pre><code>OpenAI:\n  - Key Alias: openai-prod\n  - Provider: openai\n  - API Key: sk-... (from OpenAI dashboard)\n\nAnthropic:\n  - Key Alias: anthropic-prod\n  - Provider: anthropic\n  - API Key: sk-ant-... (from Anthropic console)\n</code></pre></p> </li> <li> <p>Add Models:</p> </li> <li>Navigate to Models tab</li> <li>Click + Add Model</li> <li> <p>For each model you want to offer:      <pre><code>Example 1:\n  - Model Name: gpt-3.5-turbo\n  - LiteLLM Model Name: openai/gpt-3.5-turbo\n  - Credential: openai-prod\n  - Cost per 1K input tokens: 0.0005\n  - Cost per 1K output tokens: 0.0015\n\nExample 2:\n  - Model Name: claude-3-sonnet\n  - LiteLLM Model Name: anthropic/claude-3-sonnet-20240229\n  - Credential: anthropic-prod\n  - Cost per 1K input tokens: 0.003\n  - Cost per 1K output tokens: 0.015\n</code></pre></p> </li> <li> <p>Test Models:</p> </li> <li>Navigate to Playground tab</li> <li>Select each model</li> <li>Send test message: \"Hello, are you working?\"</li> <li>Verify responses</li> </ol> <p>\u2705 Checkpoint: All models should respond successfully in playground</p>"},{"location":"integration-flow/#phase-3-saas-api-configuration","title":"Phase 3: SaaS API Configuration","text":"<ol> <li>Access Admin Panel: http://localhost:3000</li> <li> <p>Login with <code>MASTER_KEY</code> (from SaaS API .env)</p> </li> <li> <p>Create Organization:    <pre><code>- Name: Acme Corporation\n- LiteLLM Org ID: acme-corp\n- Description: Main organization\n</code></pre></p> </li> <li> <p>Create Model Groups:    <pre><code>Group 1 - Basic Tier:\n  - Name: basic-models\n  - Models: gpt-3.5-turbo\n  - Description: Fast, cost-effective models\n\nGroup 2 - Premium Tier:\n  - Name: premium-models\n  - Models: gpt-4-turbo, claude-3-sonnet, claude-3-opus\n  - Description: Most capable models\n</code></pre></p> </li> </ol> <p>\u26a0\ufe0f Important: Model names must match EXACTLY what you configured in LiteLLM</p> <ol> <li> <p>Create Team:    <pre><code>- Name: Acme Dev Team\n- Organization: Acme Corporation\n- Model Group: basic-models\n- Max Budget: 100.0 (USD/credits)\n- Budget Duration: 30d\n- Rate Limits:\n  - RPM: 100\n  - TPM: 50000\n</code></pre></p> </li> <li> <p>Get Team Virtual Key:</p> </li> <li>After creating team, view team details</li> <li>Copy the <code>virtual_key</code> value</li> <li>This is what your customer will use</li> </ol> <p>\u2705 Checkpoint: Team created successfully with virtual_key</p>"},{"location":"integration-flow/#request-flow-end-user-perspective","title":"Request Flow (End User Perspective)","text":""},{"location":"integration-flow/#step-1-client-gets-team-credentials","title":"Step 1: Client Gets Team Credentials","text":"<p>Your SaaS application provides the team's virtual key to their application:</p> <pre><code>// Your application provides these to the customer\nconst teamVirtualKey = \"sk-litellm-virtual-key-abc123\";\nconst apiEndpoint = \"http://localhost:8002\"; // LiteLLM proxy URL\n</code></pre>"},{"location":"integration-flow/#step-2-client-makes-llm-request","title":"Step 2: Client Makes LLM Request","text":"<p>The customer's application makes requests directly to LiteLLM:</p> <pre><code>const response = await fetch('http://localhost:8002/chat/completions', {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${teamVirtualKey}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'gpt-3.5-turbo',\n    messages: [\n      { role: 'user', content: 'Hello, world!' }\n    ]\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.choices[0].message.content);\n</code></pre>"},{"location":"integration-flow/#step-3-litellm-processes-request","title":"Step 3: LiteLLM Processes Request","text":"<p>Behind the scenes, LiteLLM:</p> <ol> <li>Validates virtual key: Checks if key exists and is valid</li> <li>Checks team budget: Ensures team has enough credits</li> <li>Checks model access: Verifies team can use requested model</li> <li>Checks rate limits: Ensures RPM/TPM not exceeded</li> <li>Routes to provider: Forwards request to OpenAI/Anthropic/etc.</li> <li>Tracks costs: Records token usage and costs</li> <li>Deducts budget: Updates team's remaining budget</li> <li>Returns response: Sends completion back to client</li> </ol>"},{"location":"integration-flow/#step-4-saas-api-tracks-usage","title":"Step 4: SaaS API Tracks Usage","text":"<p>The SaaS API can query usage data:</p> <pre><code># Admin checks team usage\ncurl http://localhost:8003/api/teams/{team_id}/usage \\\n  -H \"X-Admin-Key: $MASTER_KEY\"\n\n# Response includes:\n{\n  \"team_id\": \"team_abc123\",\n  \"total_spend\": 15.50,\n  \"remaining_budget\": 84.50,\n  \"request_count\": 1234,\n  \"models_used\": {\n    \"gpt-3.5-turbo\": { \"requests\": 1000, \"cost\": 10.00 },\n    \"gpt-4\": { \"requests\": 234, \"cost\": 5.50 }\n  }\n}\n</code></pre>"},{"location":"integration-flow/#budget-management","title":"Budget Management","text":""},{"location":"integration-flow/#how-credits-work","title":"How Credits Work","text":"<ol> <li> <p>Admin adds credits to team:    <pre><code>curl -X POST http://localhost:8003/api/credits/teams/{team_id}/add \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -d '{\"amount\": 100.0, \"reason\": \"Monthly allocation\"}'\n</code></pre></p> </li> <li> <p>LiteLLM tracks spending:</p> </li> <li>Every request deducts from team budget</li> <li>Cost = (input_tokens * input_cost) + (output_tokens * output_cost)</li> <li> <p>Stored in LiteLLM's database</p> </li> <li> <p>When budget exhausted:</p> </li> <li>Team's virtual key stops working</li> <li>Requests return 429 error (quota exceeded)</li> <li>Admin must add more credits</li> </ol>"},{"location":"integration-flow/#budget-modes","title":"Budget Modes","text":"<p>Teams can have different budget enforcement modes:</p> <p>Hard Limit (default): - Requests blocked when budget reached - Team cannot exceed budget under any circumstance</p> <p>Soft Limit: - Warning sent when 80% budget used - Requests still allowed after budget reached - Useful for enterprise customers with invoicing</p>"},{"location":"integration-flow/#model-access-control","title":"Model Access Control","text":""},{"location":"integration-flow/#how-model-groups-work","title":"How Model Groups Work","text":"<ol> <li> <p>Admin creates model group in SaaS API:    <pre><code>Model Group: \"premium-models\"\nModels: [\"gpt-4-turbo\", \"claude-3-opus\"]\n</code></pre></p> </li> <li> <p>Admin assigns to team:</p> </li> <li>Team's virtual key is configured in LiteLLM</li> <li> <p>Only specified models are accessible</p> </li> <li> <p>User requests model:    <pre><code>// \u2705 Allowed (model in group)\n{ model: \"gpt-4-turbo\", messages: [...] }\n\n// \u274c Denied (model not in group)\n{ model: \"gpt-3.5-turbo\", messages: [...] }\n// Returns 403 Forbidden\n</code></pre></p> </li> </ol>"},{"location":"integration-flow/#changing-model-access","title":"Changing Model Access","text":"<p>To change a team's model access:</p> <ol> <li>Update the model group assignment in SaaS API admin panel</li> <li>Or create a new model group and reassign the team</li> <li>Changes take effect immediately</li> <li>No need to regenerate virtual keys</li> </ol>"},{"location":"integration-flow/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"integration-flow/#litellm-ui-dashboard","title":"LiteLLM UI Dashboard","text":"<p>Access: http://localhost:8002/ui</p> <p>Available views: - Usage by team - Cost breakdown by model - Request counts and latencies - Error rates - Budget remaining per team</p>"},{"location":"integration-flow/#saas-api-endpoints","title":"SaaS API Endpoints","text":"<p>Organization usage: <pre><code>GET /api/organizations/{org_id}/usage\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p> <p>Team usage: <pre><code>GET /api/teams/{team_id}/usage\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p> <p>All teams usage: <pre><code>GET /api/teams\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p>"},{"location":"integration-flow/#logs","title":"Logs","text":"<p>LiteLLM logs: <pre><code>docker logs litellm-proxy\n# Shows: requests, costs, errors, rate limits\n</code></pre></p> <p>SaaS API logs: <pre><code>docker logs saas-api\n# Shows: admin actions, team creation, credit additions\n</code></pre></p>"},{"location":"integration-flow/#production-deployment","title":"Production Deployment","text":""},{"location":"integration-flow/#environment-configuration","title":"Environment Configuration","text":"<p>Railway (recommended):</p> <ol> <li>Deploy LiteLLM:</li> <li>Expose public URL for admin access</li> <li>Use private networking for SaaS API connection</li> <li> <p>Set <code>LITELLM_MASTER_KEY</code> in Railway secrets</p> </li> <li> <p>Deploy SaaS API:</p> </li> <li>Private networking to LiteLLM</li> <li>Set <code>MASTER_KEY</code> and <code>LITELLM_MASTER_KEY</code></li> <li> <p>Set <code>LITELLM_PROXY_URL</code> to LiteLLM's internal URL</p> </li> <li> <p>Deploy Admin Panel:</p> </li> <li>Public URL for admin access</li> <li>Set <code>NEXT_PUBLIC_API_URL</code> to SaaS API URL</li> </ol>"},{"location":"integration-flow/#security-checklist","title":"Security Checklist","text":"<ul> <li> Changed default <code>MASTER_KEY</code> to strong random value</li> <li> Changed default <code>LITELLM_MASTER_KEY</code> to strong random value</li> <li> LiteLLM UI accessible only via HTTPS</li> <li> Admin panel accessible only via HTTPS</li> <li> Rate limiting enabled in LiteLLM</li> <li> Budget limits set on all teams</li> <li> Monitoring/alerting configured</li> <li> Database backups enabled</li> <li> Provider API keys stored securely</li> <li> No keys committed to git</li> </ul>"},{"location":"integration-flow/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"integration-flow/#issue-model-not-found","title":"Issue: \"Model not found\"","text":"<p>Cause: Model name mismatch between SaaS API and LiteLLM</p> <p>Solution: 1. Check model name in SaaS API model group 2. Check model name in LiteLLM UI Models tab 3. Ensure they match exactly (case-sensitive)</p>"},{"location":"integration-flow/#issue-invalid-api-key","title":"Issue: \"Invalid API key\"","text":"<p>Cause: Team virtual key not working</p> <p>Solution: 1. Verify team exists and has budget 2. Check LiteLLM UI Keys tab for the virtual key 3. Ensure model group is assigned to team 4. Try recreating the team</p>"},{"location":"integration-flow/#issue-quota-exceeded","title":"Issue: \"Quota exceeded\"","text":"<p>Cause: Team has exhausted budget</p> <p>Solution: 1. Check team's remaining budget in admin panel 2. Add more credits via admin panel 3. Or adjust budget limits for the team</p>"},{"location":"integration-flow/#issue-saas-api-cant-connect-to-litellm","title":"Issue: SaaS API can't connect to LiteLLM","text":"<p>Cause: Network configuration problem</p> <p>Solution: 1. Verify <code>LITELLM_PROXY_URL</code> in SaaS API .env 2. Check both services on same Docker network 3. Test connectivity: <code>curl http://litellm-proxy:8002/health</code> 4. Verify <code>LITELLM_MASTER_KEY</code> matches in both services</p>"},{"location":"integration-flow/#next-steps","title":"Next Steps","text":"<ul> <li>LiteLLM Setup Guide - Detailed LiteLLM configuration</li> <li>API Reference - Complete API documentation</li> <li>Deployment Guide - Production deployment</li> <li>Security Documentation - Security best practices</li> </ul>"},{"location":"litellm-setup/","title":"LiteLLM Proxy Setup Guide","text":"<p>This guide explains how to configure and integrate the LiteLLM proxy with the SaaS API.</p>"},{"location":"litellm-setup/#critical-setup-requirement","title":"\u26a0\ufe0f Critical Setup Requirement","text":"<p>You MUST configure LiteLLM with provider credentials BEFORE your SaaS API will work.</p> <p>The LiteLLM proxy is where you: 1. Store your LLM provider API keys (OpenAI, Anthropic, etc.) 2. Configure which models are available 3. Set up model parameters and routing</p> <p>The LiteLLM UI must be exposed so you can configure these settings. All end-user requests will go through your SaaS API, but the admin must configure models in LiteLLM first.</p>"},{"location":"litellm-setup/#overview","title":"Overview","text":"<p>SaasLiteLLM uses LiteLLM as a unified proxy layer for multiple LLM providers. LiteLLM provides:</p> <ul> <li>Unified API: Call 100+ LLM providers using the OpenAI format</li> <li>Cost Tracking: Automatic usage and cost tracking per team</li> <li>Rate Limiting: Built-in rate limiting and budget management</li> <li>Caching: Redis-based response caching for cost savings</li> <li>Load Balancing: Smart routing across multiple models</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  SaaS API   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   LiteLLM    \u2502\n\u2502 Application \u2502     \u2502 (Port 8003) \u2502     \u2502  (Port 8002) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                          \u2502               \u2502\n                    \u25bc                          \u25bc               \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  OpenAI  \u2502              \u2502 Anthropic\u2502    \u2502  Others  \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"litellm-setup/#quick-start-exposing-litellm-ui","title":"Quick Start: Exposing LiteLLM UI","text":""},{"location":"litellm-setup/#1-start-litellm-with-ui-exposed","title":"1. Start LiteLLM with UI Exposed","text":"<p>In your <code>docker-compose.yml</code>, ensure the LiteLLM proxy port is exposed:</p> <pre><code>services:\n  litellm-proxy:\n    image: ghcr.io/berriai/litellm:main-latest\n    ports:\n      - \"8002:8002\"  # This MUST be exposed\n    environment:\n      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}\n      - DATABASE_URL=${DATABASE_URL}\n      # ... other environment variables\n</code></pre>"},{"location":"litellm-setup/#2-access-the-litellm-admin-ui","title":"2. Access the LiteLLM Admin UI","text":"<p>Once LiteLLM is running, access the admin dashboard:</p> <p>Local Development: <pre><code>http://localhost:8002/ui\n</code></pre></p> <p>Production (Railway/Cloud): <pre><code>https://your-litellm-domain.railway.app/ui\n</code></pre></p> <p>Login with your MASTER_KEY: - When prompted, enter your <code>LITELLM_MASTER_KEY</code> value - This is the same key from your <code>.env</code> file</p>"},{"location":"litellm-setup/#3-security-for-production","title":"3. Security for Production","text":"<p>When exposing LiteLLM in production:</p> <ol> <li> <p>Use strong authentication: Change <code>LITELLM_MASTER_KEY</code> to a secure random key    <pre><code>openssl rand -hex 32\n</code></pre></p> </li> <li> <p>Restrict access by IP (recommended for Railway):</p> </li> <li>Use Railway's private networking when possible</li> <li> <p>Or configure firewall rules to allow only your admin IPs</p> </li> <li> <p>Use HTTPS: Always access via HTTPS in production</p> </li> <li> <p>Monitor access: Review LiteLLM logs regularly for unauthorized access attempts</p> </li> <li> <p>Separate keys: Never share <code>LITELLM_MASTER_KEY</code> with end users (they use team virtual keys)</p> </li> </ol>"},{"location":"litellm-setup/#step-by-step-adding-provider-credentials","title":"Step-by-Step: Adding Provider Credentials","text":"<p>This is the most important setup step. Without provider credentials, your SaaS API cannot make LLM requests.</p>"},{"location":"litellm-setup/#adding-openai-credentials","title":"Adding OpenAI Credentials","text":"<ol> <li>Navigate to Keys tab in LiteLLM UI</li> <li>Click + Add Key</li> <li>Fill in the details:</li> <li>Key Alias: <code>openai-main</code> (or any descriptive name)</li> <li>Provider: Select <code>openai</code></li> <li>API Key: Paste your OpenAI API key (<code>sk-...</code>)</li> <li>Metadata (optional): Add notes like \"Production OpenAI Key\"</li> <li>Click Save</li> </ol>"},{"location":"litellm-setup/#adding-anthropic-credentials","title":"Adding Anthropic Credentials","text":"<ol> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li>Fill in:</li> <li>Key Alias: <code>anthropic-main</code></li> <li>Provider: <code>anthropic</code></li> <li>API Key: Your Anthropic key (<code>sk-ant-...</code>)</li> <li>Click Save</li> </ol>"},{"location":"litellm-setup/#adding-azure-openai-credentials","title":"Adding Azure OpenAI Credentials","text":"<ol> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li>Fill in:</li> <li>Key Alias: <code>azure-openai</code></li> <li>Provider: <code>azure</code></li> <li>API Key: Your Azure key</li> <li>API Base: <code>https://your-resource.openai.azure.com</code></li> <li>API Version: <code>2024-02-15-preview</code></li> <li>Click Save</li> </ol>"},{"location":"litellm-setup/#adding-other-provider-credentials","title":"Adding Other Provider Credentials","text":"<p>LiteLLM supports 100+ providers. For each provider:</p> <ol> <li>Keys tab \u2192 + Add Key</li> <li>Select the provider from dropdown</li> <li>Enter the required fields (varies by provider)</li> <li>Save</li> </ol> <p>Supported providers include: - Google (Vertex AI, PaLM) - AWS Bedrock - Cohere - Hugging Face - Replicate - Together AI - Anyscale - And many more...</p> <p>See LiteLLM Providers for the complete list.</p>"},{"location":"litellm-setup/#step-by-step-adding-models","title":"Step-by-Step: Adding Models","text":"<p>Once you have credentials configured, you can add models.</p>"},{"location":"litellm-setup/#via-litellm-ui-recommended","title":"Via LiteLLM UI (Recommended)","text":"<ol> <li>Navigate to Models tab</li> <li>Click + Add Model</li> <li>Fill in the model configuration:</li> </ol> <p>Example: Adding GPT-4 Turbo <pre><code>Model Name: gpt-4-turbo\nLiteLLM Model Name: openai/gpt-4-turbo\nProvider Credential: openai-main (select from dropdown)\nModel Info:\n  - Mode: chat\n  - Input Cost (per 1K tokens): 0.01\n  - Output Cost (per 1K tokens): 0.03\n</code></pre></p> <p>Example: Adding Claude 3 Sonnet <pre><code>Model Name: claude-3-sonnet\nLiteLLM Model Name: anthropic/claude-3-sonnet-20240229\nProvider Credential: anthropic-main\nModel Info:\n  - Mode: chat\n  - Input Cost: 0.003\n  - Output Cost: 0.015\n</code></pre></p> <p>Example: Adding GPT-3.5 Turbo <pre><code>Model Name: gpt-3.5-turbo\nLiteLLM Model Name: openai/gpt-3.5-turbo\nProvider Credential: openai-main\nModel Info:\n  - Mode: chat\n  - Input Cost: 0.0005\n  - Output Cost: 0.0015\n</code></pre></p> <ol> <li>Click Save Model</li> <li>The model is now available for your SaaS API teams!</li> </ol>"},{"location":"litellm-setup/#model-name-format","title":"Model Name Format","text":"<p>The <code>LiteLLM Model Name</code> follows this pattern: <pre><code>&lt;provider&gt;/&lt;model-identifier&gt;\n</code></pre></p> <p>Examples: - OpenAI: <code>openai/gpt-4-turbo</code>, <code>openai/gpt-3.5-turbo</code> - Anthropic: <code>anthropic/claude-3-opus-20240229</code> - Azure: <code>azure/gpt-4</code> (requires API base configured in credentials) - Cohere: <code>cohere/command-r-plus</code> - Bedrock: <code>bedrock/anthropic.claude-v2</code></p>"},{"location":"litellm-setup/#testing-your-models","title":"Testing Your Models","text":"<p>After adding a model, test it in the LiteLLM UI:</p> <ol> <li>Navigate to Playground tab</li> <li>Select your model from the dropdown</li> <li>Enter a test prompt: \"Hello, are you working?\"</li> <li>Click Send</li> <li>Verify you get a response</li> </ol> <p>If you get an error: - Check that the provider credential is correct - Verify the model name format - Ensure your provider API key has access to that model - Review the error message in the LiteLLM logs</p>"},{"location":"litellm-setup/#authentication-flow","title":"Authentication Flow","text":""},{"location":"litellm-setup/#litellm_master_key","title":"LITELLM_MASTER_KEY","text":"<p>The <code>LITELLM_MASTER_KEY</code> is used for:</p> <ol> <li>Admin access to LiteLLM's management UI (<code>http://localhost:8002/ui</code>)</li> <li>SaaS API authentication when creating virtual keys for teams</li> <li>Direct LiteLLM API access (not recommended for end users)</li> </ol> <pre><code># In your .env file\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-key-here\n</code></pre>"},{"location":"litellm-setup/#virtual-team-keys","title":"Virtual Team Keys","text":"<p>The SaaS API creates virtual keys for each team automatically:</p> <ul> <li>Teams don't use <code>LITELLM_MASTER_KEY</code> directly</li> <li>Each team gets a unique key with budget/rate limits enforced</li> <li>Keys are managed through the SaaS API, not LiteLLM directly</li> </ul>"},{"location":"litellm-setup/#environment-configuration","title":"Environment Configuration","text":"<p>Set these required variables in your <code>.env</code> file:</p> <pre><code># LiteLLM Configuration\nLITELLM_MASTER_KEY=sk-litellm-$(openssl rand -hex 32)\nLITELLM_PROXY_URL=http://localhost:8002\nLITELLM_CONFIG_PATH=src/config/litellm_config.yaml\n\n# Database (shared with SaaS API and LiteLLM)\nDATABASE_URL=postgresql://postgres:postgres@localhost:5432/saas_llm_db\n\n# Redis (for caching and rate limiting)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=your-redis-password\n</code></pre> <p>Note: Provider API keys (OpenAI, Anthropic, etc.) are added through the LiteLLM UI, not as environment variables. This keeps credentials secure and allows dynamic management without restarts.</p>"},{"location":"litellm-setup/#complete-setup-workflow","title":"Complete Setup Workflow","text":"<p>Here's the complete workflow from zero to working SaaS LLM API:</p>"},{"location":"litellm-setup/#step-1-start-services","title":"Step 1: Start Services","text":"<pre><code>docker-compose up -d\n</code></pre> <p>Wait for all services to be healthy.</p>"},{"location":"litellm-setup/#step-2-configure-litellm-required","title":"Step 2: Configure LiteLLM (REQUIRED)","text":"<ol> <li>Access LiteLLM UI: http://localhost:8002/ui</li> <li>Login with your <code>LITELLM_MASTER_KEY</code></li> <li>Add Provider Credentials:</li> <li>Go to Keys tab</li> <li>Add OpenAI key (or your preferred provider)</li> <li>Example: Alias=<code>openai-main</code>, Provider=<code>openai</code>, Key=<code>sk-...</code></li> <li>Add Models:</li> <li>Go to Models tab</li> <li>Add at least one model</li> <li>Example: Name=<code>gpt-3.5-turbo</code>, LiteLLM Name=<code>openai/gpt-3.5-turbo</code>, Credential=<code>openai-main</code></li> <li>Test Model:</li> <li>Go to Playground tab</li> <li>Select your model and send a test message</li> <li>Verify you get a response</li> </ol>"},{"location":"litellm-setup/#step-3-configure-saas-api","title":"Step 3: Configure SaaS API","text":"<ol> <li>Access Admin Panel: http://localhost:3000</li> <li>Login with your <code>MASTER_KEY</code> (from SaaS API .env)</li> <li>Create Organization:</li> <li>Go to Organizations</li> <li>Create your first organization</li> <li>Create Model Group:</li> <li>Go to Model Groups</li> <li>Create a group with the models you added in LiteLLM</li> <li>Example: Name=<code>standard-models</code>, Models=<code>gpt-3.5-turbo, claude-3-sonnet</code></li> <li>Create Team:</li> <li>Go to Teams</li> <li>Create a team under your organization</li> <li>Assign the model group</li> <li>Set budget (credits)</li> </ol>"},{"location":"litellm-setup/#step-4-test-end-to-end","title":"Step 4: Test End-to-End","text":"<p>Your team now has a virtual key. Test the complete flow:</p> <pre><code># Get team info (includes virtual_key)\ncurl http://localhost:8003/api/teams/{team_id} \\\n  -H \"X-Admin-Key: $MASTER_KEY\"\n\n# Use team's virtual key to make LLM request\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $TEAM_VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre> <p>\u2705 If you get a response, your setup is complete!</p>"},{"location":"litellm-setup/#model-groups-saas-api-feature","title":"Model Groups (SaaS API Feature)","text":"<p>The SaaS API adds Model Groups on top of LiteLLM models:</p> <ul> <li>Group models together: Create logical groupings like \"fast-models\", \"premium-models\"</li> <li>Control team access: Assign specific model groups to teams</li> <li>Flexible pricing: Different teams can have different model access</li> </ul> <p>Important: Model names in your Model Groups must match the model names you configured in LiteLLM.</p> <p>Example workflow:</p> <ol> <li>\u2705 Add models in LiteLLM UI (e.g., <code>gpt-4-turbo</code>, <code>claude-3-sonnet</code>)</li> <li>Create model groups in SaaS API admin panel</li> <li>Add the same model names to your groups</li> <li>Assign groups to teams</li> <li>Teams can now use those models via their virtual keys</li> </ol>"},{"location":"litellm-setup/#testing-the-setup","title":"Testing the Setup","text":""},{"location":"litellm-setup/#1-test-litellm-directly","title":"1. Test LiteLLM Directly","text":"<p>Test that LiteLLM can reach your providers:</p> <pre><code>curl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $LITELLM_MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"litellm-setup/#2-test-saas-api-litellm-connection","title":"2. Test SaaS API \u2192 LiteLLM Connection","text":"<p>Create a test organization and team:</p> <pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Test Org\",\n    \"litellm_organization_id\": \"test-org\"\n  }'\n\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Test Team\",\n    \"organization_id\": \"org_...\",\n    \"litellm_team_id\": \"test-team\",\n    \"max_budget\": 100.0\n  }'\n</code></pre> <p>The SaaS API will create a virtual key in LiteLLM for the team.</p>"},{"location":"litellm-setup/#3-test-with-team-key","title":"3. Test with Team Key","text":"<p>Use the team's virtual key to make requests:</p> <pre><code>curl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $TEAM_VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from team!\"}]\n  }'\n</code></pre>"},{"location":"litellm-setup/#configuration-reference","title":"Configuration Reference","text":""},{"location":"litellm-setup/#litellm_configyaml-structure","title":"litellm_config.yaml Structure","text":"<pre><code># Model definitions (if not using database)\nmodel_list: []\n\n# General settings\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n  database_url: os.environ/DATABASE_URL\n  store_model_in_db: true  # Use database for model management\n\n  # Redis caching\n  cache: true\n  cache_params:\n    type: \"redis\"\n    host: os.environ/REDIS_HOST\n    port: os.environ/REDIS_PORT\n    password: os.environ/REDIS_PASSWORD\n    ttl: 600  # Cache TTL in seconds\n\n  # Cost tracking\n  track_cost_per_deployment: true\n\n# Router settings\nrouter_settings:\n  redis_host: os.environ/REDIS_HOST\n  redis_port: os.environ/REDIS_PORT\n  redis_password: os.environ/REDIS_PASSWORD\n  enable_pre_call_checks: true\n  routing_strategy: \"usage-based-routing\"\n  num_retries: 3\n  timeout: 600\n  redis_enabled: true\n</code></pre>"},{"location":"litellm-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"litellm-setup/#cant-access-litellm-ui","title":"Can't Access LiteLLM UI","text":"<p>Symptoms: Cannot reach http://localhost:8002/ui</p> <p>Solutions:</p> <ol> <li> <p>Verify LiteLLM container is running:    <pre><code>docker ps | grep litellm\n</code></pre></p> </li> <li> <p>Check that port 8002 is exposed in docker-compose.yml:    <pre><code>ports:\n  - \"8002:8002\"\n</code></pre></p> </li> <li> <p>View LiteLLM logs for errors:    <pre><code>docker logs litellm-proxy\n</code></pre></p> </li> <li> <p>Ensure no other service is using port 8002:    <pre><code>lsof -i :8002\n</code></pre></p> </li> </ol>"},{"location":"litellm-setup/#litellm-ui-login-failing","title":"LiteLLM UI Login Failing","text":"<p>Symptoms: Invalid credentials error when logging in</p> <p>Solutions:</p> <ol> <li>Verify you're using <code>LITELLM_MASTER_KEY</code> (not <code>MASTER_KEY</code>)</li> <li>Check the key value in your <code>.env</code> file</li> <li>Ensure the key doesn't have extra whitespace or quotes</li> <li>Try restarting LiteLLM: <code>docker-compose restart litellm-proxy</code></li> </ol>"},{"location":"litellm-setup/#provider-credential-not-working","title":"Provider Credential Not Working","text":"<p>Symptoms: \"Invalid API key\" errors when testing models</p> <p>Solutions:</p> <ol> <li>Verify the key is valid:</li> <li>Test directly with the provider (e.g., OpenAI playground)</li> <li>Check if the key has been revoked</li> <li> <p>Ensure you have sufficient credits with the provider</p> </li> <li> <p>Check key format:</p> </li> <li>OpenAI: Should start with <code>sk-...</code></li> <li>Anthropic: Should start with <code>sk-ant-...</code></li> <li> <p>Azure: Varies by deployment</p> </li> <li> <p>Verify provider selection:</p> </li> <li>Make sure you selected the correct provider in the dropdown</li> <li> <p>Provider name must match exactly (case-sensitive)</p> </li> <li> <p>Check API base (for Azure/custom endpoints):</p> </li> <li>Ensure the API base URL is correct</li> <li> <p>Verify the API version is supported</p> </li> <li> <p>Review LiteLLM logs:    <pre><code>docker logs litellm-proxy | grep -i error\n</code></pre></p> </li> </ol>"},{"location":"litellm-setup/#model-not-working","title":"Model Not Working","text":"<p>Symptoms: \"Model not found\" or model requests failing</p> <p>Solutions:</p> <ol> <li>Verify model exists in LiteLLM:</li> <li>Check the Models tab in LiteLLM UI</li> <li> <p>Ensure the model is saved (not just added)</p> </li> <li> <p>Check model name format:</p> </li> <li>Must use provider prefix: <code>openai/gpt-4-turbo</code> (not just <code>gpt-4-turbo</code>)</li> <li> <p>Check for typos (case-sensitive)</p> </li> <li> <p>Verify credential is linked:</p> </li> <li>Each model must be linked to a provider credential</li> <li> <p>The credential must be valid</p> </li> <li> <p>Test in playground:</p> </li> <li>Use LiteLLM's Playground tab</li> <li>Try a simple prompt</li> <li> <p>Review error messages</p> </li> <li> <p>Check provider access:</p> </li> <li>Ensure your API key has access to that specific model</li> <li>Some models require special access (e.g., GPT-4, Claude Opus)</li> </ol>"},{"location":"litellm-setup/#saas-api-cant-reach-litellm","title":"SaaS API Can't Reach LiteLLM","text":"<p>Symptoms: Errors when creating teams or model groups</p> <p>Solutions:</p> <ol> <li> <p>Verify <code>LITELLM_PROXY_URL</code> in SaaS API <code>.env</code>:    <pre><code># Should be\nLITELLM_PROXY_URL=http://localhost:8002\n# Or for Docker network\nLITELLM_PROXY_URL=http://litellm-proxy:8002\n</code></pre></p> </li> <li> <p>Test connectivity from SaaS API container:    <pre><code>docker exec saas-api curl http://litellm-proxy:8002/health\n</code></pre></p> </li> <li> <p>Check both services are on the same Docker network:    <pre><code>docker network inspect saas-litellm_default\n</code></pre></p> </li> <li> <p>Verify <code>LITELLM_MASTER_KEY</code> matches in both services' .env files</p> </li> </ol>"},{"location":"litellm-setup/#virtual-keys-not-working","title":"Virtual Keys Not Working","text":"<p>Symptoms: Teams can't use their assigned virtual keys</p> <p>Solutions:</p> <ol> <li>Verify key was created:</li> <li>Check LiteLLM UI Keys tab</li> <li> <p>Look for keys with team_id in metadata</p> </li> <li> <p>Check team budget:</p> </li> <li>View team details in SaaS API admin panel</li> <li>Verify credits haven't been exhausted</li> <li> <p>Check budget limits in LiteLLM</p> </li> <li> <p>Verify model access:</p> </li> <li>Ensure team's model group includes the requested model</li> <li> <p>Check model name matches exactly what's in LiteLLM</p> </li> <li> <p>Review rate limits:</p> </li> <li>Check if team has hit rate limits (RPM/TPM)</li> <li> <p>View limits in LiteLLM team configuration</p> </li> <li> <p>Check LiteLLM logs:    <pre><code>docker logs litellm-proxy | grep team_id\n</code></pre></p> </li> </ol>"},{"location":"litellm-setup/#model-group-mismatch","title":"Model Group Mismatch","text":"<p>Symptoms: Team can't access models despite having a model group assigned</p> <p>Solutions:</p> <ol> <li>Verify model names match:</li> <li>Model names in SaaS API Model Group MUST match LiteLLM model names</li> <li> <p>Check for typos, extra spaces, or case differences</p> </li> <li> <p>Confirm models exist in LiteLLM:</p> </li> <li>Open LiteLLM UI \u2192 Models tab</li> <li> <p>Verify each model in your group exists</p> </li> <li> <p>Re-create the team (if needed):</p> </li> <li>This refreshes the virtual key configuration in LiteLLM</li> <li>Ensures latest model group is applied</li> </ol>"},{"location":"litellm-setup/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never expose <code>LITELLM_MASTER_KEY</code> to end users</li> <li>Use strong, random keys for production:    <pre><code>openssl rand -hex 32\n</code></pre></li> <li>Rotate keys regularly (quarterly recommended)</li> <li>Set appropriate budgets on all teams to prevent runaway costs</li> <li>Monitor usage via LiteLLM dashboard</li> <li>Use rate limits to prevent abuse</li> <li>Enable Redis caching to reduce costs</li> </ol>"},{"location":"litellm-setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>LiteLLM Documentation</li> <li>LiteLLM Supported Providers</li> <li>LiteLLM GitHub Repository</li> <li>OpenAI API Reference</li> <li>Anthropic Claude API</li> </ul>"},{"location":"litellm-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Integration Guide - Complete authentication and connection flow</li> <li>API Reference - SaaS API endpoints</li> <li>Deployment Guide - Deploy to production</li> </ul>"},{"location":"admin-dashboard/authentication/","title":"Admin Dashboard Authentication","text":"<p>Learn how to authenticate with the Admin Dashboard using JWT tokens with role-based access control (RBAC) or legacy MASTER_KEY authentication.</p>"},{"location":"admin-dashboard/authentication/#overview","title":"Overview","text":"<p>The Admin Dashboard now uses JWT (JSON Web Token) authentication with role-based access control. This provides secure, session-based authentication with proper user management, audit logging, and granular permissions.</p> <p>Two authentication methods are supported:</p> <ol> <li>JWT Bearer Token (Preferred): Email/password login with role-based permissions</li> <li>Legacy X-Admin-Key (Backward Compatibility): Direct MASTER_KEY authentication</li> </ol> <p>JWT Authentication Benefits</p> <ul> <li>User Management: Create admin users with different roles (owner, admin, user)</li> <li>Session Tracking: Track active sessions, revoke access, monitor activity</li> <li>Audit Logging: Complete audit trail of all admin actions</li> <li>Role-Based Access Control: Fine-grained permissions based on user roles</li> <li>Secure: Password hashing, token expiration, session revocation</li> </ul> <p>Admin vs Team Authentication</p> <ul> <li>Admin JWT/X-Admin-Key: Admin access to dashboard and management APIs</li> <li>Virtual Keys: Team access to make LLM requests (via <code>Authorization: Bearer sk-team-key</code>)</li> </ul> <p>These are completely separate authentication systems!</p>"},{"location":"admin-dashboard/authentication/#new-jwt-authentication-system","title":"New JWT Authentication System","text":""},{"location":"admin-dashboard/authentication/#initial-setup","title":"Initial Setup","text":""},{"location":"admin-dashboard/authentication/#1-first-time-owner-account-creation","title":"1. First-Time Owner Account Creation","text":"<p>When no users exist, the setup endpoint allows creating the first owner account:</p> <p> First-time admin setup screen - create your owner account</p> <p>URL: <code>POST /api/admin-users/setup</code></p> <p>No authentication required (only works when no users exist)</p> <pre><code>curl -X POST http://localhost:8004/api/admin-users/setup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"password\": \"SecurePassword123!\"\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"bearer\",\n  \"user\": {\n    \"user_id\": \"...\",\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"role\": \"owner\",\n    \"is_active\": true,\n    \"created_at\": \"2025-10-15T...\",\n    \"last_login\": \"2025-10-15T...\"\n  }\n}\n</code></pre>"},{"location":"admin-dashboard/authentication/#2-login-flow","title":"2. Login Flow","text":"<p>After setup, users login with email/password:</p> <p>URL: <code>POST /api/admin-users/login</code></p> <pre><code>curl -X POST http://localhost:8004/api/admin-users/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@example.com\",\n    \"password\": \"SecurePassword123!\"\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"bearer\",\n  \"user\": {\n    \"user_id\": \"...\",\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"role\": \"owner\",\n    \"is_active\": true\n  }\n}\n</code></pre> <p>Store the <code>access_token</code> and include it in subsequent requests:</p> <pre><code>curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\" \\\n  http://localhost:8004/api/admin-users/me\n</code></pre>"},{"location":"admin-dashboard/authentication/#user-roles","title":"User Roles","text":"<p>Three roles with different permission levels:</p> Role Permissions owner Full access to all features, can create/modify/delete any user admin Manage organizations, teams, models, credits. Can create 'user' role accounts but not owners/admins user Read-only access to dashboard, can view but not modify resources"},{"location":"admin-dashboard/authentication/#role-based-endpoint-access","title":"Role-Based Endpoint Access","text":"<pre><code># All admin users can access (owner, admin, user)\nGET /api/admin-users/me\n\n# Owner and admin can access\nGET /api/admin-users\nPOST /api/admin-users\nPUT /api/admin-users/{id}\nDELETE /api/admin-users/{id}\n\n# Role restrictions enforced:\n# - Admins can only create \"user\" role\n# - Admins cannot modify owner/admin users\n# - Owners have no restrictions\n</code></pre>"},{"location":"admin-dashboard/authentication/#admin-dashboard-login","title":"Admin Dashboard Login","text":""},{"location":"admin-dashboard/authentication/#1-navigate-to-dashboard","title":"1. Navigate to Dashboard","text":"<p>Open the admin dashboard URL:</p> <pre><code>http://localhost:3002  (local)\nhttps://admin.your-saas.com  (production)\n</code></pre>"},{"location":"admin-dashboard/authentication/#2-setup-detection","title":"2. Setup Detection","text":"<p>The dashboard automatically checks if setup is needed:</p> <pre><code>GET /api/admin-users/setup/status\n</code></pre> <p>Response: <pre><code>{\n  \"needs_setup\": false,\n  \"has_users\": true\n}\n</code></pre></p> <p>If <code>needs_setup: true</code>, the dashboard shows the setup form. Otherwise, shows login.</p>"},{"location":"admin-dashboard/authentication/#3-loginsetup","title":"3. Login/Setup","text":"<p>Setup Screen (first time): - Enter email, display name, and password - Creates owner account automatically - Returns JWT token and redirects to dashboard</p> <p>Login Screen (subsequent): - Enter email and password - Validates credentials and returns JWT token - Token stored in localStorage - Redirects to dashboard</p>"},{"location":"admin-dashboard/authentication/#4-authenticated-requests","title":"4. Authenticated Requests","text":"<p>All subsequent API requests include the JWT token:</p> <pre><code>// admin-panel/lib/api-client.ts\nconst token = localStorage.getItem('authToken');\n\nconst response = await fetch(`${API_URL}${endpoint}`, {\n  headers: {\n    'Authorization': `Bearer ${token}`,\n    'Content-Type': 'application/json',\n  },\n});\n</code></pre>"},{"location":"admin-dashboard/authentication/#legacy-x-admin-key-authentication","title":"Legacy X-Admin-Key Authentication","text":"<p>For backward compatibility, the old MASTER_KEY authentication still works:</p> <pre><code>curl -H \"X-Admin-Key: sk-admin-local-dev-change-in-production\" \\\n  http://localhost:8004/api/teams\n</code></pre> <p>Use Cases for Legacy Auth: - Existing scripts/integrations not yet updated - CI/CD pipelines - Server-to-server communication - Temporary admin access</p> <p>Migration Path: 1. Create admin user accounts via JWT system 2. Update scripts to use JWT tokens 3. Eventually deprecate X-Admin-Key (not yet scheduled)</p>"},{"location":"admin-dashboard/authentication/#protected-endpoints","title":"Protected Endpoints","text":""},{"location":"admin-dashboard/authentication/#jwt-only-endpoints","title":"JWT-Only Endpoints","text":"<p>User management requires JWT authentication:</p> <pre><code># List users (requires owner or admin role)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users\n\n# Create user (requires owner or admin role)\ncurl -X POST http://localhost:8004/api/admin-users \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"newuser@example.com\",\n    \"display_name\": \"New User\",\n    \"password\": \"SecurePass123!\",\n    \"role\": \"user\"\n  }'\n\n# Get current user info\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users/me\n\n# Logout (revokes current session)\ncurl -X POST http://localhost:8004/api/admin-users/logout \\\n  -H \"Authorization: Bearer $TOKEN\"\n\n# View audit logs (requires owner or admin role)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users/audit-logs\n</code></pre>"},{"location":"admin-dashboard/authentication/#dual-authentication-endpoints","title":"Dual Authentication Endpoints","text":"<p>Management endpoints support both JWT and X-Admin-Key:</p>"},{"location":"admin-dashboard/authentication/#organization-management","title":"Organization Management","text":"<pre><code># JWT (preferred)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/organizations\n\n# Legacy X-Admin-Key (still works)\ncurl -H \"X-Admin-Key: sk-admin-local-dev-change-in-production\" \\\n  http://localhost:8004/api/organizations\n</code></pre>"},{"location":"admin-dashboard/authentication/#team-management","title":"Team Management","text":"<pre><code># JWT\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/teams\n\n# Legacy\ncurl -H \"X-Admin-Key: $MASTER_KEY\" \\\n  http://localhost:8004/api/teams\n</code></pre>"},{"location":"admin-dashboard/authentication/#model-group-management","title":"Model Group Management","text":"<pre><code># JWT\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/model-groups\n\n# Legacy\ncurl -H \"X-Admin-Key: $MASTER_KEY\" \\\n  http://localhost:8004/api/model-groups\n</code></pre>"},{"location":"admin-dashboard/authentication/#credit-management","title":"Credit Management","text":"<pre><code># JWT\ncurl -X POST http://localhost:8004/api/credits/teams/team_engineering/add \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"credits\": 100, \"reason\": \"Monthly allocation\"}'\n\n# Legacy\ncurl -X POST http://localhost:8004/api/credits/teams/team_engineering/add \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"credits\": 100, \"reason\": \"Monthly allocation\"}'\n</code></pre>"},{"location":"admin-dashboard/authentication/#session-management","title":"Session Management","text":""},{"location":"admin-dashboard/authentication/#jwt-token-details","title":"JWT Token Details","text":"<ul> <li>Expiration: 24 hours from creation</li> <li>Storage: Database session tracking with token hash</li> <li>Revocation: Sessions can be revoked via logout endpoint</li> <li>Validation: Every request checks session validity</li> </ul>"},{"location":"admin-dashboard/authentication/#session-tracking","title":"Session Tracking","text":"<p>Each login creates a session record:</p> <pre><code># Database: admin_sessions table\nsession = AdminSession(\n    user_id=user.user_id,\n    token_hash=create_token_hash(token),  # SHA256 hash\n    expires_at=datetime.utcnow() + timedelta(hours=24),\n    ip_address=request.client.host,\n    user_agent=request.headers.get(\"user-agent\"),\n    is_revoked=False\n)\n</code></pre>"},{"location":"admin-dashboard/authentication/#auto-logout","title":"Auto-Logout","text":"<p>The frontend automatically logs out on: - JWT token expiration (401 response) - Session revocation - Manual logout</p> <pre><code>// admin-panel/lib/api-client.ts\nif (response.status === 401) {\n  localStorage.removeItem('authToken');\n  localStorage.removeItem('user');\n  window.location.href = '/login';\n}\n</code></pre>"},{"location":"admin-dashboard/authentication/#audit-logging","title":"Audit Logging","text":"<p>All admin actions are logged:</p> <pre><code># View audit logs\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8004/api/admin-users/audit-logs?limit=50\"\n</code></pre> <p>Logged Actions: - setup_owner, login, logout - created_user, updated_user, deleted_user - changed_password - All actions include IP address, timestamp, user_id, and details</p> <p>Response:</p> <pre><code>[\n  {\n    \"audit_id\": \"...\",\n    \"user_id\": \"...\",\n    \"action\": \"created_user\",\n    \"resource_type\": \"admin_user\",\n    \"resource_id\": \"...\",\n    \"details\": {\n      \"email\": \"newuser@example.com\",\n      \"role\": \"user\"\n    },\n    \"ip_address\": \"127.0.0.1\",\n    \"created_at\": \"2025-10-15T...\"\n  }\n]\n</code></pre>"},{"location":"admin-dashboard/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"admin-dashboard/authentication/#1-strong-passwords","title":"1. Strong Passwords","text":"<p>Enforce password requirements: - Minimum 8 characters - Mix of uppercase, lowercase, numbers, symbols - Not common/dictionary words - Unique per user</p>"},{"location":"admin-dashboard/authentication/#2-jwt-secret-key","title":"2. JWT Secret Key","text":"<p>The JWT secret is derived from MASTER_KEY:</p> <pre><code># .env\nMASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\n\n# JWT secret is: MASTER_KEY value\n# Tokens signed with HS256 algorithm\n</code></pre> <p>Generate secure MASTER_KEY: <pre><code>openssl rand -hex 32\n# Use output: sk-admin-&lt;generated-hex&gt;\n</code></pre></p>"},{"location":"admin-dashboard/authentication/#3-environment-separation","title":"3. Environment Separation","text":"<p>Use different keys for each environment:</p> <pre><code># Development\nMASTER_KEY=sk-admin-dev-key-here\n\n# Staging\nMASTER_KEY=sk-admin-staging-key-here\n\n# Production\nMASTER_KEY=sk-admin-prod-key-here\n</code></pre>"},{"location":"admin-dashboard/authentication/#4-user-management","title":"4. User Management","text":"<p>Best Practices: - Create individual accounts for each admin - Use appropriate roles (don't make everyone owner) - Deactivate users when they leave - Regular audit log reviews - Password rotation policy</p>"},{"location":"admin-dashboard/authentication/#5-session-security","title":"5. Session Security","text":"<ul> <li>HTTPS only in production</li> <li>Short token expiration (24 hours)</li> <li>Session revocation on logout</li> <li>IP and user agent tracking</li> <li>Monitor for suspicious activity</li> </ul>"},{"location":"admin-dashboard/authentication/#setting-up-master_key","title":"Setting Up MASTER_KEY","text":""},{"location":"admin-dashboard/authentication/#local-development","title":"Local Development","text":"<ol> <li> <p>Copy <code>.env.example</code> to <code>.env</code>:    <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Edit <code>.env</code> and set MASTER_KEY:    <pre><code>MASTER_KEY=sk-admin-local-dev-change-in-production\n</code></pre></p> </li> <li> <p>Restart the SaaS API:    <pre><code>python3 -m uvicorn src.saas_api:app --port 8004 --reload\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/authentication/#production-railway","title":"Production (Railway)","text":"<ol> <li>Go to Railway Dashboard:</li> <li>Select your SaaS API service</li> <li> <p>Navigate to \"Variables\" tab</p> </li> <li> <p>Add MASTER_KEY variable:    <pre><code>MASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\n</code></pre></p> </li> <li> <p>Generate a secure key:    <pre><code>openssl rand -hex 32\n# Use output: sk-admin-&lt;generated-hex&gt;\n</code></pre></p> </li> <li> <p>Deploy:</p> </li> <li>Railway auto-deploys on variable changes</li> </ol>"},{"location":"admin-dashboard/authentication/#admin-panel-configuration","title":"Admin Panel Configuration","text":"<p>The admin panel needs the API URL:</p> <pre><code># admin-panel/.env.local\nNEXT_PUBLIC_API_URL=http://localhost:8004\n</code></pre>"},{"location":"admin-dashboard/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/authentication/#invalid-credentials-on-login","title":"\"Invalid credentials\" on Login","text":"<p>Problem: Login fails with invalid credentials</p> <p>Solutions: 1. Verify email is correct (case-sensitive) 2. Check password (minimum 8 characters) 3. Ensure user account exists and is active 4. Check if setup is needed: <code>GET /api/admin-users/setup/status</code></p>"},{"location":"admin-dashboard/authentication/#setup-already-completed","title":"\"Setup already completed\"","text":"<p>Problem: Trying to access setup when users exist</p> <p>Solution: Use the login endpoint instead of setup</p>"},{"location":"admin-dashboard/authentication/#token-expired-error","title":"\"Token expired\" Error","text":"<p>Problem: JWT token has expired (24 hour lifetime)</p> <p>Solution: Login again to get a new token</p>"},{"location":"admin-dashboard/authentication/#insufficient-permissions","title":"\"Insufficient permissions\"","text":"<p>Problem: User role doesn't have access to endpoint</p> <p>Solutions: 1. Check user role: <code>GET /api/admin-users/me</code> 2. Verify role requirements for endpoint 3. Contact owner to upgrade role if needed</p>"},{"location":"admin-dashboard/authentication/#legacy-auth-not-working","title":"Legacy Auth Not Working","text":"<p>Problem: X-Admin-Key authentication fails</p> <p>Solutions: 1. Verify MASTER_KEY is set in environment 2. Check for typos in header name: <code>X-Admin-Key</code> 3. Restart API after changing MASTER_KEY 4. Test with curl:    <pre><code>curl -H \"X-Admin-Key: $MASTER_KEY\" \\\n  http://localhost:8004/api/model-groups\n</code></pre></p>"},{"location":"admin-dashboard/authentication/#migration-from-legacy-to-jwt","title":"Migration from Legacy to JWT","text":""},{"location":"admin-dashboard/authentication/#step-1-create-admin-users","title":"Step 1: Create Admin Users","text":"<pre><code># Setup owner account (first time)\ncurl -X POST http://localhost:8004/api/admin-users/setup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"password\": \"SecurePassword123!\"\n  }'\n\n# Create additional admin users\ncurl -X POST http://localhost:8004/api/admin-users \\\n  -H \"Authorization: Bearer $OWNER_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"user@example.com\",\n    \"display_name\": \"Team User\",\n    \"password\": \"SecurePass456!\",\n    \"role\": \"admin\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/authentication/#step-2-update-scripts","title":"Step 2: Update Scripts","text":"<p>Replace X-Admin-Key with JWT tokens:</p> <pre><code># Old method\nheaders = {\n    \"X-Admin-Key\": \"sk-admin-local-dev-change-in-production\"\n}\n\n# New method\n# 1. Login once\nlogin_response = requests.post(\n    \"http://localhost:8004/api/admin-users/login\",\n    json={\n        \"email\": \"admin@example.com\",\n        \"password\": \"SecurePassword123!\"\n    }\n)\ntoken = login_response.json()[\"access_token\"]\n\n# 2. Use token for subsequent requests\nheaders = {\n    \"Authorization\": f\"Bearer {token}\"\n}\n</code></pre>"},{"location":"admin-dashboard/authentication/#step-3-update-dashboard","title":"Step 3: Update Dashboard","text":"<p>The admin panel automatically supports JWT. Just login with email/password.</p>"},{"location":"admin-dashboard/authentication/#step-4-monitor-legacy-usage","title":"Step 4: Monitor Legacy Usage","text":"<p>Check audit logs to see if X-Admin-Key is still being used:</p> <pre><code># JWT requests are logged with user_id\n# Legacy requests have no user_id in logs\n</code></pre>"},{"location":"admin-dashboard/authentication/#testing","title":"Testing","text":"<p>The JWT authentication system has been thoroughly tested with comprehensive integration tests:</p> <p>Test Script: <code>scripts/test_jwt_integration.py</code></p> <p>What's Tested: - Setup/login flows - JWT and legacy authentication - Dual auth on all endpoints - Role-based access control - Security features - Session management - Audit logging</p> <p>Test Results: All 12 integration tests passed (100% success rate)</p> <p>For detailed test information, see: - Integration Tests Documentation - How to run the tests - Integration Test Results - Detailed test results</p>"},{"location":"admin-dashboard/authentication/#next-steps","title":"Next Steps","text":"<p>Now that you understand admin authentication:</p> <ol> <li>User Management - Manage admin users and roles</li> <li>Manage Organizations - Create and configure organizations</li> <li>Manage Teams - Set up teams with virtual keys</li> <li>Configure Model Access - Control model permissions</li> <li>Monitor Usage - Track system activity</li> </ol>"},{"location":"admin-dashboard/authentication/#additional-resources","title":"Additional Resources","text":"<ul> <li>Environment Variables Guide - Complete env var reference</li> <li>Team Authentication - How teams authenticate (different from admin)</li> <li>API Reference: Admin Users - Complete API documentation</li> <li>Integration Tests - Testing the authentication system</li> </ul>"},{"location":"admin-dashboard/credits/","title":"Credits","text":"<p>Learn how the credit system works and how to allocate credits to teams.</p>"},{"location":"admin-dashboard/credits/#how-credits-work","title":"How Credits Work","text":"<p>SaaS LiteLLM uses a credit-based billing system built on top of LiteLLM to simplify cost tracking:</p> <ul> <li>1 credit = 1 completed job (regardless of how many LLM calls the job makes)</li> <li>Credits are allocated per team</li> <li>Credits are only deducted when a job completes successfully</li> <li>Failed jobs don't consume credits</li> </ul> <p>Built on LiteLLM</p> <p>While LiteLLM tracks token-based costs from providers, SaaS LiteLLM abstracts this into simple credit-based billing. This allows you to set predictable pricing for your clients while tracking actual provider costs internally.</p>"},{"location":"admin-dashboard/credits/#why-credits-instead-of-tokens","title":"Why Credits Instead of Tokens?","text":"<p>Traditional Token Billing: <pre><code>- Document analysis: 2,345 tokens = $0.0234\n- Chat session: 5,123 tokens = $0.0512\n- Data extraction: 1,234 tokens = $0.0123\n</code></pre> \u274c Complex for clients to predict costs \u274c Varies based on input/output length \u274c Hard to budget</p> <p>Credit Billing: <pre><code>- Document analysis: 1 credit\n- Chat session: 1 credit\n- Data extraction: 1 credit\n</code></pre> \u2705 Predictable costs \u2705 Easy to understand \u2705 Simple budgeting</p>"},{"location":"admin-dashboard/credits/#credit-allocation","title":"Credit Allocation","text":""},{"location":"admin-dashboard/credits/#initial-allocation","title":"Initial Allocation","text":"<p>When creating a team, specify initial credits:</p> <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_client\",\n    \"team_id\": \"client-prod\",\n    \"team_alias\": \"Production\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 1000\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#adding-credits","title":"Adding Credits","text":"<p>Add credits to an existing team:</p> <p>Via Dashboard: 1. Navigate to Teams \u2192 Select team 2. Click \"Add Credits\" 3. Enter amount 4. Click \"Add\"</p> <p>Via API: <pre><code>curl -X POST http://localhost:8003/api/credits/add \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"client-prod\",\n    \"amount\": 500,\n    \"description\": \"Monthly credit top-up - November 2024\"\n  }'\n</code></pre></p> <p>Response: <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"credits_added\": 500,\n  \"credits_remaining\": 1250,\n  \"transaction_id\": \"txn_abc123\"\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#checking-credit-balance","title":"Checking Credit Balance","text":""},{"location":"admin-dashboard/credits/#via-dashboard","title":"Via Dashboard","text":"<p>Navigate to Teams \u2192 Select team \u2192 See credits displayed</p>"},{"location":"admin-dashboard/credits/#via-api","title":"Via API","text":"<pre><code>curl \"http://localhost:8003/api/credits/balance?team_id=client-prod\"\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"credits_allocated\": 1500,\n  \"credits_remaining\": 750,\n  \"credits_used\": 750,\n  \"percentage_used\": 50.0,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#credit-deduction-logic","title":"Credit Deduction Logic","text":""},{"location":"admin-dashboard/credits/#when-credits-are-deducted","title":"When Credits Are Deducted","text":"<p>Credits are deducted only when a job completes successfully:</p> <pre><code># Create job - No credits deducted yet\njob = create_job(\"document_analysis\")\n\n# Make LLM calls - No credits deducted yet\nllm_call(job_id, messages)\nllm_call(job_id, messages)\nllm_call(job_id, messages)\n\n# Complete job - NOW 1 credit is deducted\ncomplete_job(job_id, \"completed\")\n</code></pre>"},{"location":"admin-dashboard/credits/#failed-jobs-dont-cost-credits","title":"Failed Jobs Don't Cost Credits","text":"<pre><code># Create job\njob = create_job(\"analysis\")\n\ntry:\n    # Make LLM call that fails\n    llm_call(job_id, messages)\nexcept Exception as e:\n    # Complete as failed\n    complete_job(job_id, \"failed\")\n    # No credits deducted! \u2705\n</code></pre>"},{"location":"admin-dashboard/credits/#multiple-calls-one-credit","title":"Multiple Calls = One Credit","text":"<p>The beauty of job-based billing:</p> <pre><code># One job with 5 LLM calls = 1 credit\njob = create_job(\"complex_analysis\")\n\nextract_text(job_id)      # Call 1\nclassify(job_id)          # Call 2\nsummarize(job_id)         # Call 3\ngenerate_insights(job_id) # Call 4\nquality_check(job_id)     # Call 5\n\ncomplete_job(job_id, \"completed\")\n# Total cost: 1 credit (not 5!)\n</code></pre>"},{"location":"admin-dashboard/credits/#credit-transactions","title":"Credit Transactions","text":""},{"location":"admin-dashboard/credits/#view-transaction-history","title":"View Transaction History","text":"<p>Track all credit additions and deductions:</p> <pre><code>curl \"http://localhost:8003/api/credits/transactions?team_id=client-prod&amp;limit=10\"\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"transactions\": [\n    {\n      \"transaction_id\": \"txn_abc123\",\n      \"type\": \"addition\",\n      \"amount\": 500,\n      \"description\": \"Monthly credit top-up\",\n      \"timestamp\": \"2024-10-14T10:00:00Z\",\n      \"balance_after\": 1250\n    },\n    {\n      \"transaction_id\": \"txn_abc122\",\n      \"type\": \"deduction\",\n      \"amount\": 1,\n      \"description\": \"Job: job_xyz123 completed\",\n      \"job_id\": \"job_xyz123\",\n      \"timestamp\": \"2024-10-14T09:30:00Z\",\n      \"balance_after\": 750\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#low-credit-alerts","title":"Low Credit Alerts","text":""},{"location":"admin-dashboard/credits/#setting-up-alerts","title":"Setting Up Alerts","text":"<p>Monitor teams approaching zero credits:</p> <pre><code># Check if team needs alert\ndef check_credit_alerts(team_id):\n    balance = get_credit_balance(team_id)\n\n    allocated = balance['credits_allocated']\n    remaining = balance['credits_remaining']\n    percentage = (remaining / allocated) * 100\n\n    # Alert at 20% remaining\n    if percentage &lt;= 20 and percentage &gt; 10:\n        send_alert(\"low_credits\", team_id, remaining)\n\n    # Critical alert at 10%\n    elif percentage &lt;= 10:\n        send_alert(\"critical_credits\", team_id, remaining)\n</code></pre>"},{"location":"admin-dashboard/credits/#automated-top-ups","title":"Automated Top-ups","text":"<p>Set up automatic credit additions:</p> <pre><code># Auto top-up when credits hit threshold\ndef auto_topup_check(team_id):\n    balance = get_credit_balance(team_id)\n\n    if balance['credits_remaining'] &lt; 100:\n        # Add predefined amount\n        add_credits(team_id, 1000, \"Automatic monthly top-up\")\n        notify_client(team_id, \"Credits topped up to 1000\")\n</code></pre>"},{"location":"admin-dashboard/credits/#pricing-strategies","title":"Pricing Strategies","text":""},{"location":"admin-dashboard/credits/#strategy-1-credit-packages","title":"Strategy 1: Credit Packages","text":"<p>Sell credits in packages:</p> <pre><code>Starter: 1,000 credits = $99/month\nProfessional: 5,000 credits = $399/month\nEnterprise: 20,000 credits = $1,299/month\n</code></pre> <p>Implementation: <pre><code># Starter plan\ncurl -X POST http://localhost:8003/api/credits/add \\\n  -d '{\"team_id\": \"client-prod\", \"amount\": 1000}'\n\n# Professional plan\ncurl -X POST http://localhost:8003/api/credits/add \\\n  -d '{\"team_id\": \"client-prod\", \"amount\": 5000}'\n</code></pre></p>"},{"location":"admin-dashboard/credits/#strategy-2-pay-as-you-go","title":"Strategy 2: Pay-As-You-Go","text":"<p>Charge per credit used:</p> <pre><code>$0.10 per credit\nMinimum purchase: 100 credits ($10)\n</code></pre> <p>Track Actual Costs: <pre><code># Get usage for billing\ncurl \"http://localhost:8003/api/teams/client-prod/usage?period=2024-10\"\n\n# Response includes:\n# - credits_used: 534\n# - Your charge: 534 \u00d7 $0.10 = $53.40\n# - Actual LiteLLM cost: $45.67 (internal tracking)\n# - Your profit: $7.73\n</code></pre></p>"},{"location":"admin-dashboard/credits/#strategy-3-subscription-overage","title":"Strategy 3: Subscription + Overage","text":"<p>Base subscription with overage charges:</p> <pre><code>Plan: $99/month includes 1,000 credits\nOverage: $0.08 per additional credit\n</code></pre>"},{"location":"admin-dashboard/credits/#strategy-4-tiered-pricing","title":"Strategy 4: Tiered Pricing","text":"<p>Volume discounts:</p> <pre><code>First 1,000 credits: $0.10 each\nNext 4,000 credits: $0.08 each\nOver 5,000 credits: $0.06 each\n</code></pre>"},{"location":"admin-dashboard/credits/#budget-modes","title":"Budget Modes","text":""},{"location":"admin-dashboard/credits/#mode-1-hard-limit-default","title":"Mode 1: Hard Limit (Default)","text":"<p>Team cannot exceed allocated credits:</p> <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"budget_mode\": \"hard_limit\",\n  \"credits_allocated\": 1000\n}\n</code></pre> <p>Behavior: - Job creation succeeds - LLM calls succeed - Job completion fails if credits exhausted - API returns 403 \"Insufficient credits\"</p>"},{"location":"admin-dashboard/credits/#mode-2-soft-limit-with-alerts","title":"Mode 2: Soft Limit with Alerts","text":"<p>Allow overage with notifications:</p> <pre><code>{\n  \"team_id\": \"client-prod\",\n  \"budget_mode\": \"soft_limit\",\n  \"credits_allocated\": 1000,\n  \"alert_at_percentage\": 80\n}\n</code></pre> <p>Behavior: - Can exceed allocated credits - Alerts sent at 80%, 100%, 120% - Track overage separately for billing</p>"},{"location":"admin-dashboard/credits/#mode-3-unlimited-enterprise","title":"Mode 3: Unlimited (Enterprise)","text":"<p>No credit limits:</p> <pre><code>{\n  \"team_id\": \"enterprise-prod\",\n  \"budget_mode\": \"unlimited\"\n}\n</code></pre> <p>Behavior: - No credit checks - Track usage for billing - Typically for enterprise contracts</p>"},{"location":"admin-dashboard/credits/#credit-replenishment","title":"Credit Replenishment","text":"<p>Add credits to teams from payments (subscriptions or one-time purchases).</p>"},{"location":"admin-dashboard/credits/#replenishing-from-payment","title":"Replenishing from Payment","text":"<p>When a client pays for credits, use the replenish endpoint:</p> <p>API Endpoint: <code>POST /api/credits/teams/{team_id}/replenish</code></p> <p>Authentication: Admin only (JWT Bearer token or X-Admin-Key header)</p> <pre><code>curl -X POST http://localhost:8003/api/credits/teams/acme-prod/replenish \\\n  -H \"Authorization: Bearer your-admin-jwt\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"credits\": 5000,\n    \"payment_type\": \"subscription\",\n    \"payment_amount_usd\": 499.00,\n    \"reason\": \"November 2024 subscription payment\"\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"credits_added\": 5000,\n  \"credits_before\": 1250,\n  \"credits_after\": 6250,\n  \"payment_type\": \"subscription\",\n  \"payment_amount_usd\": 499.00,\n  \"transaction\": {\n    \"transaction_id\": \"txn_abc123\",\n    \"transaction_type\": \"subscription_payment\",\n    \"credits_amount\": 5000,\n    \"credits_before\": 1250,\n    \"credits_after\": 6250,\n    \"reason\": \"November 2024 subscription payment ($499.00 USD)\",\n    \"created_at\": \"2024-11-01T00:00:00Z\"\n  }\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#payment-types","title":"Payment Types","text":"<p>Subscription Payment: <pre><code>{\n  \"payment_type\": \"subscription\",\n  \"credits\": 5000,\n  \"payment_amount_usd\": 499.00\n}\n</code></pre> - Recurring monthly/annual payments - Updates <code>last_refill_at</code> timestamp - Creates <code>subscription_payment</code> transaction</p> <p>One-Time Payment: <pre><code>{\n  \"payment_type\": \"one_time\",\n  \"credits\": 1000,\n  \"payment_amount_usd\": 99.00\n}\n</code></pre> - Ad-hoc credit purchases - Top-ups or overages - Creates <code>one_time_payment</code> transaction</p>"},{"location":"admin-dashboard/credits/#auto-refill-configuration","title":"Auto-Refill Configuration","text":"<p>Set up automatic credit refills tied to subscription billing:</p> <p>API Endpoint: <code>POST /api/credits/teams/{team_id}/configure-auto-refill</code></p> <pre><code>curl -X POST http://localhost:8003/api/credits/teams/acme-prod/configure-auto-refill \\\n  -H \"Authorization: Bearer your-admin-jwt\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"enabled\": true,\n    \"refill_amount\": 5000,\n    \"refill_period\": \"monthly\"\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"auto_refill_enabled\": true,\n  \"refill_amount\": 5000,\n  \"refill_period\": \"monthly\",\n  \"last_refill_at\": \"2024-11-01T00:00:00Z\",\n  \"message\": \"Auto-refill enabled successfully\"\n}\n</code></pre></p> <p>Refill Periods: - <code>monthly</code> - Refill once per month - <code>weekly</code> - Refill once per week - <code>daily</code> - Refill once per day</p>"},{"location":"admin-dashboard/credits/#disabling-auto-refill","title":"Disabling Auto-Refill","text":"<pre><code>curl -X POST http://localhost:8003/api/credits/teams/acme-prod/configure-auto-refill \\\n  -H \"Authorization: Bearer your-admin-jwt\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"enabled\": false\n  }'\n</code></pre>"},{"location":"admin-dashboard/credits/#integration-with-payment-processors","title":"Integration with Payment Processors","text":"<p>Example: Stripe Webhook Handler</p> <pre><code>import requests\nfrom datetime import datetime\n\ndef handle_stripe_subscription_payment(event):\n    \"\"\"Handle successful subscription payment from Stripe\"\"\"\n    subscription = event['data']['object']\n    customer_id = subscription['customer']\n    amount_paid = subscription['latest_invoice']['amount_paid'] / 100  # cents to dollars\n\n    # Map Stripe customer to team_id\n    team_id = get_team_id_from_stripe_customer(customer_id)\n\n    # Calculate credits based on plan\n    credits_to_add = calculate_credits_from_amount(amount_paid)\n\n    # Replenish credits\n    response = requests.post(\n        f\"http://localhost:8003/api/credits/teams/{team_id}/replenish\",\n        headers={\"Authorization\": f\"Bearer {ADMIN_JWT}\"},\n        json={\n            \"credits\": credits_to_add,\n            \"payment_type\": \"subscription\",\n            \"payment_amount_usd\": amount_paid,\n            \"reason\": f\"Stripe subscription payment - {subscription['id']}\"\n        }\n    )\n\n    return response.json()\n</code></pre>"},{"location":"admin-dashboard/credits/#transaction-tracking","title":"Transaction Tracking","text":"<p>All replenishments create audit trail transactions:</p> <p>View Replenishment History: <pre><code>curl \"http://localhost:8003/api/credits/teams/acme-prod/transactions?limit=50\" \\\n  -H \"Authorization: Bearer sk-virtual-key\"\n</code></pre></p> <p>Response showing replenishment transactions: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"total\": 3,\n  \"transactions\": [\n    {\n      \"transaction_id\": \"txn_abc123\",\n      \"transaction_type\": \"subscription_payment\",\n      \"credits_amount\": 5000,\n      \"credits_before\": 1250,\n      \"credits_after\": 6250,\n      \"reason\": \"November 2024 subscription payment ($499.00 USD)\",\n      \"created_at\": \"2024-11-01T00:00:00Z\"\n    },\n    {\n      \"transaction_id\": \"txn_abc122\",\n      \"transaction_type\": \"one_time_payment\",\n      \"credits_amount\": 1000,\n      \"credits_before\": 250,\n      \"credits_after\": 1250,\n      \"reason\": \"Additional credits purchase ($99.00 USD)\",\n      \"created_at\": \"2024-10-15T10:30:00Z\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#replenishment-best-practices","title":"Replenishment Best Practices","text":"<ol> <li>Track Payment References</li> <li>Include payment processor ID in reason field</li> <li> <p>Enables reconciliation between payments and credits</p> </li> <li> <p>Validate Before Replenishing</p> </li> <li>Verify payment completed successfully</li> <li>Check for duplicate webhook events</li> <li> <p>Implement idempotency</p> </li> <li> <p>Monitor Refill Timing</p> </li> <li>Check <code>last_refill_at</code> timestamp</li> <li>Prevent duplicate monthly refills</li> <li> <p>Handle edge cases (failed payments, cancellations)</p> </li> <li> <p>Automate Subscription Refills</p> </li> <li>Enable auto-refill for subscription customers</li> <li>Configure appropriate refill_period</li> <li>Decouple from manual payment processing</li> </ol>"},{"location":"admin-dashboard/credits/#example-workflow-monthly-subscription","title":"Example Workflow: Monthly Subscription","text":"<pre><code># 1. Customer subscribes to $99/month plan (1000 credits)\nteam_id = \"customer-prod\"\nmonthly_credits = 1000\n\n# 2. Configure auto-refill\nrequests.post(\n    f\"http://localhost:8003/api/credits/teams/{team_id}/configure-auto-refill\",\n    headers={\"Authorization\": f\"Bearer {ADMIN_JWT}\"},\n    json={\n        \"enabled\": True,\n        \"refill_amount\": monthly_credits,\n        \"refill_period\": \"monthly\"\n    }\n)\n\n# 3. On payment success (e.g., Stripe webhook)\ndef on_payment_success(amount_usd, team_id):\n    requests.post(\n        f\"http://localhost:8003/api/credits/teams/{team_id}/replenish\",\n        headers={\"Authorization\": f\"Bearer {ADMIN_JWT}\"},\n        json={\n            \"credits\": monthly_credits,\n            \"payment_type\": \"subscription\",\n            \"payment_amount_usd\": amount_usd,\n            \"reason\": f\"Monthly subscription - {datetime.now().strftime('%B %Y')}\"\n        }\n    )\n\n# 4. Customer usage tracked normally\n# 5. Next month, repeat step 3 on next payment\n</code></pre>"},{"location":"admin-dashboard/credits/#client-communication","title":"Client Communication","text":""},{"location":"admin-dashboard/credits/#credit-allocation-email","title":"Credit Allocation Email","text":"<pre><code>Subject: Your Credits Have Been Allocated\n\nHi [Client],\n\nWe've allocated 1,000 credits to your account!\n\nWHAT ARE CREDITS?\n- 1 credit = 1 completed job\n- Jobs can contain multiple LLM calls\n- Only successful jobs consume credits\n\nYOUR BALANCE:\n- Allocated: 1,000 credits\n- Remaining: 1,000 credits\n\nESTIMATED USAGE:\n- Document analysis: ~1 credit per document\n- Chat sessions: ~1 credit per conversation\n- Your 1,000 credits = approximately 1,000 operations\n\nMONITOR USAGE:\nView real-time usage at: https://dashboard.yourcompany.com\n\nNeed more credits? Reply to this email or visit your dashboard.\n\nQuestions? support@yourcompany.com\n</code></pre>"},{"location":"admin-dashboard/credits/#low-credit-warning","title":"Low Credit Warning","text":"<pre><code>Subject: Low Credit Alert - 20% Remaining\n\nHi [Client],\n\nYour credit balance is running low:\n\nCURRENT BALANCE: 200 credits (20% remaining)\nESTIMATED DEPLETION: 2-3 days at current usage\n\nACTION REQUIRED:\n1. Purchase additional credits: https://dashboard.yourcompany.com/credits\n2. Or contact us: support@yourcompany.com\n\nDon't let your integration stop! Top up now.\n\nBest regards,\nYour Company Team\n</code></pre>"},{"location":"admin-dashboard/credits/#out-of-credits-notice","title":"Out of Credits Notice","text":"<pre><code>Subject: URGENT: Credits Exhausted\n\nHi [Client],\n\nYour account has run out of credits.\n\nCURRENT BALANCE: 0 credits\nSTATUS: API calls suspended\n\nTO RESTORE SERVICE:\n1. Purchase credits immediately: https://dashboard.yourcompany.com/credits\n2. Or contact support: support@yourcompany.com\n\nService will resume automatically once credits are added.\n\nQuestions? We're here to help!\n</code></pre>"},{"location":"admin-dashboard/credits/#monitoring-analytics","title":"Monitoring &amp; Analytics","text":""},{"location":"admin-dashboard/credits/#team-credit-utilization","title":"Team Credit Utilization","text":"<pre><code># Get utilization across all teams\ncurl \"http://localhost:8003/api/credits/utilization\"\n</code></pre> <p>Response: <pre><code>{\n  \"total_teams\": 25,\n  \"summary\": {\n    \"total_allocated\": 50000,\n    \"total_remaining\": 32000,\n    \"total_used\": 18000,\n    \"avg_utilization\": 36.0\n  },\n  \"by_status\": {\n    \"healthy\": 20,      // &gt;50% remaining\n    \"warning\": 3,       // 20-50% remaining\n    \"critical\": 2       // &lt;20% remaining\n  },\n  \"teams_needing_attention\": [\n    {\n      \"team_id\": \"client-a-prod\",\n      \"credits_remaining\": 50,\n      \"percentage\": 5.0,\n      \"status\": \"critical\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/credits/#usage-trends","title":"Usage Trends","text":"<p>Track credit consumption over time:</p> <pre><code>curl \"http://localhost:8003/api/credits/trends?team_id=client-prod&amp;days=30\"\n</code></pre> <p>Use For: - Predicting when team will run out - Recommending plan upgrades - Identifying usage spikes - Forecasting revenue</p>"},{"location":"admin-dashboard/credits/#best-practices","title":"Best Practices","text":""},{"location":"admin-dashboard/credits/#for-admins","title":"For Admins","text":"<ol> <li>Start Conservative</li> <li>Begin with modest allocation (1,000 credits)</li> <li>Monitor usage first week</li> <li> <p>Adjust based on actual patterns</p> </li> <li> <p>Set Up Alerts</p> </li> <li>Alert at 50%, 20%, 10%, 0%</li> <li>Proactive outreach before depletion</li> <li> <p>Automated top-up for trusted clients</p> </li> <li> <p>Review Monthly</p> </li> <li>Check all team credit levels</li> <li>Identify heavy users (upsell opportunity)</li> <li> <p>Identify low users (check satisfaction)</p> </li> <li> <p>Track Profit Margins</p> </li> <li>Monitor actual LiteLLM costs vs. credits charged</li> <li>Adjust pricing if margins too thin</li> <li>Offer volume discounts for retention</li> </ol>"},{"location":"admin-dashboard/credits/#for-clients","title":"For Clients","text":"<p>Share these tips with clients:</p> <ol> <li>Monitor Your Balance</li> <li>Check dashboard regularly</li> <li>Set up low-balance alerts</li> <li> <p>Don't let credits hit zero</p> </li> <li> <p>Estimate Usage</p> </li> <li>1 credit \u2248 1 business operation</li> <li>Track your typical job types</li> <li> <p>Budget accordingly</p> </li> <li> <p>Complete Jobs Properly</p> </li> <li>Always call <code>complete_job()</code></li> <li>Failed jobs don't cost credits</li> <li> <p>Incomplete jobs won't be billed</p> </li> <li> <p>Optimize Usage</p> </li> <li>Group related calls into single jobs</li> <li>Don't create unnecessary jobs</li> <li>Cache responses when possible</li> </ol>"},{"location":"admin-dashboard/credits/#common-scenarios","title":"Common Scenarios","text":""},{"location":"admin-dashboard/credits/#scenario-1-client-runs-out-mid-month","title":"Scenario 1: Client Runs Out Mid-Month","text":"<p>Problem: Client exhausted their 1,000 credits in 2 weeks</p> <p>Actions: 1. Add immediate emergency credits (100-200) 2. Analyze usage patterns 3. Recommend plan upgrade 4. Set up automatic top-ups going forward</p>"},{"location":"admin-dashboard/credits/#scenario-2-client-barely-uses-credits","title":"Scenario 2: Client Barely Uses Credits","text":"<p>Problem: Client using &lt;10% of allocation</p> <p>Actions: 1. Check if they're having integration issues 2. Offer to help with implementation 3. Consider downgrading to save them money (builds trust) 4. Check if they need different features</p>"},{"location":"admin-dashboard/credits/#scenario-3-unexpected-usage-spike","title":"Scenario 3: Unexpected Usage Spike","text":"<p>Problem: Team uses 3x normal credits in one day</p> <p>Actions: 1. Check for runaway processes 2. Contact client to verify legitimate usage 3. Temporarily increase limits if needed 4. Investigate potential security issues</p>"},{"location":"admin-dashboard/credits/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/credits/#credits-not-deducted","title":"Credits Not Deducted","text":"<p>Problem: Jobs completing but credits unchanged</p> <p>Solutions: 1. Check job actually reached \"completed\" status 2. Verify credit deduction logic in code 3. Check database transaction logs 4. Ensure job_id is valid</p>"},{"location":"admin-dashboard/credits/#cant-add-credits","title":"Can't Add Credits","text":"<p>Problem: API returns error when adding credits</p> <p>Solutions: 1. Verify team exists and is active 2. Check team not suspended 3. Validate credit amount is positive integer 4. Check for database connection issues</p>"},{"location":"admin-dashboard/credits/#balance-shows-negative","title":"Balance Shows Negative","text":"<p>Problem: Credits_remaining is negative</p> <p>Solutions: 1. This can happen with race conditions 2. Investigate concurrent job completions 3. Add credits to bring back to positive 4. Implement better locking in credit deduction</p>"},{"location":"admin-dashboard/credits/#next-steps","title":"Next Steps","text":"<p>Now that you understand credits:</p> <ol> <li>Create Teams - Allocate credits when creating teams</li> <li>Monitor Usage - Track credit consumption</li> <li>Set Up Model Access - Control which models teams can use</li> <li>Review Best Practices - Optimize credit management</li> </ol>"},{"location":"admin-dashboard/model-access-groups/","title":"Model Access Groups","text":"<p>Learn how to control which LLM models your clients can access using model access groups.</p>"},{"location":"admin-dashboard/model-access-groups/#what-are-model-access-groups","title":"What are Model Access Groups?","text":"<p>Model Access Groups are collections of model aliases that define which LLMs a team can use. They provide granular access control and enable tiered pricing strategies.</p> <p>Built on LiteLLM</p> <p>SaaS LiteLLM uses LiteLLM for routing to 100+ LLM providers. Model access groups control which of these providers and models your teams can access.</p> <p>Key Benefits: - \u2705 Control costs by limiting expensive models - \u2705 Create pricing tiers (Basic, Pro, Enterprise) - \u2705 Grant access to specific providers (OpenAI, Anthropic, etc.) - \u2705 Simplify team management - \u2705 Implement progressive access upgrades</p>"},{"location":"admin-dashboard/model-access-groups/#how-model-access-groups-work","title":"How Model Access Groups Work","text":"<pre><code>Team \u2192 Access Groups \u2192 Model Aliases \u2192 Actual LLM Models\n</code></pre> <p>Example: <pre><code>Team: \"acme-prod\"\n  \u2514\u2500\u2500 Access Groups: [\"gpt-models\", \"claude-models\"]\n       \u251c\u2500\u2500 gpt-models\n       \u2502    \u251c\u2500\u2500 gpt-4 \u2192 openai/gpt-4\n       \u2502    \u2514\u2500\u2500 gpt-3.5-turbo \u2192 openai/gpt-3.5-turbo\n       \u2502\n       \u2514\u2500\u2500 claude-models\n            \u251c\u2500\u2500 claude-3-opus \u2192 anthropic/claude-3-opus-20240229\n            \u2514\u2500\u2500 claude-3-sonnet \u2192 anthropic/claude-3-sonnet-20240229\n</code></pre></p> <p>The team can now use models: <code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>claude-3-opus</code>, <code>claude-3-sonnet</code></p> <p> Learn more about model aliases</p>"},{"location":"admin-dashboard/model-access-groups/#creating-model-access-groups","title":"Creating Model Access Groups","text":"<p> Model Access Groups interface - control which models teams can access</p>"},{"location":"admin-dashboard/model-access-groups/#via-admin-dashboard","title":"Via Admin Dashboard","text":"<ol> <li>Navigate to Model Access</li> <li>Click \"Model Access\" in sidebar</li> <li> <p>Click \"Create Access Group\"</p> </li> <li> <p>Fill in Details</p> </li> <li>Group Name: <code>gpt-models</code> (lowercase, hyphens)</li> <li>Description: \"OpenAI GPT models\"</li> <li> <p>Model Aliases: Select models to include</p> </li> <li> <p>Save</p> </li> <li>Click \"Create\"</li> <li>Group is ready to assign to teams</li> </ol>"},{"location":"admin-dashboard/model-access-groups/#via-api","title":"Via API","text":"<pre><code>curl -X POST http://localhost:8003/api/model-access-groups/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"group_name\": \"gpt-models\",\n    \"description\": \"OpenAI GPT models\",\n    \"model_aliases\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-turbo\"]\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"group_name\": \"gpt-models\",\n  \"description\": \"OpenAI GPT models\",\n  \"model_aliases\": [\n    \"gpt-4\",\n    \"gpt-3.5-turbo\",\n    \"gpt-4-turbo\"\n  ],\n  \"created_at\": \"2024-10-14T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#common-access-group-setups","title":"Common Access Group Setups","text":""},{"location":"admin-dashboard/model-access-groups/#setup-1-by-provider","title":"Setup 1: By Provider","text":"<p>Group models by AI provider:</p> <p>OpenAI Models: <pre><code>{\n  \"group_name\": \"openai-models\",\n  \"description\": \"All OpenAI models\",\n  \"model_aliases\": [\n    \"gpt-4\",\n    \"gpt-4-turbo\",\n    \"gpt-3.5-turbo\"\n  ]\n}\n</code></pre></p> <p>Anthropic Models: <pre><code>{\n  \"group_name\": \"anthropic-models\",\n  \"description\": \"All Anthropic Claude models\",\n  \"model_aliases\": [\n    \"claude-3-opus\",\n    \"claude-3-sonnet\",\n    \"claude-3-haiku\"\n  ]\n}\n</code></pre></p> <p>Google Models: <pre><code>{\n  \"group_name\": \"google-models\",\n  \"description\": \"Google Gemini models\",\n  \"model_aliases\": [\n    \"gemini-pro\",\n    \"gemini-1.5-pro\"\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#setup-2-by-pricing-tier","title":"Setup 2: By Pricing Tier","text":"<p>Group models by cost/capabilities for tiered pricing:</p> <p>Basic Tier (Fast &amp; Cheap): <pre><code>{\n  \"group_name\": \"basic-models\",\n  \"description\": \"Fast, cost-effective models\",\n  \"model_aliases\": [\n    \"gpt-3.5-turbo\",\n    \"claude-3-haiku\"\n  ]\n}\n</code></pre></p> <p>Professional Tier (Balanced): <pre><code>{\n  \"group_name\": \"pro-models\",\n  \"description\": \"Balanced performance and cost\",\n  \"model_aliases\": [\n    \"gpt-4-turbo\",\n    \"claude-3-sonnet\",\n    \"gemini-pro\"\n  ]\n}\n</code></pre></p> <p>Enterprise Tier (Most Capable): <pre><code>{\n  \"group_name\": \"enterprise-models\",\n  \"description\": \"Most capable models\",\n  \"model_aliases\": [\n    \"gpt-4\",\n    \"claude-3-opus\",\n    \"gemini-1.5-pro\"\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#setup-3-by-use-case","title":"Setup 3: By Use Case","text":"<p>Group models by intended application:</p> <p>Chat/Conversational: <pre><code>{\n  \"group_name\": \"chat-models\",\n  \"description\": \"Optimized for conversation\",\n  \"model_aliases\": [\n    \"gpt-3.5-turbo\",\n    \"claude-3-sonnet\"\n  ]\n}\n</code></pre></p> <p>Analysis/Complex Tasks: <pre><code>{\n  \"group_name\": \"analysis-models\",\n  \"description\": \"Complex reasoning and analysis\",\n  \"model_aliases\": [\n    \"gpt-4\",\n    \"claude-3-opus\"\n  ]\n}\n</code></pre></p> <p>Fast Tasks: <pre><code>{\n  \"group_name\": \"fast-models\",\n  \"description\": \"Quick responses for simple tasks\",\n  \"model_aliases\": [\n    \"gpt-3.5-turbo\",\n    \"claude-3-haiku\"\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#assigning-access-groups-to-teams","title":"Assigning Access Groups to Teams","text":""},{"location":"admin-dashboard/model-access-groups/#during-team-creation","title":"During Team Creation","text":"<pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-prod\",\n    \"team_alias\": \"Production\",\n    \"access_groups\": [\"gpt-models\", \"claude-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#update-existing-team","title":"Update Existing Team","text":"<pre><code>curl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"access_groups\": [\"gpt-models\", \"claude-models\", \"gemini-models\"]\n  }'\n</code></pre> <p>Via Dashboard: 1. Navigate to team 2. Click \"Edit Access Groups\" 3. Select/deselect groups 4. Click \"Save\"</p>"},{"location":"admin-dashboard/model-access-groups/#viewing-access-groups","title":"Viewing Access Groups","text":""},{"location":"admin-dashboard/model-access-groups/#list-all-access-groups","title":"List All Access Groups","text":"<p>Via Dashboard: - Navigate to \"Model Access\" - See all access groups with model counts</p> <p>Via API: <pre><code>curl http://localhost:8003/api/model-access-groups\n</code></pre></p> <p>Response: <pre><code>{\n  \"access_groups\": [\n    {\n      \"group_name\": \"gpt-models\",\n      \"description\": \"OpenAI GPT models\",\n      \"model_count\": 3,\n      \"team_count\": 15\n    },\n    {\n      \"group_name\": \"claude-models\",\n      \"description\": \"Anthropic Claude models\",\n      \"model_count\": 3,\n      \"team_count\": 8\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#view-access-group-details","title":"View Access Group Details","text":"<pre><code>curl http://localhost:8003/api/model-access-groups/gpt-models\n</code></pre> <p>Response: <pre><code>{\n  \"group_name\": \"gpt-models\",\n  \"description\": \"OpenAI GPT models\",\n  \"model_aliases\": [\n    {\n      \"alias\": \"gpt-4\",\n      \"litellm_model\": \"openai/gpt-4\",\n      \"description\": \"Most capable GPT-4 model\"\n    },\n    {\n      \"alias\": \"gpt-3.5-turbo\",\n      \"litellm_model\": \"openai/gpt-3.5-turbo\",\n      \"description\": \"Fast and efficient\"\n    }\n  ],\n  \"teams_with_access\": [\n    \"acme-prod\",\n    \"techco-dev\",\n    \"client-staging\"\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#managing-access-groups","title":"Managing Access Groups","text":""},{"location":"admin-dashboard/model-access-groups/#add-models-to-group","title":"Add Models to Group","text":"<pre><code>curl -X POST http://localhost:8003/api/model-access-groups/gpt-models/add-models \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_aliases\": [\"gpt-4-vision\"]\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#remove-models-from-group","title":"Remove Models from Group","text":"<pre><code>curl -X POST http://localhost:8003/api/model-access-groups/gpt-models/remove-models \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_aliases\": [\"gpt-3.5-turbo\"]\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#delete-access-group","title":"Delete Access Group","text":"<p>Warning</p> <p>Deleting an access group will remove it from all teams. Teams will lose access to models in this group unless they have the models through other access groups.</p> <pre><code>curl -X DELETE http://localhost:8003/api/model-access-groups/old-models\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#pricing-strategies-using-access-groups","title":"Pricing Strategies Using Access Groups","text":""},{"location":"admin-dashboard/model-access-groups/#strategy-1-tiered-plans","title":"Strategy 1: Tiered Plans","text":"<p>Offer different plans with different model access:</p> <p>Starter Plan - $99/month <pre><code>{\n  \"access_groups\": [\"basic-models\"],\n  \"credits_allocated\": 1000\n}\n</code></pre> - Access: GPT-3.5-turbo, Claude 3 Haiku - Best for: Simple tasks, high volume</p> <p>Professional Plan - $299/month <pre><code>{\n  \"access_groups\": [\"basic-models\", \"pro-models\"],\n  \"credits_allocated\": 5000\n}\n</code></pre> - Access: All basic + GPT-4-turbo, Claude 3 Sonnet - Best for: Complex tasks, balanced performance</p> <p>Enterprise Plan - $999/month <pre><code>{\n  \"access_groups\": [\"basic-models\", \"pro-models\", \"enterprise-models\"],\n  \"credits_allocated\": 20000\n}\n</code></pre> - Access: All models including GPT-4, Claude 3 Opus - Best for: Maximum capabilities</p>"},{"location":"admin-dashboard/model-access-groups/#strategy-2-provider-based-pricing","title":"Strategy 2: Provider-Based Pricing","text":"<p>Charge based on provider access:</p> <p>OpenAI Only - $149/month <pre><code>{\n  \"access_groups\": [\"openai-models\"],\n  \"credits_allocated\": 2000\n}\n</code></pre></p> <p>Multi-Provider - $299/month <pre><code>{\n  \"access_groups\": [\"openai-models\", \"anthropic-models\", \"google-models\"],\n  \"credits_allocated\": 5000\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#strategy-3-use-case-bundles","title":"Strategy 3: Use Case Bundles","text":"<p>Bundle models by intended use:</p> <p>Chat Bundle - $199/month <pre><code>{\n  \"access_groups\": [\"chat-models\"],\n  \"credits_allocated\": 3000\n}\n</code></pre></p> <p>Analysis Bundle - $399/month <pre><code>{\n  \"access_groups\": [\"analysis-models\", \"chat-models\"],\n  \"credits_allocated\": 5000\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-access-groups/#team-model-access-workflow","title":"Team Model Access Workflow","text":""},{"location":"admin-dashboard/model-access-groups/#when-team-makes-api-call","title":"When Team Makes API Call","text":"<ol> <li> <p>Team sends request: <pre><code>POST /api/jobs/{job_id}/llm-call\n{\n  \"model\": \"gpt-4\",\n  \"messages\": [...]\n}\n</code></pre></p> </li> <li> <p>SaaS API checks:</p> </li> <li>Does team have \"gpt-4\" in any access group? \u2705</li> <li>Is team active? \u2705</li> <li> <p>Does team have credits? \u2705</p> </li> <li> <p>SaaS API resolves:</p> </li> <li> <p><code>gpt-4</code> \u2192 <code>openai/gpt-4</code> (via model alias)</p> </li> <li> <p>Routes to LiteLLM:</p> </li> <li>LiteLLM routes to actual OpenAI API</li> <li> <p>Response returns through chain</p> </li> <li> <p>If access denied: <pre><code>{\n  \"error\": \"Model access denied\",\n  \"message\": \"Team does not have access to model 'gpt-4'\",\n  \"available_models\": [\"gpt-3.5-turbo\", \"claude-3-haiku\"]\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-access-groups/#common-workflows","title":"Common Workflows","text":""},{"location":"admin-dashboard/model-access-groups/#workflow-1-upgrade-team-to-higher-tier","title":"Workflow 1: Upgrade Team to Higher Tier","text":"<p>Client wants access to more powerful models:</p> <pre><code># 1. Check current access\ncurl http://localhost:8003/api/teams/acme-prod\n\n# 2. Update access groups\ncurl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -d '{\n    \"access_groups\": [\"basic-models\", \"pro-models\", \"enterprise-models\"]\n  }'\n\n# 3. Add more credits for higher usage\ncurl -X POST http://localhost:8003/api/credits/add \\\n  -d '{\n    \"team_id\": \"acme-prod\",\n    \"amount\": 10000\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#workflow-2-create-custom-access-for-enterprise-client","title":"Workflow 2: Create Custom Access for Enterprise Client","text":"<p>Enterprise client wants specific models:</p> <pre><code># 1. Create custom access group\ncurl -X POST http://localhost:8003/api/model-access-groups/create \\\n  -d '{\n    \"group_name\": \"acme-custom\",\n    \"description\": \"Custom models for ACME Corp\",\n    \"model_aliases\": [\n      \"gpt-4\",\n      \"claude-3-opus\",\n      \"gemini-1.5-pro\"\n    ]\n  }'\n\n# 2. Assign to team\ncurl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -d '{\n    \"access_groups\": [\"acme-custom\"]\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#workflow-3-temporarily-grant-access-for-testing","title":"Workflow 3: Temporarily Grant Access for Testing","text":"<p>Client wants to test a new model:</p> <pre><code># 1. Add access group temporarily\ncurl -X PUT http://localhost:8003/api/teams/acme-dev \\\n  -d '{\n    \"access_groups\": [\"current-models\", \"experimental-models\"]\n  }'\n\n# 2. Monitor usage\n\n# 3. Remove after testing\ncurl -X PUT http://localhost:8003/api/teams/acme-dev \\\n  -d '{\n    \"access_groups\": [\"current-models\"]\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#best-practices","title":"Best Practices","text":""},{"location":"admin-dashboard/model-access-groups/#naming-conventions","title":"Naming Conventions","text":"<p>Access Group Names: - Use lowercase with hyphens - Be descriptive: <code>gpt-models</code>, <code>premium-models</code>, <code>chat-optimized</code> - Use consistent prefixes for organization: <code>tier-basic</code>, <code>tier-pro</code>, <code>tier-enterprise</code></p>"},{"location":"admin-dashboard/model-access-groups/#organization","title":"Organization","text":"<ol> <li>Start with Provider Groups</li> <li>Create one group per provider (OpenAI, Anthropic, Google)</li> <li> <p>Easy to understand and manage</p> </li> <li> <p>Add Tier Groups as You Grow</p> </li> <li>Basic, Professional, Enterprise</li> <li> <p>Maps to pricing plans</p> </li> <li> <p>Create Use Case Groups for Specific Clients</p> </li> <li>Custom bundles for enterprise clients</li> <li>Special access for partners</li> </ol>"},{"location":"admin-dashboard/model-access-groups/#access-control","title":"Access Control","text":"<ol> <li>Principle of Least Privilege</li> <li>Start teams with minimal access</li> <li>Upgrade as needed</li> <li> <p>Don't grant all models by default</p> </li> <li> <p>Separate Dev and Prod</p> </li> <li>Dev teams: cheaper models for testing</li> <li> <p>Prod teams: full access to needed models</p> </li> <li> <p>Monitor and Adjust</p> </li> <li>Track which models teams actually use</li> <li>Remove unused access groups</li> <li>Identify upgrade opportunities</li> </ol>"},{"location":"admin-dashboard/model-access-groups/#cost-management","title":"Cost Management","text":"<ol> <li>Group by Cost</li> <li>Create \"expensive-models\" group</li> <li>Restrict to paying customers only</li> <li> <p>Monitor usage of costly models</p> </li> <li> <p>Gradual Rollout</p> </li> <li>New models \u2192 small test group first</li> <li>Monitor costs and performance</li> <li> <p>Expand access gradually</p> </li> <li> <p>Sunset Old Models</p> </li> <li>Remove deprecated models from groups</li> <li>Notify teams before removal</li> <li>Provide migration path</li> </ol>"},{"location":"admin-dashboard/model-access-groups/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/model-access-groups/#team-cant-access-model","title":"Team Can't Access Model","text":"<p>Problem: Client reports \"Model access denied\" for <code>gpt-4</code></p> <p>Solutions: 1. Check team's access groups:    <pre><code>curl http://localhost:8003/api/teams/acme-prod\n</code></pre></p> <ol> <li> <p>Check if any access group contains the model:    <pre><code>curl http://localhost:8003/api/model-access-groups/gpt-models\n</code></pre></p> </li> <li> <p>Add missing access group:    <pre><code>curl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -d '{\"access_groups\": [\"existing-groups\", \"gpt-models\"]}'\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-access-groups/#model-alias-not-found","title":"Model Alias Not Found","text":"<p>Problem: Access group won't accept model alias</p> <p>Solutions: 1. Verify model alias exists:    <pre><code>curl http://localhost:8003/api/model-aliases\n</code></pre></p> <ol> <li> <p>Create model alias if missing:    <pre><code>curl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"GPT-4\"\n  }'\n</code></pre></p> </li> <li> <p>Add to access group:    <pre><code>curl -X POST http://localhost:8003/api/model-access-groups/gpt-models/add-models \\\n  -d '{\"model_aliases\": [\"gpt-4\"]}'\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-access-groups/#access-group-changes-not-taking-effect","title":"Access Group Changes Not Taking Effect","text":"<p>Problem: Updated access groups but team still can't access model</p> <p>Solutions: 1. Verify update succeeded:    <pre><code>curl http://localhost:8003/api/teams/acme-prod\n</code></pre></p> <ol> <li> <p>Check model is in the access group:    <pre><code>curl http://localhost:8003/api/model-access-groups/gpt-models\n</code></pre></p> </li> <li> <p>Restart SaaS API server if needed (rare):    <pre><code>docker compose restart saas-api\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-access-groups/#example-complete-client-onboarding-with-access-control","title":"Example: Complete Client Onboarding with Access Control","text":"<pre><code># 1. Create organization\ncurl -X POST http://localhost:8003/api/organizations/create \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"name\": \"New Client Inc\"\n  }'\n\n# 2. Create access groups (if not exists)\ncurl -X POST http://localhost:8003/api/model-access-groups/create \\\n  -d '{\n    \"group_name\": \"starter-models\",\n    \"description\": \"Starter tier models\",\n    \"model_aliases\": [\"gpt-3.5-turbo\", \"claude-3-haiku\"]\n  }'\n\n# 3. Create team with access\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"team_id\": \"newclient-prod\",\n    \"team_alias\": \"Production\",\n    \"access_groups\": [\"starter-models\"],\n    \"credits_allocated\": 1000\n  }'\n\n# 4. Share virtual key with client\n# (From response above)\n\n# 5. Later: Upgrade to pro tier\ncurl -X PUT http://localhost:8003/api/teams/newclient-prod \\\n  -d '{\n    \"access_groups\": [\"starter-models\", \"pro-models\"]\n  }'\n\n# 6. Add more credits\ncurl -X POST http://localhost:8003/api/credits/add \\\n  -d '{\n    \"team_id\": \"newclient-prod\",\n    \"amount\": 5000\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#next-steps","title":"Next Steps","text":"<p>Now that you understand model access groups:</p> <ol> <li>Configure Model Aliases - Set up the actual models</li> <li>Create Teams - Assign access groups to teams</li> <li>Monitor Usage - Track which models are used most</li> <li>Best Practices - Advanced access control patterns</li> </ol>"},{"location":"admin-dashboard/model-access-groups/#quick-reference","title":"Quick Reference","text":""},{"location":"admin-dashboard/model-access-groups/#create-access-group","title":"Create Access Group","text":"<pre><code>POST /api/model-access-groups/create\n{\n  \"group_name\": \"gpt-models\",\n  \"description\": \"OpenAI GPT models\",\n  \"model_aliases\": [\"gpt-4\", \"gpt-3.5-turbo\"]\n}\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#assign-to-team","title":"Assign to Team","text":"<pre><code>PUT /api/teams/{team_id}\n{\n  \"access_groups\": [\"gpt-models\", \"claude-models\"]\n}\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#add-models-to-group_1","title":"Add Models to Group","text":"<pre><code>POST /api/model-access-groups/{group_name}/add-models\n{\n  \"model_aliases\": [\"new-model\"]\n}\n</code></pre>"},{"location":"admin-dashboard/model-access-groups/#view-group-details","title":"View Group Details","text":"<pre><code>GET /api/model-access-groups/{group_name}\n</code></pre>"},{"location":"admin-dashboard/model-aliases/","title":"Model Aliases","text":"<p>Configure simple, friendly names for the 100+ LLM models available through LiteLLM.</p>"},{"location":"admin-dashboard/model-aliases/#what-are-model-aliases","title":"What are Model Aliases?","text":"<p>Model Aliases are simple names that map to actual LLM provider models. They allow you to:</p> <ul> <li>\u2705 Hide complex provider-specific model names from clients</li> <li>\u2705 Create consistent naming across providers</li> <li>\u2705 Switch providers without changing client code</li> <li>\u2705 Offer tiered model options (fast, balanced, smart)</li> <li>\u2705 Control pricing and access per model</li> </ul> <p>Built on LiteLLM</p> <p>LiteLLM supports 100+ LLM providers. Model aliases let you expose these models with simple, friendly names instead of provider-specific formats like <code>openai/gpt-4</code> or <code>anthropic/claude-3-opus-20240229</code>.</p>"},{"location":"admin-dashboard/model-aliases/#how-model-aliases-work","title":"How Model Aliases Work","text":"<pre><code>Client Code         Alias           LiteLLM Model                Provider\n-----------         -----           -------------                --------\nmodel: \"gpt-4\"  \u2192   gpt-4      \u2192   openai/gpt-4            \u2192   OpenAI API\nmodel: \"claude\"  \u2192   claude     \u2192   anthropic/claude-3-opus \u2192   Anthropic API\nmodel: \"smart\"   \u2192   smart      \u2192   openai/gpt-4            \u2192   OpenAI API\n</code></pre> <p>Example Client Code: <pre><code># Client just uses simple alias\nresponse = await client.chat(\n    job_id=job_id,\n    model=\"gpt-4\",  # Simple, clean\n    messages=[...]\n)\n</code></pre></p> <p>What Actually Happens: 1. Client sends <code>model: \"gpt-4\"</code> 2. SaaS API looks up alias: <code>gpt-4</code> \u2192 <code>openai/gpt-4</code> 3. LiteLLM routes to OpenAI with model <code>gpt-4</code> 4. Response flows back to client</p>"},{"location":"admin-dashboard/model-aliases/#creating-model-aliases","title":"Creating Model Aliases","text":"<p> Model Aliases interface - create user-facing model names with pricing</p>"},{"location":"admin-dashboard/model-aliases/#via-admin-dashboard","title":"Via Admin Dashboard","text":"<ol> <li>Navigate to Model Aliases</li> <li>Click \"Model Management\" \u2192 \"Aliases\"</li> <li> <p>Click \"Create Alias\"</p> </li> <li> <p>Fill in Details</p> </li> <li>Alias: Simple name (e.g., <code>gpt-4</code>)</li> <li>LiteLLM Model: Provider/model format (e.g., <code>openai/gpt-4</code>)</li> <li>Description: What this model is for</li> <li> <p>Active: Enable/disable alias</p> </li> <li> <p>Save</p> </li> <li>Click \"Create\"</li> <li>Alias is ready to use</li> </ol>"},{"location":"admin-dashboard/model-aliases/#via-api","title":"Via API","text":"<pre><code>curl -X POST http://localhost:8003/api/model-aliases/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"alias\": \"gpt-4\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"OpenAI GPT-4 - Most capable model\",\n    \"active\": true\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"alias\": \"gpt-4\",\n  \"litellm_model\": \"openai/gpt-4\",\n  \"description\": \"OpenAI GPT-4 - Most capable model\",\n  \"active\": true,\n  \"created_at\": \"2024-10-14T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#litellm-model-format","title":"LiteLLM Model Format","text":"<p>Model aliases must map to valid LiteLLM model names:</p> <p>Format: <code>provider/model-name</code></p>"},{"location":"admin-dashboard/model-aliases/#common-providers","title":"Common Providers","text":"<p>OpenAI: <pre><code>openai/gpt-4\nopenai/gpt-4-turbo\nopenai/gpt-3.5-turbo\nopenai/gpt-4-vision-preview\n</code></pre></p> <p>Anthropic: <pre><code>anthropic/claude-3-opus-20240229\nanthropic/claude-3-sonnet-20240229\nanthropic/claude-3-haiku-20240307\nanthropic/claude-3-5-sonnet-20240620\n</code></pre></p> <p>Google: <pre><code>gemini/gemini-pro\ngemini/gemini-1.5-pro\ngemini/gemini-1.5-flash\n</code></pre></p> <p>Azure OpenAI: <pre><code>azure/gpt-4-deployment-name\nazure/gpt-35-turbo-deployment-name\n</code></pre></p> <p>AWS Bedrock: <pre><code>bedrock/anthropic.claude-3-opus-20240229-v1:0\nbedrock/anthropic.claude-3-sonnet-20240229-v1:0\nbedrock/meta.llama3-70b-instruct-v1:0\n</code></pre></p> <p> See all supported providers in LiteLLM docs</p>"},{"location":"admin-dashboard/model-aliases/#alias-naming-strategies","title":"Alias Naming Strategies","text":""},{"location":"admin-dashboard/model-aliases/#strategy-1-provider-based-naming","title":"Strategy 1: Provider-Based Naming","text":"<p>Keep provider in the alias name:</p> <pre><code>[\n  {\"alias\": \"openai-gpt4\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"openai-gpt35\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"claude-opus\", \"litellm_model\": \"anthropic/claude-3-opus-20240229\"},\n  {\"alias\": \"claude-sonnet\", \"litellm_model\": \"anthropic/claude-3-sonnet-20240229\"},\n  {\"alias\": \"gemini-pro\", \"litellm_model\": \"gemini/gemini-pro\"}\n]\n</code></pre> <p>Pros: - Clear which provider is being used - Easy to understand - Simple mapping</p> <p>Cons: - Harder to switch providers later - Clients know provider details</p>"},{"location":"admin-dashboard/model-aliases/#strategy-2-genericabstract-naming","title":"Strategy 2: Generic/Abstract Naming","text":"<p>Hide provider details with abstract names:</p> <pre><code>[\n  {\"alias\": \"smart\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"balanced\", \"litellm_model\": \"openai/gpt-4-turbo\"},\n  {\"alias\": \"fast\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"vision\", \"litellm_model\": \"openai/gpt-4-vision-preview\"}\n]\n</code></pre> <p>Pros: - Can switch providers transparently - Simple, memorable names - Provider-agnostic</p> <p>Cons: - Less clear what model is actually used - May need documentation</p>"},{"location":"admin-dashboard/model-aliases/#strategy-3-tiered-naming","title":"Strategy 3: Tiered Naming","text":"<p>Name by capability/price tier:</p> <pre><code>[\n  {\"alias\": \"basic\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"professional\", \"litellm_model\": \"openai/gpt-4-turbo\"},\n  {\"alias\": \"enterprise\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"premium\", \"litellm_model\": \"anthropic/claude-3-opus-20240229\"}\n]\n</code></pre> <p>Pros: - Maps to pricing plans - Easy for clients to choose - Clear value proposition</p> <p>Cons: - Clients don't know actual model - May limit flexibility</p>"},{"location":"admin-dashboard/model-aliases/#strategy-4-use-case-naming","title":"Strategy 4: Use Case Naming","text":"<p>Name by intended application:</p> <pre><code>[\n  {\"alias\": \"chat\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"analysis\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"creative\", \"litellm_model\": \"anthropic/claude-3-opus-20240229\"},\n  {\"alias\": \"code\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"vision\", \"litellm_model\": \"openai/gpt-4-vision-preview\"}\n]\n</code></pre> <p>Pros: - Guides clients to right model - Self-documenting - Clear purpose</p> <p>Cons: - Same model may appear multiple times - Can be confusing</p>"},{"location":"admin-dashboard/model-aliases/#recommended-hybrid-approach","title":"Recommended: Hybrid Approach","text":"<p>Combine strategies for clarity and flexibility:</p> <pre><code>[\n  // Standard names (most common)\n  {\"alias\": \"gpt-4\", \"litellm_model\": \"openai/gpt-4\"},\n  {\"alias\": \"gpt-3.5-turbo\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"claude-3-opus\", \"litellm_model\": \"anthropic/claude-3-opus-20240229\"},\n  {\"alias\": \"claude-3-sonnet\", \"litellm_model\": \"anthropic/claude-3-sonnet-20240229\"},\n\n  // Convenience aliases\n  {\"alias\": \"fast\", \"litellm_model\": \"openai/gpt-3.5-turbo\"},\n  {\"alias\": \"smart\", \"litellm_model\": \"openai/gpt-4\"},\n\n  // Use case aliases\n  {\"alias\": \"vision\", \"litellm_model\": \"openai/gpt-4-vision-preview\"}\n]\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#common-model-alias-setups","title":"Common Model Alias Setups","text":""},{"location":"admin-dashboard/model-aliases/#setup-1-openai-only","title":"Setup 1: OpenAI Only","text":"<pre><code># GPT-4\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"Most capable GPT-4 model\"\n  }'\n\n# GPT-4 Turbo\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4-turbo\",\n    \"litellm_model\": \"openai/gpt-4-turbo\",\n    \"description\": \"Faster and cheaper GPT-4\"\n  }'\n\n# GPT-3.5 Turbo\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-3.5-turbo\",\n    \"litellm_model\": \"openai/gpt-3.5-turbo\",\n    \"description\": \"Fast and efficient for most tasks\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#setup-2-multi-provider","title":"Setup 2: Multi-Provider","text":"<pre><code># OpenAI GPT-4\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"OpenAI GPT-4\"\n  }'\n\n# Anthropic Claude 3 Opus\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"claude-3-opus\",\n    \"litellm_model\": \"anthropic/claude-3-opus-20240229\",\n    \"description\": \"Claude 3 Opus - Most capable Claude model\"\n  }'\n\n# Google Gemini Pro\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gemini-pro\",\n    \"litellm_model\": \"gemini/gemini-pro\",\n    \"description\": \"Google Gemini Pro\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#setup-3-tiered-models","title":"Setup 3: Tiered Models","text":"<pre><code># Basic Tier\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"basic\",\n    \"litellm_model\": \"openai/gpt-3.5-turbo\",\n    \"description\": \"Fast, cost-effective model\"\n  }'\n\n# Professional Tier\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"pro\",\n    \"litellm_model\": \"openai/gpt-4-turbo\",\n    \"description\": \"Balanced performance and cost\"\n  }'\n\n# Enterprise Tier\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"enterprise\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"Most capable model\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#managing-model-aliases","title":"Managing Model Aliases","text":""},{"location":"admin-dashboard/model-aliases/#list-all-aliases","title":"List All Aliases","text":"<pre><code>curl http://localhost:8003/api/model-aliases\n</code></pre> <p>Response: <pre><code>{\n  \"aliases\": [\n    {\n      \"alias\": \"gpt-4\",\n      \"litellm_model\": \"openai/gpt-4\",\n      \"description\": \"Most capable GPT-4 model\",\n      \"active\": true,\n      \"usage_count\": 1543\n    },\n    {\n      \"alias\": \"claude-3-opus\",\n      \"litellm_model\": \"anthropic/claude-3-opus-20240229\",\n      \"description\": \"Claude 3 Opus\",\n      \"active\": true,\n      \"usage_count\": 892\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#view-alias-details","title":"View Alias Details","text":"<pre><code>curl http://localhost:8003/api/model-aliases/gpt-4\n</code></pre> <p>Response: <pre><code>{\n  \"alias\": \"gpt-4\",\n  \"litellm_model\": \"openai/gpt-4\",\n  \"description\": \"Most capable GPT-4 model\",\n  \"active\": true,\n  \"access_groups\": [\"gpt-models\", \"premium-models\"],\n  \"teams_with_access\": 15,\n  \"usage_stats\": {\n    \"total_calls\": 1543,\n    \"total_cost_usd\": 234.56,\n    \"avg_tokens_per_call\": 850\n  }\n}\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#update-alias","title":"Update Alias","text":"<pre><code>curl -X PUT http://localhost:8003/api/model-aliases/gpt-4 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"description\": \"OpenAI GPT-4 - Most capable model (updated)\",\n    \"active\": true\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#disable-alias","title":"Disable Alias","text":"<p>Temporarily disable without deleting:</p> <pre><code>curl -X PUT http://localhost:8003/api/model-aliases/old-model \\\n  -d '{\n    \"active\": false\n  }'\n</code></pre> <p>When disabled: - Teams can't use this model - API returns error if requested - Alias stays in system for future re-enabling</p>"},{"location":"admin-dashboard/model-aliases/#delete-alias","title":"Delete Alias","text":"<p>Warning</p> <p>Deleting an alias removes it from all access groups. Teams using this alias will lose access.</p> <pre><code>curl -X DELETE http://localhost:8003/api/model-aliases/deprecated-model\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#switching-providers","title":"Switching Providers","text":"<p>One powerful use of aliases: switch providers without changing client code.</p>"},{"location":"admin-dashboard/model-aliases/#example-switch-from-openai-to-anthropic","title":"Example: Switch from OpenAI to Anthropic","text":"<p>Initial Setup: <pre><code>curl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"smart\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"Smart model for complex tasks\"\n  }'\n</code></pre></p> <p>Clients use it: <pre><code>response = await client.chat(\n    job_id=job_id,\n    model=\"smart\",  # Points to GPT-4\n    messages=[...]\n)\n</code></pre></p> <p>Later: Switch to Claude without client changes: <pre><code>curl -X PUT http://localhost:8003/api/model-aliases/smart \\\n  -d '{\n    \"litellm_model\": \"anthropic/claude-3-opus-20240229\",\n    \"description\": \"Smart model for complex tasks (now using Claude)\"\n  }'\n</code></pre></p> <p>Clients still use same code: <pre><code>response = await client.chat(\n    job_id=job_id,\n    model=\"smart\",  # Now points to Claude 3 Opus!\n    messages=[...]\n)\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#adding-new-models","title":"Adding New Models","text":"<p>When a new model is released:</p>"},{"location":"admin-dashboard/model-aliases/#1-check-litellm-support","title":"1. Check LiteLLM Support","text":"<p>Verify the model is supported: - Check LiteLLM providers documentation - Look for model format (e.g., <code>openai/gpt-4o</code>)</p>"},{"location":"admin-dashboard/model-aliases/#2-add-api-keys-if-new-provider","title":"2. Add API Keys (if new provider)","text":"<p>If it's a new provider, add credentials to LiteLLM config:</p> <pre><code># litellm_config.yaml\nmodel_list:\n  - model_name: gpt-4o\n    litellm_params:\n      model: openai/gpt-4o\n      api_key: os.environ/OPENAI_API_KEY\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#3-create-model-alias","title":"3. Create Model Alias","text":"<pre><code>curl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4o\",\n    \"litellm_model\": \"openai/gpt-4o\",\n    \"description\": \"GPT-4 Omni - Latest OpenAI model\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#4-add-to-access-groups","title":"4. Add to Access Groups","text":"<pre><code>curl -X POST http://localhost:8003/api/model-access-groups/gpt-models/add-models \\\n  -d '{\n    \"model_aliases\": [\"gpt-4o\"]\n  }'\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#5-notify-teams","title":"5. Notify Teams","text":"<p>Inform clients about new model availability: <pre><code>New Model Available: gpt-4o\n\nWe've added OpenAI's latest GPT-4 Omni model to your available models.\n\nTo use it, simply specify: model=\"gpt-4o\" in your API calls.\n\nBenefits:\n- Faster response times\n- Improved reasoning\n- Better at complex tasks\n\nTry it today!\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#complete-client-onboarding-example","title":"Complete Client Onboarding Example","text":"<pre><code># 1. Create model aliases\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-4\",\n    \"litellm_model\": \"openai/gpt-4\",\n    \"description\": \"GPT-4 for complex tasks\"\n  }'\n\ncurl -X POST http://localhost:8003/api/model-aliases/create \\\n  -d '{\n    \"alias\": \"gpt-3.5-turbo\",\n    \"litellm_model\": \"openai/gpt-3.5-turbo\",\n    \"description\": \"Fast model for simple tasks\"\n  }'\n\n# 2. Create access group\ncurl -X POST http://localhost:8003/api/model-access-groups/create \\\n  -d '{\n    \"group_name\": \"starter-models\",\n    \"description\": \"Models for starter plan\",\n    \"model_aliases\": [\"gpt-3.5-turbo\"]\n  }'\n\n# 3. Create organization\ncurl -X POST http://localhost:8003/api/organizations/create \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"name\": \"New Client Inc\"\n  }'\n\n# 4. Create team\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"team_id\": \"newclient-prod\",\n    \"team_alias\": \"Production\",\n    \"access_groups\": [\"starter-models\"],\n    \"credits_allocated\": 1000\n  }'\n\n# 5. Client can now use model aliases\n# In their code: model=\"gpt-3.5-turbo\"\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#best-practices","title":"Best Practices","text":""},{"location":"admin-dashboard/model-aliases/#naming","title":"Naming","text":"<ol> <li>Use Standard Names When Possible</li> <li><code>gpt-4</code>, <code>claude-3-opus</code>, <code>gemini-pro</code></li> <li>Familiar to developers</li> <li> <p>Easy to remember</p> </li> <li> <p>Be Consistent</p> </li> <li>If you use hyphens, use them everywhere</li> <li>Stick to lowercase</li> <li> <p>Follow a naming pattern</p> </li> <li> <p>Avoid Version Numbers in Aliases <pre><code># \u274c Bad: Hard to maintain\nalias: \"gpt-4-0613\"\n\n# \u2705 Good: Can update underlying model\nalias: \"gpt-4\"\nlitellm_model: \"openai/gpt-4-0613\"\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-aliases/#organization","title":"Organization","text":"<ol> <li>Group Related Models</li> <li>All OpenAI models together</li> <li>All fast models together</li> <li> <p>All vision models together</p> </li> <li> <p>Use Descriptions</p> </li> <li>Explain what the model is good for</li> <li>Mention speed/cost trade-offs</li> <li> <p>Note any special capabilities</p> </li> <li> <p>Track Usage</p> </li> <li>Monitor which models are popular</li> <li>Identify underused models</li> <li>Optimize based on actual usage</li> </ol>"},{"location":"admin-dashboard/model-aliases/#maintenance","title":"Maintenance","text":"<ol> <li>Regular Updates</li> <li>Update to newer model versions</li> <li>Deprecate old models gradually</li> <li> <p>Test new models before rolling out</p> </li> <li> <p>Communicate Changes</p> </li> <li>Notify teams before removing models</li> <li>Provide migration guides</li> <li> <p>Offer grace periods</p> </li> <li> <p>Monitor Costs</p> </li> <li>Track spend per model</li> <li>Identify expensive models</li> <li>Adjust pricing if needed</li> </ol>"},{"location":"admin-dashboard/model-aliases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/model-aliases/#invalid-model-error","title":"Invalid Model Error","text":"<p>Problem: \"Model not found\" or \"Invalid model\"</p> <p>Solutions: 1. Verify alias exists:    <pre><code>curl http://localhost:8003/api/model-aliases/gpt-4\n</code></pre></p> <ol> <li> <p>Check alias is active:    <pre><code>curl http://localhost:8003/api/model-aliases/gpt-4\n# Should show \"active\": true\n</code></pre></p> </li> <li> <p>Verify team has access:    <pre><code>curl http://localhost:8003/api/teams/acme-prod\n# Check access_groups include group with this model\n</code></pre></p> </li> </ol>"},{"location":"admin-dashboard/model-aliases/#litellm-routing-error","title":"LiteLLM Routing Error","text":"<p>Problem: \"Provider authentication failed\" or routing error</p> <p>Solutions: 1. Check LiteLLM config has provider credentials 2. Verify litellm_model format is correct 3. Test model directly via LiteLLM:    <pre><code>curl http://localhost:8002/chat/completions \\\n  -d '{\"model\": \"openai/gpt-4\", \"messages\": [...]}'\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#alias-already-exists","title":"Alias Already Exists","text":"<p>Problem: \"Alias already exists\"</p> <p>Solutions: 1. Use different alias name 2. Update existing alias instead:    <pre><code>curl -X PUT http://localhost:8003/api/model-aliases/gpt-4 \\\n  -d '{\"litellm_model\": \"openai/gpt-4-turbo\"}'\n</code></pre></p>"},{"location":"admin-dashboard/model-aliases/#next-steps","title":"Next Steps","text":"<p>Now that you understand model aliases:</p> <ol> <li>Create Access Groups - Group aliases for team access</li> <li>Assign to Teams - Give teams access to models</li> <li>Monitor Usage - Track which models are used</li> <li>Review LiteLLM Docs - See all supported models</li> </ol>"},{"location":"admin-dashboard/model-aliases/#quick-reference","title":"Quick Reference","text":""},{"location":"admin-dashboard/model-aliases/#create-alias","title":"Create Alias","text":"<pre><code>POST /api/model-aliases/create\n{\n  \"alias\": \"gpt-4\",\n  \"litellm_model\": \"openai/gpt-4\",\n  \"description\": \"GPT-4 model\",\n  \"active\": true\n}\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#list-aliases","title":"List Aliases","text":"<pre><code>GET /api/model-aliases\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#update-alias_1","title":"Update Alias","text":"<pre><code>PUT /api/model-aliases/{alias}\n{\n  \"description\": \"Updated description\",\n  \"litellm_model\": \"openai/gpt-4-turbo\"\n}\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#disable-alias_1","title":"Disable Alias","text":"<pre><code>PUT /api/model-aliases/{alias}\n{\n  \"active\": false\n}\n</code></pre>"},{"location":"admin-dashboard/model-aliases/#delete-alias_1","title":"Delete Alias","text":"<pre><code>DELETE /api/model-aliases/{alias}\n</code></pre>"},{"location":"admin-dashboard/monitoring/","title":"Monitoring &amp; Analytics","text":"<p>Monitor system health, track usage metrics, analyze credit consumption, and set up alerts for your SaaS LiteLLM platform.</p>"},{"location":"admin-dashboard/monitoring/#overview","title":"Overview","text":"<p>The monitoring system provides comprehensive insights into:</p> <ul> <li>System health and performance</li> <li>Usage metrics and analytics</li> <li>Credit consumption tracking</li> <li>Cost analysis</li> <li>Team activity monitoring</li> <li>Performance metrics</li> <li>Alert notifications</li> </ul>"},{"location":"admin-dashboard/monitoring/#dashboard-overview","title":"Dashboard Overview","text":""},{"location":"admin-dashboard/monitoring/#access-the-monitoring-dashboard","title":"Access the Monitoring Dashboard","text":"<p>Local Development: <pre><code>http://localhost:3002/monitoring\n</code></pre></p> <p>Production: <pre><code>https://your-admin-dashboard.com/monitoring\n</code></pre></p>"},{"location":"admin-dashboard/monitoring/#key-metrics-at-a-glance","title":"Key Metrics at a Glance","text":"<p>The monitoring dashboard displays:</p> <ol> <li>Platform Health</li> <li>API uptime status</li> <li>Database connection health</li> <li>LiteLLM proxy status</li> <li> <p>Average response times</p> </li> <li> <p>Usage Statistics</p> </li> <li>Total jobs today/this month</li> <li>Active teams count</li> <li>Total API calls</li> <li> <p>Token consumption</p> </li> <li> <p>Financial Metrics</p> </li> <li>Total costs (USD)</li> <li>Credits allocated vs. used</li> <li>Cost per team breakdown</li> <li> <p>Revenue analytics</p> </li> <li> <p>Performance Indicators</p> </li> <li>Average latency</li> <li>Success/failure rates</li> <li>Model usage distribution</li> <li>Error rates by type</li> </ol>"},{"location":"admin-dashboard/monitoring/#system-health-monitoring","title":"System Health Monitoring","text":""},{"location":"admin-dashboard/monitoring/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>The SaaS API provides a health check endpoint:</p> <pre><code>GET /health\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"service\": \"saas-llm-api\"\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#component-health-checks","title":"Component Health Checks","text":"<p>Monitor all system components:</p> <pre><code># health_check.py\nimport httpx\nfrom sqlalchemy import create_engine, text\n\nasync def check_system_health():\n    \"\"\"\n    Comprehensive health check for all system components\n    \"\"\"\n    health_status = {\n        \"saas_api\": False,\n        \"litellm_proxy\": False,\n        \"database\": False,\n        \"overall\": \"unhealthy\"\n    }\n\n    # 1. Check SaaS API\n    try:\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            response = await client.get(\"http://localhost:8003/health\")\n            health_status[\"saas_api\"] = response.status_code == 200\n    except Exception as e:\n        print(f\"SaaS API check failed: {e}\")\n\n    # 2. Check LiteLLM Proxy\n    try:\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            response = await client.get(\"http://localhost:8002/health\")\n            health_status[\"litellm_proxy\"] = response.status_code == 200\n    except Exception as e:\n        print(f\"LiteLLM Proxy check failed: {e}\")\n\n    # 3. Check Database\n    try:\n        engine = create_engine(\"postgresql://...\")\n        with engine.connect() as conn:\n            result = conn.execute(text(\"SELECT 1\"))\n            health_status[\"database\"] = result.fetchone()[0] == 1\n    except Exception as e:\n        print(f\"Database check failed: {e}\")\n\n    # Overall health\n    if all([health_status[\"saas_api\"], health_status[\"litellm_proxy\"], health_status[\"database\"]]):\n        health_status[\"overall\"] = \"healthy\"\n    elif any([health_status[\"saas_api\"], health_status[\"litellm_proxy\"], health_status[\"database\"]]):\n        health_status[\"overall\"] = \"degraded\"\n\n    return health_status\n\n# Usage\nhealth = await check_system_health()\nprint(health)\n</code></pre> <p>Schedule health checks:</p> <pre><code># crontab -e\n*/5 * * * * python /path/to/health_check.py &gt;&gt; /var/log/health.log 2&gt;&amp;1\n</code></pre>"},{"location":"admin-dashboard/monitoring/#real-time-health-dashboard","title":"Real-Time Health Dashboard","text":"<pre><code>// components/HealthDashboard.jsx\nimport { useEffect, useState } from 'react';\n\nexport function HealthDashboard() {\n  const [health, setHealth] = useState(null);\n\n  useEffect(() =&gt; {\n    const checkHealth = async () =&gt; {\n      const response = await fetch('/api/health-check');\n      const data = await response.json();\n      setHealth(data);\n    };\n\n    // Check health every 30 seconds\n    checkHealth();\n    const interval = setInterval(checkHealth, 30000);\n    return () =&gt; clearInterval(interval);\n  }, []);\n\n  return (\n    &lt;div className=\"health-dashboard\"&gt;\n      &lt;ServiceStatus name=\"SaaS API\" status={health?.saas_api} /&gt;\n      &lt;ServiceStatus name=\"LiteLLM Proxy\" status={health?.litellm_proxy} /&gt;\n      &lt;ServiceStatus name=\"Database\" status={health?.database} /&gt;\n      &lt;OverallHealth status={health?.overall} /&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#usage-metrics-analytics","title":"Usage Metrics &amp; Analytics","text":""},{"location":"admin-dashboard/monitoring/#team-usage-summary","title":"Team Usage Summary","text":"<p>Get comprehensive usage statistics for any team:</p> <p>API Endpoint:</p> <pre><code>GET /api/teams/{team_id}/usage?period={period}\nAuthorization: Bearer {virtual_key}\n</code></pre> <p>Parameters: - <code>period</code>: Format \"YYYY-MM\" (e.g., \"2025-10\") or \"YYYY-MM-DD\"</p> <p>Example Request:</p> <pre><code>curl -X GET \"http://localhost:8003/api/teams/team_abc123/usage?period=2025-10\" \\\n  -H \"Authorization: Bearer sk-team-key-abc123\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"team_id\": \"team_abc123\",\n  \"period\": \"2025-10\",\n  \"summary\": {\n    \"total_jobs\": 1234,\n    \"successful_jobs\": 1180,\n    \"failed_jobs\": 54,\n    \"total_cost_usd\": 156.78,\n    \"total_tokens\": 1250000,\n    \"avg_cost_per_job\": 0.1270\n  },\n  \"job_types\": {\n    \"document_analysis\": {\n      \"count\": 456,\n      \"cost_usd\": 67.89\n    },\n    \"content_generation\": {\n      \"count\": 378,\n      \"cost_usd\": 45.23\n    },\n    \"data_extraction\": {\n      \"count\": 400,\n      \"cost_usd\": 43.66\n    }\n  }\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#organization-wide-usage","title":"Organization-Wide Usage","text":"<p>Track usage across all teams in an organization:</p> <p>API Endpoint:</p> <pre><code>GET /api/organizations/{organization_id}/usage?period={period}\n</code></pre> <p>Example Request:</p> <pre><code>curl -X GET \"http://localhost:8003/api/organizations/org_xyz789/usage?period=2025-10\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"organization_id\": \"org_xyz789\",\n  \"period\": \"2025-10\",\n  \"summary\": {\n    \"total_jobs\": 5678,\n    \"completed_jobs\": 5432,\n    \"failed_jobs\": 246,\n    \"credits_used\": 5432,\n    \"total_cost_usd\": 678.90,\n    \"total_tokens\": 5600000\n  },\n  \"teams\": {\n    \"team_abc123\": {\n      \"jobs\": 1234,\n      \"credits_used\": 1180\n    },\n    \"team_def456\": {\n      \"jobs\": 2345,\n      \"credits_used\": 2301\n    },\n    \"team_ghi789\": {\n      \"jobs\": 2099,\n      \"credits_used\": 1951\n    }\n  }\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#database-queries-for-analytics","title":"Database Queries for Analytics","text":"<p>The system uses several tables for tracking usage:</p> <p>1. Jobs Table - Individual job tracking</p> <pre><code>-- Get job statistics for the current month\nSELECT\n    job_type,\n    status,\n    COUNT(*) as job_count,\n    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration_seconds\nFROM jobs\nWHERE created_at &gt;= DATE_TRUNC('month', CURRENT_DATE)\nGROUP BY job_type, status\nORDER BY job_count DESC;\n</code></pre> <p>2. LLM Calls Table - Individual API call tracking</p> <pre><code>-- Get model usage statistics\nSELECT\n    model_group_used,\n    resolved_model,\n    COUNT(*) as call_count,\n    SUM(total_tokens) as total_tokens,\n    SUM(cost_usd) as total_cost_usd,\n    AVG(latency_ms) as avg_latency_ms\nFROM llm_calls\nWHERE created_at &gt;= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY model_group_used, resolved_model\nORDER BY call_count DESC;\n</code></pre> <p>3. Job Cost Summaries Table - Aggregated job costs</p> <pre><code>-- Get cost summary for all jobs\nSELECT\n    j.team_id,\n    j.organization_id,\n    COUNT(DISTINCT j.job_id) as total_jobs,\n    SUM(jcs.total_calls) as total_api_calls,\n    SUM(jcs.total_tokens) as total_tokens,\n    SUM(jcs.total_cost_usd) as total_cost_usd,\n    AVG(jcs.avg_latency_ms) as avg_latency_ms\nFROM jobs j\nJOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nWHERE j.created_at &gt;= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY j.team_id, j.organization_id\nORDER BY total_cost_usd DESC;\n</code></pre> <p>4. Team Usage Summaries Table - Pre-calculated analytics</p> <pre><code>-- Get monthly usage summaries for all teams\nSELECT\n    team_id,\n    period,\n    period_type,\n    total_jobs,\n    successful_jobs,\n    failed_jobs,\n    total_cost_usd,\n    total_tokens,\n    job_type_breakdown\nFROM team_usage_summaries\nWHERE period_type = 'monthly'\n  AND period &gt;= TO_CHAR(CURRENT_DATE - INTERVAL '6 months', 'YYYY-MM')\nORDER BY period DESC, total_cost_usd DESC;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#usage-analytics-dashboard","title":"Usage Analytics Dashboard","text":"<pre><code>// pages/monitoring/usage.jsx\nimport { BarChart, LineChart } from '@/components/Charts';\n\nexport default function UsageAnalytics() {\n  const [period, setPeriod] = useState('2025-10');\n  const [data, setData] = useState(null);\n\n  useEffect(() =&gt; {\n    // Fetch usage data\n    fetch(`/api/analytics/usage?period=${period}`)\n      .then(res =&gt; res.json())\n      .then(data =&gt; setData(data));\n  }, [period]);\n\n  return (\n    &lt;div className=\"usage-analytics\"&gt;\n      &lt;h1&gt;Usage Analytics - {period}&lt;/h1&gt;\n\n      &lt;div className=\"stats-grid\"&gt;\n        &lt;StatCard\n          title=\"Total Jobs\"\n          value={data?.total_jobs}\n          trend={data?.jobs_trend}\n        /&gt;\n        &lt;StatCard\n          title=\"Success Rate\"\n          value={`${data?.success_rate}%`}\n          trend={data?.success_trend}\n        /&gt;\n        &lt;StatCard\n          title=\"Total Cost\"\n          value={`$${data?.total_cost}`}\n          trend={data?.cost_trend}\n        /&gt;\n        &lt;StatCard\n          title=\"Avg Latency\"\n          value={`${data?.avg_latency}ms`}\n          trend={data?.latency_trend}\n        /&gt;\n      &lt;/div&gt;\n\n      &lt;BarChart\n        title=\"Jobs by Type\"\n        data={data?.job_types}\n        xAxis=\"job_type\"\n        yAxis=\"count\"\n      /&gt;\n\n      &lt;LineChart\n        title=\"Daily Usage Trend\"\n        data={data?.daily_usage}\n        xAxis=\"date\"\n        yAxis=\"jobs\"\n      /&gt;\n\n      &lt;TeamUsageTable teams={data?.teams} /&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#credit-consumption-tracking","title":"Credit Consumption Tracking","text":""},{"location":"admin-dashboard/monitoring/#credit-balance-monitoring","title":"Credit Balance Monitoring","text":"<p>Check credit balance for any team:</p> <p>API Endpoint:</p> <pre><code>GET /api/credits/teams/{team_id}/balance\nAuthorization: Bearer {virtual_key}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"team_id\": \"team_abc123\",\n  \"organization_id\": \"org_xyz789\",\n  \"credits_allocated\": 1000,\n  \"credits_used\": 245,\n  \"credits_remaining\": 755,\n  \"credit_limit\": 1000,\n  \"auto_refill\": false,\n  \"refill_amount\": null,\n  \"refill_period\": null,\n  \"created_at\": \"2025-10-01T00:00:00Z\",\n  \"updated_at\": \"2025-10-15T14:30:00Z\"\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#credit-transaction-history","title":"Credit Transaction History","text":"<p>Track all credit transactions:</p> <p>API Endpoint:</p> <pre><code>GET /api/credits/teams/{team_id}/transactions?limit=50\nAuthorization: Bearer {virtual_key}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"team_id\": \"team_abc123\",\n  \"total\": 15,\n  \"transactions\": [\n    {\n      \"transaction_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"team_id\": \"team_abc123\",\n      \"organization_id\": \"org_xyz789\",\n      \"job_id\": \"7c9e6679-7425-40de-944b-e07fc1f90ae7\",\n      \"transaction_type\": \"deduction\",\n      \"credits_amount\": -1,\n      \"credits_before\": 756,\n      \"credits_after\": 755,\n      \"reason\": \"Job document_analysis completed successfully\",\n      \"created_at\": \"2025-10-15T14:30:00Z\"\n    },\n    {\n      \"transaction_id\": \"660e8400-e29b-41d4-a716-446655440001\",\n      \"team_id\": \"team_abc123\",\n      \"organization_id\": \"org_xyz789\",\n      \"job_id\": null,\n      \"transaction_type\": \"allocation\",\n      \"credits_amount\": 500,\n      \"credits_before\": 256,\n      \"credits_after\": 756,\n      \"reason\": \"Monthly credit allocation\",\n      \"created_at\": \"2025-10-01T00:00:00Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#credit-consumption-analytics","title":"Credit Consumption Analytics","text":"<p>Query credit usage patterns:</p> <pre><code>-- Daily credit consumption trend\nSELECT\n    DATE(created_at) as date,\n    SUM(CASE WHEN transaction_type = 'deduction' THEN ABS(credits_amount) ELSE 0 END) as credits_used,\n    SUM(CASE WHEN transaction_type = 'allocation' THEN credits_amount ELSE 0 END) as credits_added\nFROM credit_transactions\nWHERE team_id = 'team_abc123'\n  AND created_at &gt;= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY DATE(created_at)\nORDER BY date DESC;\n</code></pre> <pre><code>-- Top teams by credit consumption\nSELECT\n    tc.team_id,\n    tc.organization_id,\n    tc.credits_allocated,\n    tc.credits_used,\n    tc.credits_remaining,\n    ROUND((tc.credits_used::float / NULLIF(tc.credits_allocated, 0) * 100), 2) as usage_percentage\nFROM team_credits tc\nWHERE tc.credits_allocated &gt; 0\nORDER BY usage_percentage DESC\nLIMIT 20;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#low-credit-alerts","title":"Low Credit Alerts","text":"<p>Monitor teams approaching credit exhaustion:</p> <pre><code># services/credit_alerts.py\nfrom sqlalchemy import and_\nfrom models.credits import TeamCredits\nfrom models.organizations import Organization\n\ndef check_low_credit_teams(db: Session, threshold_percent: float = 20.0):\n    \"\"\"\n    Find teams with credits below threshold percentage\n    \"\"\"\n    teams = db.query(TeamCredits).filter(\n        and_(\n            TeamCredits.credits_allocated &gt; 0,\n            (TeamCredits.credits_remaining / TeamCredits.credits_allocated * 100) &lt;= threshold_percent\n        )\n    ).all()\n\n    alerts = []\n    for team in teams:\n        org = db.query(Organization).filter(\n            Organization.organization_id == team.organization_id\n        ).first()\n\n        alerts.append({\n            \"team_id\": team.team_id,\n            \"organization_id\": team.organization_id,\n            \"organization_name\": org.name if org else \"Unknown\",\n            \"credits_remaining\": team.credits_remaining,\n            \"credits_allocated\": team.credits_allocated,\n            \"usage_percent\": round((team.credits_used / team.credits_allocated * 100), 2)\n        })\n\n    return alerts\n\n# Usage\nlow_credit_teams = check_low_credit_teams(db, threshold_percent=10.0)\nfor team in low_credit_teams:\n    send_alert(team)\n</code></pre>"},{"location":"admin-dashboard/monitoring/#performance-metrics","title":"Performance Metrics","text":""},{"location":"admin-dashboard/monitoring/#latency-tracking","title":"Latency Tracking","text":"<p>Monitor API response times:</p> <pre><code>-- Average latency by model group\nSELECT\n    model_group_used,\n    COUNT(*) as call_count,\n    AVG(latency_ms) as avg_latency_ms,\n    MIN(latency_ms) as min_latency_ms,\n    MAX(latency_ms) as max_latency_ms,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY latency_ms) as p50_latency_ms,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) as p95_latency_ms,\n    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY latency_ms) as p99_latency_ms\nFROM llm_calls\nWHERE created_at &gt;= CURRENT_DATE - INTERVAL '24 hours'\n  AND latency_ms IS NOT NULL\nGROUP BY model_group_used\nORDER BY avg_latency_ms DESC;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#error-rate-monitoring","title":"Error Rate Monitoring","text":"<p>Track failure rates:</p> <pre><code>-- Error rates by job type\nSELECT\n    job_type,\n    COUNT(*) as total_jobs,\n    SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_jobs,\n    ROUND((SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END)::float / COUNT(*) * 100), 2) as error_rate\nFROM jobs\nWHERE created_at &gt;= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY job_type\nORDER BY error_rate DESC;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#success-rate-dashboard","title":"Success Rate Dashboard","text":"<pre><code>// components/PerformanceMetrics.jsx\nexport function PerformanceMetrics({ data }) {\n  return (\n    &lt;div className=\"performance-metrics\"&gt;\n      &lt;MetricCard\n        title=\"Success Rate\"\n        value={`${data.success_rate}%`}\n        description=\"Jobs completed successfully\"\n        trend={data.success_trend}\n      /&gt;\n\n      &lt;MetricCard\n        title=\"Average Latency\"\n        value={`${data.avg_latency}ms`}\n        description=\"API response time (P95)\"\n        trend={data.latency_trend}\n      /&gt;\n\n      &lt;MetricCard\n        title=\"Error Rate\"\n        value={`${data.error_rate}%`}\n        description=\"Failed requests\"\n        trend={data.error_trend}\n      /&gt;\n\n      &lt;MetricCard\n        title=\"Throughput\"\n        value={`${data.requests_per_minute}/min`}\n        description=\"Requests per minute\"\n        trend={data.throughput_trend}\n      /&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#alerts-notifications","title":"Alerts &amp; Notifications","text":""},{"location":"admin-dashboard/monitoring/#alert-types","title":"Alert Types","text":"<p>1. System Alerts - Service downtime - Database connection failures - High error rates - Performance degradation</p> <p>2. Usage Alerts - Unusual traffic spikes - Team approaching credit limit - Zero credit remaining - High cost anomalies</p> <p>3. Security Alerts - Multiple failed authentication attempts - Suspicious API usage patterns - Rate limit violations - Unauthorized access attempts</p>"},{"location":"admin-dashboard/monitoring/#alert-configuration","title":"Alert Configuration","text":"<pre><code># config/alerts.py\nALERT_THRESHOLDS = {\n    \"credit_low\": 10,  # Percentage\n    \"credit_critical\": 0,  # Credits remaining\n    \"error_rate_high\": 5.0,  # Percentage\n    \"latency_high\": 5000,  # Milliseconds\n    \"cost_spike\": 200.0,  # Percent increase\n}\n\nALERT_CHANNELS = {\n    \"email\": [\"admin@company.com\", \"ops@company.com\"],\n    \"slack\": \"https://hooks.slack.com/services/...\",\n    \"webhook\": \"https://your-monitoring-system.com/alerts\"\n}\n</code></pre>"},{"location":"admin-dashboard/monitoring/#alert-implementation","title":"Alert Implementation","text":"<pre><code># services/alert_manager.py\nimport httpx\nfrom typing import Dict, Any\n\nclass AlertManager:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n\n    async def send_alert(self, alert_type: str, message: str, severity: str = \"warning\"):\n        \"\"\"\n        Send alert to configured channels\n        \"\"\"\n        alert_data = {\n            \"type\": alert_type,\n            \"message\": message,\n            \"severity\": severity,\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n\n        # Send to email\n        if \"email\" in self.config:\n            await self._send_email(alert_data)\n\n        # Send to Slack\n        if \"slack\" in self.config:\n            await self._send_slack(alert_data)\n\n        # Send to webhook\n        if \"webhook\" in self.config:\n            await self._send_webhook(alert_data)\n\n    async def _send_slack(self, alert_data: Dict[str, Any]):\n        \"\"\"Send alert to Slack\"\"\"\n        webhook_url = self.config[\"slack\"]\n\n        payload = {\n            \"text\": f\"\ud83d\udea8 {alert_data['severity'].upper()}: {alert_data['message']}\",\n            \"attachments\": [\n                {\n                    \"color\": self._get_color(alert_data['severity']),\n                    \"fields\": [\n                        {\"title\": \"Type\", \"value\": alert_data['type'], \"short\": True},\n                        {\"title\": \"Time\", \"value\": alert_data['timestamp'], \"short\": True}\n                    ]\n                }\n            ]\n        }\n\n        async with httpx.AsyncClient() as client:\n            await client.post(webhook_url, json=payload)\n\n    def _get_color(self, severity: str) -&gt; str:\n        colors = {\n            \"info\": \"#36a64f\",\n            \"warning\": \"#ff9800\",\n            \"error\": \"#f44336\",\n            \"critical\": \"#9c27b0\"\n        }\n        return colors.get(severity, \"#808080\")\n\n# Usage\nalert_manager = AlertManager(ALERT_CHANNELS)\n\n# Low credit alert\nif team.credits_remaining &lt;= ALERT_THRESHOLDS[\"credit_critical\"]:\n    await alert_manager.send_alert(\n        alert_type=\"credit_critical\",\n        message=f\"Team {team.team_id} has {team.credits_remaining} credits remaining\",\n        severity=\"critical\"\n    )\n</code></pre>"},{"location":"admin-dashboard/monitoring/#scheduled-alert-checks","title":"Scheduled Alert Checks","text":"<pre><code># scripts/check_alerts.py\nimport asyncio\nfrom services.alert_manager import AlertManager, ALERT_THRESHOLDS\n\nasync def check_all_alerts():\n    \"\"\"\n    Run all alert checks\n    \"\"\"\n    alert_manager = AlertManager(ALERT_CHANNELS)\n\n    # Check low credits\n    low_credit_teams = check_low_credit_teams(db, ALERT_THRESHOLDS[\"credit_low\"])\n    for team in low_credit_teams:\n        await alert_manager.send_alert(\n            \"credit_low\",\n            f\"Team {team['team_id']} has {team['credits_remaining']} credits ({team['usage_percent']}% used)\",\n            \"warning\"\n        )\n\n    # Check high error rates\n    error_stats = check_error_rates(db)\n    for stat in error_stats:\n        if stat['error_rate'] &gt; ALERT_THRESHOLDS[\"error_rate_high\"]:\n            await alert_manager.send_alert(\n                \"error_rate_high\",\n                f\"Job type {stat['job_type']} has {stat['error_rate']}% error rate\",\n                \"error\"\n            )\n\n    # Check high latency\n    latency_stats = check_latency(db)\n    for stat in latency_stats:\n        if stat['p95_latency'] &gt; ALERT_THRESHOLDS[\"latency_high\"]:\n            await alert_manager.send_alert(\n                \"latency_high\",\n                f\"Model group {stat['model_group']} has {stat['p95_latency']}ms P95 latency\",\n                \"warning\"\n            )\n\nif __name__ == \"__main__\":\n    asyncio.run(check_all_alerts())\n</code></pre> <p>Schedule via cron:</p> <pre><code># crontab -e\n*/15 * * * * python /path/to/check_alerts.py\n</code></pre>"},{"location":"admin-dashboard/monitoring/#monitoring-best-practices","title":"Monitoring Best Practices","text":""},{"location":"admin-dashboard/monitoring/#1-set-up-comprehensive-logging","title":"1. Set Up Comprehensive Logging","text":"<pre><code># config/logging.py\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\ndef setup_logging():\n    logger = logging.getLogger('saas_llm')\n    logger.setLevel(logging.INFO)\n\n    # File handler with rotation\n    handler = RotatingFileHandler(\n        'logs/saas_llm.log',\n        maxBytes=10485760,  # 10MB\n        backupCount=10\n    )\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    return logger\n</code></pre>"},{"location":"admin-dashboard/monitoring/#2-track-key-business-metrics","title":"2. Track Key Business Metrics","text":"<pre><code>-- Create a view for quick dashboard queries\nCREATE VIEW monitoring_dashboard AS\nSELECT\n    COUNT(DISTINCT j.team_id) as active_teams,\n    COUNT(j.job_id) as total_jobs_today,\n    SUM(CASE WHEN j.status = 'completed' THEN 1 ELSE 0 END) as successful_jobs_today,\n    SUM(CASE WHEN j.status = 'failed' THEN 1 ELSE 0 END) as failed_jobs_today,\n    SUM(jcs.total_cost_usd) as total_cost_today,\n    AVG(jcs.avg_latency_ms) as avg_latency_today\nFROM jobs j\nLEFT JOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nWHERE j.created_at &gt;= CURRENT_DATE;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#3-implement-real-time-dashboards","title":"3. Implement Real-Time Dashboards","text":"<p>Use WebSockets for live updates:</p> <pre><code>// Real-time monitoring\nconst ws = new WebSocket('ws://localhost:8003/ws/monitoring');\n\nws.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n\n  // Update dashboard in real-time\n  updateMetrics(data);\n};\n</code></pre>"},{"location":"admin-dashboard/monitoring/#4-regular-performance-reviews","title":"4. Regular Performance Reviews","text":"<p>Schedule weekly/monthly performance reviews:</p> <pre><code>-- Weekly performance report\nSELECT\n    DATE_TRUNC('week', j.created_at) as week,\n    COUNT(*) as total_jobs,\n    AVG(jcs.total_cost_usd) as avg_cost_per_job,\n    AVG(jcs.avg_latency_ms) as avg_latency,\n    SUM(CASE WHEN j.status = 'failed' THEN 1 ELSE 0 END)::float / COUNT(*) * 100 as error_rate\nFROM jobs j\nJOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nWHERE j.created_at &gt;= CURRENT_DATE - INTERVAL '12 weeks'\nGROUP BY week\nORDER BY week DESC;\n</code></pre>"},{"location":"admin-dashboard/monitoring/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/monitoring/#high-latency-issues","title":"High Latency Issues","text":"<p>Problem: Average latency above 2000ms</p> <p>Investigation:</p> <pre><code>-- Find slow API calls\nSELECT\n    call_id,\n    job_id,\n    model_group_used,\n    resolved_model,\n    latency_ms,\n    total_tokens,\n    created_at\nFROM llm_calls\nWHERE latency_ms &gt; 5000\n  AND created_at &gt;= CURRENT_DATE - INTERVAL '24 hours'\nORDER BY latency_ms DESC\nLIMIT 50;\n</code></pre> <p>Solutions: 1. Check LiteLLM proxy logs 2. Verify model provider status 3. Consider adding timeout configurations 4. Review model selection strategy</p>"},{"location":"admin-dashboard/monitoring/#missing-usage-data","title":"Missing Usage Data","text":"<p>Problem: Usage summaries not updating</p> <p>Solution: Manually recalculate summaries</p> <pre><code># scripts/recalculate_usage.py\nfrom services.usage_calculator import calculate_team_usage\n\ndef recalculate_all_usage(period: str):\n    teams = db.query(TeamCredits).all()\n\n    for team in teams:\n        summary = calculate_team_usage(\n            db,\n            team_id=team.team_id,\n            period=period\n        )\n\n        # Store summary\n        db.merge(summary)\n\n    db.commit()\n\n# Usage\nrecalculate_all_usage(\"2025-10\")\n</code></pre>"},{"location":"admin-dashboard/monitoring/#next-steps","title":"Next Steps","text":"<p>Now that you understand monitoring:</p> <ol> <li>Set Up Alerts - Configure notifications</li> <li>Analyze Costs - Deep dive into costs</li> <li>Optimize Performance - Improve latency</li> </ol>"},{"location":"admin-dashboard/monitoring/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Reference - Monitoring endpoints</li> <li>Database Schema - Tracking tables</li> <li>Performance Tuning - Optimization guide</li> </ul>"},{"location":"admin-dashboard/organizations/","title":"Organizations","text":"<p>Learn how to create and manage organizations in the SaaS LiteLLM platform.</p>"},{"location":"admin-dashboard/organizations/#what-are-organizations","title":"What are Organizations?","text":"<p>Organizations are the top-level entity in the SaaS LiteLLM hierarchy. They represent your customers, clients, or business units.</p> <p>Hierarchy: <pre><code>Organization\n  \u2514\u2500\u2500 Teams\n       \u2514\u2500\u2500 Model Access Groups\n            \u2514\u2500\u2500 Model Aliases\n</code></pre></p> <p>Key Concepts: - One organization can have multiple teams - Organizations provide logical grouping - Usage can be tracked per organization - Billing is typically done at the organization level</p>"},{"location":"admin-dashboard/organizations/#organization-structure","title":"Organization Structure","text":""},{"location":"admin-dashboard/organizations/#organization-properties","title":"Organization Properties","text":"Property Type Description <code>organization_id</code> string Unique identifier (e.g., \"org_acme\") <code>name</code> string Display name (e.g., \"ACME Corporation\") <code>metadata</code> object Custom data (industry, tier, etc.) <code>created_at</code> timestamp When organization was created <code>status</code> string active, suspended"},{"location":"admin-dashboard/organizations/#example-organization","title":"Example Organization","text":"<pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"tier\": \"Enterprise\",\n    \"account_manager\": \"jane@yourcompany.com\",\n    \"billing_email\": \"billing@acme.com\"\n  },\n  \"created_at\": \"2024-10-14T12:00:00Z\",\n  \"status\": \"active\"\n}\n</code></pre>"},{"location":"admin-dashboard/organizations/#creating-organizations","title":"Creating Organizations","text":"<p> Organizations management interface - create and manage customer organizations</p>"},{"location":"admin-dashboard/organizations/#via-admin-dashboard","title":"Via Admin Dashboard","text":"<ol> <li>Navigate to Organizations</li> <li>Click \"Organizations\" in the sidebar</li> <li> <p>Click \"Create Organization\" button</p> </li> <li> <p>Fill in Details</p> </li> <li>Organization ID: Unique identifier (lowercase, hyphens, e.g., \"org-acme\")</li> <li>Name: Display name (e.g., \"ACME Corporation\")</li> <li> <p>Metadata (optional): Add custom fields</p> </li> <li> <p>Save</p> </li> <li>Click \"Create\"</li> <li>Organization is created immediately</li> </ol>"},{"location":"admin-dashboard/organizations/#via-api","title":"Via API","text":"<pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"name\": \"ACME Corporation\",\n    \"metadata\": {\n      \"industry\": \"Technology\",\n      \"tier\": \"Enterprise\"\n    }\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"tier\": \"Enterprise\"\n  },\n  \"created_at\": \"2024-10-14T12:00:00Z\",\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"admin-dashboard/organizations/#naming-conventions","title":"Naming Conventions","text":"<p>Organization ID Best Practices:</p> <p>\u2705 Good: - <code>org_acme</code> - <code>org-acme-corp</code> - <code>acme_corporation</code></p> <p>\u274c Avoid: - <code>ACME Corp</code> (spaces) - <code>org/acme</code> (special characters) - <code>123acme</code> (starting with numbers)</p> <p>Recommendations: - Use prefix like <code>org_</code> for clarity - Use lowercase - Use hyphens or underscores for spaces - Keep it short but descriptive - Make it URL-safe</p>"},{"location":"admin-dashboard/organizations/#viewing-organizations","title":"Viewing Organizations","text":""},{"location":"admin-dashboard/organizations/#list-all-organizations","title":"List All Organizations","text":"<p>Via Dashboard: - Navigate to Organizations - See table with all organizations - Search by name or ID - Filter by status</p> <p>Via API: <pre><code>curl http://localhost:8003/api/organizations\n</code></pre></p> <p>Response: <pre><code>{\n  \"organizations\": [\n    {\n      \"organization_id\": \"org_acme\",\n      \"name\": \"ACME Corporation\",\n      \"team_count\": 5,\n      \"total_credits_allocated\": 10000,\n      \"status\": \"active\"\n    },\n    {\n      \"organization_id\": \"org_techco\",\n      \"name\": \"TechCo Inc\",\n      \"team_count\": 2,\n      \"total_credits_allocated\": 5000,\n      \"status\": \"active\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"admin-dashboard/organizations/#view-organization-details","title":"View Organization Details","text":"<p>Via Dashboard: - Click on organization name - See full details including:   - All teams in organization   - Total credits allocated   - Usage statistics   - Recent activity</p> <p>Via API: <pre><code>curl http://localhost:8003/api/organizations/org_acme\n</code></pre></p> <p>Response: <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"tier\": \"Enterprise\"\n  },\n  \"teams\": [\n    {\n      \"team_id\": \"acme-dev\",\n      \"team_alias\": \"ACME Dev Team\",\n      \"credits_remaining\": 850,\n      \"status\": \"active\"\n    },\n    {\n      \"team_id\": \"acme-prod\",\n      \"team_alias\": \"ACME Production\",\n      \"credits_remaining\": 1200,\n      \"status\": \"active\"\n    }\n  ],\n  \"usage_summary\": {\n    \"total_jobs\": 1543,\n    \"total_cost_usd\": 234.56,\n    \"credits_used\": 1543\n  }\n}\n</code></pre></p>"},{"location":"admin-dashboard/organizations/#managing-organizations","title":"Managing Organizations","text":""},{"location":"admin-dashboard/organizations/#update-organization","title":"Update Organization","text":"<p>Via Dashboard: - Click organization name - Click \"Edit\" - Update name or metadata - Click \"Save\"</p> <p>Via API: <pre><code>curl -X PUT http://localhost:8003/api/organizations/org_acme \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"ACME Corporation (Updated)\",\n    \"metadata\": {\n      \"industry\": \"Technology\",\n      \"tier\": \"Enterprise Plus\",\n      \"renewal_date\": \"2025-01-01\"\n    }\n  }'\n</code></pre></p>"},{"location":"admin-dashboard/organizations/#organization-metadata","title":"Organization Metadata","text":"<p>Use metadata to store custom information:</p> <pre><code>{\n  \"metadata\": {\n    // Billing\n    \"tier\": \"Enterprise\",\n    \"billing_email\": \"billing@acme.com\",\n    \"payment_method\": \"invoice\",\n    \"renewal_date\": \"2025-01-01\",\n\n    // Account Management\n    \"account_manager\": \"jane@yourcompany.com\",\n    \"sales_rep\": \"john@yourcompany.com\",\n    \"support_tier\": \"premium\",\n\n    // Business Info\n    \"industry\": \"Technology\",\n    \"company_size\": \"500-1000\",\n    \"country\": \"United States\",\n\n    // Custom Fields\n    \"contract_id\": \"CONTRACT-2024-123\",\n    \"notes\": \"High-value customer, priority support\"\n  }\n}\n</code></pre>"},{"location":"admin-dashboard/organizations/#organization-teams","title":"Organization Teams","text":""},{"location":"admin-dashboard/organizations/#view-teams-in-organization","title":"View Teams in Organization","text":"<p>Via Dashboard: - Navigate to organization details - See \"Teams\" section - Click \"View All Teams\" for filtered team list</p> <p>Via API: <pre><code>curl http://localhost:8003/api/organizations/org_acme/teams\n</code></pre></p>"},{"location":"admin-dashboard/organizations/#create-team-in-organization","title":"Create Team in Organization","text":"<p>When creating a team, specify the organization_id:</p> <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-analytics\",\n    \"team_alias\": \"ACME Analytics Team\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre> <p> Learn more about creating teams</p>"},{"location":"admin-dashboard/organizations/#organization-usage-analytics","title":"Organization Usage &amp; Analytics","text":""},{"location":"admin-dashboard/organizations/#consolidated-analytics-dashboard","title":"Consolidated Analytics Dashboard","text":"<p>The admin dashboard provides a unified analytics view that combines organization job statistics with usage analytics in a single interface.</p> <p>Via Dashboard: - Navigate to \"Organizations\" in the sidebar - Click on an organization name - Click the \"Analytics\" tab</p> <p>Features: - Job Statistics: Total jobs, success rate, failure rate - Usage Metrics: Total calls, tokens consumed, cost in USD - Team Breakdown: Usage per team with drill-down capability - Job Type Analysis: Distribution by job type - Time Series: Usage trends over customizable date ranges</p> <p>Consolidated View Benefits: - Single page for all organization metrics - No need to navigate between multiple views - Real-time data from jobs and llm_calls tables - Exportable reports in CSV/JSON format</p>"},{"location":"admin-dashboard/organizations/#usage-statistics","title":"Usage Statistics","text":"<p>Track usage across all teams in an organization:</p> <p>Metrics Available: - Total jobs completed - Total API calls made - Total tokens consumed - Total cost in USD - Credits used - Average cost per job - Usage by job type - Usage by team</p> <p>Via Dashboard:</p> <p>The analytics page displays:</p> <ol> <li>Overview Cards</li> <li>Total Jobs (with success/failure breakdown)</li> <li>Total LLM Calls</li> <li>Total Tokens Used</li> <li> <p>Total Cost (USD)</p> </li> <li> <p>Team Usage Table</p> </li> <li>Jobs per team</li> <li>Calls per team</li> <li>Cost per team</li> <li>Tokens per team</li> <li> <p>Click team to drill down</p> </li> <li> <p>Job Type Distribution</p> </li> <li>Pie chart showing job types</li> <li>Count and percentage for each type</li> <li> <p>Cost breakdown by type</p> </li> <li> <p>Timeline Graph</p> </li> <li>Daily/weekly/monthly aggregation</li> <li>Toggle between jobs, calls, tokens, cost</li> <li>Filterable date range</li> </ol> <p>Via API: <pre><code>curl \"http://localhost:8003/api/organizations/org_acme/usage?period=2024-10\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"period\": \"2024-10\",\n  \"summary\": {\n    \"total_jobs\": 1543,\n    \"successful_jobs\": 1501,\n    \"failed_jobs\": 42,\n    \"total_cost_usd\": 234.56,\n    \"total_tokens\": 2500000,\n    \"credits_used\": 1543\n  },\n  \"by_team\": {\n    \"acme-dev\": {\n      \"jobs\": 843,\n      \"cost_usd\": 120.34\n    },\n    \"acme-prod\": {\n      \"jobs\": 700,\n      \"cost_usd\": 114.22\n    }\n  },\n  \"by_job_type\": {\n    \"document_analysis\": {\n      \"count\": 500,\n      \"cost_usd\": 89.50\n    },\n    \"chat_session\": {\n      \"count\": 1043,\n      \"cost_usd\": 145.06\n    }\n  }\n}\n</code></pre></p> <p>Analytics Consolidation</p> <p>Previously, organization analytics required navigating to <code>/jobs/organizations/{id}</code> separately from the organization details page. The analytics have been consolidated into a single \"Analytics\" tab on the organization details page for better user experience.</p>"},{"location":"admin-dashboard/organizations/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor costs at the organization level:</p> <pre><code>curl \"http://localhost:8003/api/organizations/org_acme/costs?start_date=2024-10-01&amp;end_date=2024-10-31\"\n</code></pre> <p>Track: - Actual costs (what you pay LiteLLM providers) - Credits charged (what you charge the organization) - Profit margins - Cost trends over time</p>"},{"location":"admin-dashboard/organizations/#deleting-organizations","title":"Deleting Organizations","text":"<p>Permanent Action</p> <p>Deleting an organization will also delete all teams and associated data. This action cannot be undone!</p>"},{"location":"admin-dashboard/organizations/#prerequisites","title":"Prerequisites","text":"<p>Before deleting an organization:</p> <ol> <li>Export data if needed</li> <li>Notify teams that access will be revoked</li> <li>Complete billing if applicable</li> <li>Ensure no active jobs</li> </ol>"},{"location":"admin-dashboard/organizations/#via-dashboard","title":"Via Dashboard","text":"<ol> <li>Navigate to organization details</li> <li>Scroll to bottom</li> <li>Click \"Delete Organization\"</li> <li>Type organization ID to confirm</li> <li>Click \"Confirm Delete\"</li> </ol>"},{"location":"admin-dashboard/organizations/#via-api_1","title":"Via API","text":"<pre><code>curl -X DELETE http://localhost:8003/api/organizations/org_acme \\\n  -H \"Content-Type: application/json\"\n</code></pre> <p>What Gets Deleted: - Organization record - All teams in the organization - All virtual keys - Job history (optional - can be archived) - Usage statistics (optional - can be archived)</p>"},{"location":"admin-dashboard/organizations/#common-workflows","title":"Common Workflows","text":""},{"location":"admin-dashboard/organizations/#workflow-1-onboard-new-customer","title":"Workflow 1: Onboard New Customer","text":"<pre><code># 1. Create organization\ncurl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_newcustomer\",\n    \"name\": \"New Customer Inc\",\n    \"metadata\": {\n      \"tier\": \"Standard\",\n      \"billing_email\": \"billing@newcustomer.com\"\n    }\n  }'\n\n# 2. Create initial team\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_newcustomer\",\n    \"team_id\": \"newcustomer-prod\",\n    \"team_alias\": \"Production Team\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n\n# 3. Share virtual key with customer\n# (Retrieved from team creation response)\n</code></pre>"},{"location":"admin-dashboard/organizations/#workflow-2-upgrade-organization-tier","title":"Workflow 2: Upgrade Organization Tier","text":"<pre><code># Update organization metadata\ncurl -X PUT http://localhost:8003/api/organizations/org_acme \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"ACME Corporation\",\n    \"metadata\": {\n      \"tier\": \"Enterprise Plus\",\n      \"upgraded_at\": \"2024-10-14\"\n    }\n  }'\n\n# Add credits to all teams\n# (Done individually per team or via bulk operation)\n</code></pre>"},{"location":"admin-dashboard/organizations/#workflow-3-offboard-customer","title":"Workflow 3: Offboard Customer","text":"<pre><code># 1. Suspend all teams\ncurl -X POST http://localhost:8003/api/teams/acme-dev/suspend\ncurl -X POST http://localhost:8003/api/teams/acme-prod/suspend\n\n# 2. Export usage data for final billing\ncurl \"http://localhost:8003/api/organizations/org_acme/usage?period=all\" &gt; acme_usage.json\n\n# 3. Delete organization (after grace period)\ncurl -X DELETE http://localhost:8003/api/organizations/org_acme\n</code></pre>"},{"location":"admin-dashboard/organizations/#best-practices","title":"Best Practices","text":""},{"location":"admin-dashboard/organizations/#naming","title":"Naming","text":"<ol> <li>Use consistent prefixes: <code>org_</code> for all organizations</li> <li>Keep it short: Easier to reference in logs and code</li> <li>Use descriptive names: Helps identify customer quickly</li> <li>Avoid PII: Don't use customer email or personal data</li> </ol>"},{"location":"admin-dashboard/organizations/#metadata","title":"Metadata","text":"<ol> <li>Store billing info: Contact, payment method, renewal dates</li> <li>Track account management: Who manages this account internally</li> <li>Business classification: Industry, size, tier</li> <li>Custom fields: Contract IDs, notes, tags</li> </ol>"},{"location":"admin-dashboard/organizations/#organization-management","title":"Organization Management","text":"<ol> <li>Review usage monthly: Check which organizations are heavy users</li> <li>Monitor growth: Track team count over time</li> <li>Identify opportunities: Upgrade candidates, upsell potential</li> <li>Clean up: Archive or delete inactive organizations</li> </ol>"},{"location":"admin-dashboard/organizations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/organizations/#cant-create-organization","title":"Can't Create Organization","text":"<p>Problem: \"Organization ID already exists\"</p> <p>Solution: Choose a different organization_id. Each must be unique.</p>"},{"location":"admin-dashboard/organizations/#cant-delete-organization","title":"Can't Delete Organization","text":"<p>Problem: \"Organization has active teams\"</p> <p>Solution: Delete or suspend all teams first, then delete organization.</p>"},{"location":"admin-dashboard/organizations/#missing-usage-data","title":"Missing Usage Data","text":"<p>Problem: Organization shows no usage statistics</p> <p>Solution: 1. Check that teams have completed jobs 2. Verify database connection 3. Check date range filters 4. Ensure jobs completed (not just created)</p>"},{"location":"admin-dashboard/organizations/#next-steps","title":"Next Steps","text":"<p>Now that you understand organizations:</p> <ol> <li>Create Teams - Add teams to your organization</li> <li>Set Up Model Access - Control which models teams can use</li> <li>Allocate Credits - Give teams credits to make API calls</li> <li>Monitor Usage - Track organization usage and costs</li> </ol>"},{"location":"admin-dashboard/overview/","title":"Admin Dashboard Overview","text":"<p>The SaaS LiteLLM Admin Dashboard is a Next.js web application for managing your platform, organizations, teams, and model access.</p>"},{"location":"admin-dashboard/overview/#what-is-the-admin-dashboard","title":"What is the Admin Dashboard?","text":"<p>The Admin Dashboard provides a user-friendly interface for platform administrators to:</p> <ul> <li>\u2705 Create and manage organizations</li> <li>\u2705 Create and manage teams with virtual keys</li> <li>\u2705 Configure model access groups</li> <li>\u2705 Define model aliases with pricing</li> <li>\u2705 Allocate and monitor credits</li> <li>\u2705 Suspend and resume teams</li> <li>\u2705 Monitor usage and costs</li> </ul> <p>Access URL (Local): http://localhost:3002</p> <p>Access URL (Production): https://your-admin-dashboard.com</p>"},{"location":"admin-dashboard/overview/#screenshots","title":"Screenshots","text":""},{"location":"admin-dashboard/overview/#first-time-setup","title":"First-Time Setup","text":"<p> Create your first admin account with the owner role</p>"},{"location":"admin-dashboard/overview/#dashboard-overview","title":"Dashboard Overview","text":"<p> Monitor your LiteLLM infrastructure at a glance</p>"},{"location":"admin-dashboard/overview/#organizations-management","title":"Organizations Management","text":"<p> Create and manage customer organizations</p>"},{"location":"admin-dashboard/overview/#teams-management","title":"Teams Management","text":"<p> Manage teams, credits, and virtual API keys</p>"},{"location":"admin-dashboard/overview/#model-access-groups","title":"Model Access Groups","text":"<p> Control which models teams can access</p>"},{"location":"admin-dashboard/overview/#model-aliases","title":"Model Aliases","text":"<p> Create semantic model names with pricing configuration</p> <p>Built on LiteLLM</p> <p>The Admin Dashboard manages the SaaS layer built on top of LiteLLM. It provides a UI for features like job-based cost tracking and multi-tenant architecture that SaaS LiteLLM adds to the core LiteLLM routing capabilities.</p>"},{"location":"admin-dashboard/overview/#key-features","title":"Key Features","text":""},{"location":"admin-dashboard/overview/#organization-management","title":"\ud83c\udfe2 Organization Management","text":"<ul> <li>Create top-level organizations</li> <li>Manage organization metadata</li> <li>View all teams in an organization</li> <li>Track organization-level usage</li> </ul> <p> Learn more about organizations</p>"},{"location":"admin-dashboard/overview/#team-management","title":"\ud83d\udc65 Team Management","text":"<ul> <li>Create teams within organizations</li> <li>Generate virtual API keys automatically</li> <li>Assign model access groups to teams</li> <li>Set credit allocations</li> <li>Suspend/resume teams</li> <li>View team statistics</li> </ul> <p> Learn more about teams</p>"},{"location":"admin-dashboard/overview/#model-access-control","title":"\ud83d\udd10 Model Access Control","text":"<ul> <li>Define model access groups (e.g., \"gpt-models\", \"claude-models\")</li> <li>Assign model aliases to access groups</li> <li>Control which teams can access which models</li> <li>Flexible permission system</li> </ul> <p> Learn more about model access groups</p>"},{"location":"admin-dashboard/overview/#model-aliases_1","title":"\ud83c\udfaf Model Aliases","text":"<ul> <li>Create semantic model names (e.g., \"ChatAgent\" \u2192 \"gpt-4\")</li> <li>Configure pricing per model (input/output tokens)</li> <li>Provider configuration</li> <li>Fallback routing</li> </ul> <p> Learn more about model aliases</p>"},{"location":"admin-dashboard/overview/#credit-management","title":"\ud83d\udcb3 Credit Management","text":"<ul> <li>Allocate credits to teams</li> <li>Monitor credit usage</li> <li>Track credit transactions</li> <li>Set budget modes (job-based, consumption-based)</li> <li>Low credit alerts</li> </ul> <p> Learn more about credits</p>"},{"location":"admin-dashboard/overview/#monitoring-analytics","title":"\ud83d\udcca Monitoring &amp; Analytics","text":"<ul> <li>Real-time usage statistics</li> <li>Cost tracking per team</li> <li>Job monitoring</li> <li>Team activity logs</li> <li>Usage trends</li> </ul> <p> Learn more about monitoring</p>"},{"location":"admin-dashboard/overview/#starting-the-dashboard","title":"Starting the Dashboard","text":""},{"location":"admin-dashboard/overview/#local-development","title":"Local Development","text":"<pre><code>cd admin-dashboard\n\n# Install dependencies (first time only)\nnpm install\n\n# Start development server\nnpm run dev\n</code></pre> <p>The dashboard will be available at: http://localhost:3002</p>"},{"location":"admin-dashboard/overview/#production-deployment","title":"Production Deployment","text":"<p>Deploy to Vercel, Netlify, or any Node.js hosting:</p> <pre><code># Build for production\nnpm run build\n\n# Start production server\nnpm start\n</code></pre>"},{"location":"admin-dashboard/overview/#dashboard-architecture","title":"Dashboard Architecture","text":"<pre><code>graph TD\n    A[Admin Dashboard :3002] --&gt;|HTTP/JSON| B[SaaS API :8003]\n    B --&gt; C[PostgreSQL Database]\n\n    A -.Manage.-&gt; D[Organizations]\n    A -.Manage.-&gt; E[Teams]\n    A -.Manage.-&gt; F[Model Access Groups]\n    A -.Manage.-&gt; G[Model Aliases]\n    A -.Monitor.-&gt; H[Usage &amp; Costs]\n\n    D --&gt; C\n    E --&gt; C\n    F --&gt; C\n    G --&gt; C\n    H --&gt; C\n\n    style A fill:#9C27B0\n    style B fill:#4CAF50\n    style C fill:#FF9800</code></pre> <p>Components: - Admin Dashboard (Next.js) - Web UI on port 3002 - SaaS API - Backend REST API on port 8003 - PostgreSQL - Data storage</p>"},{"location":"admin-dashboard/overview/#common-workflows","title":"Common Workflows","text":""},{"location":"admin-dashboard/overview/#workflow-1-onboard-a-new-customer","title":"Workflow 1: Onboard a New Customer","text":"<ol> <li>Create Organization</li> <li>Navigate to Organizations \u2192 Create</li> <li>Enter organization name and metadata</li> <li> <p>Save organization</p> </li> <li> <p>Create Model Access Group (if needed)</p> </li> <li>Navigate to Model Access \u2192 Create Group</li> <li>Define which models this customer can access</li> <li> <p>Assign model aliases</p> </li> <li> <p>Create Team</p> </li> <li>Navigate to Teams \u2192 Create</li> <li>Select the organization</li> <li>Assign access groups</li> <li>Allocate initial credits</li> <li> <p>Save and copy the virtual key</p> </li> <li> <p>Share Virtual Key</p> </li> <li>Provide virtual key to customer securely</li> <li>Customer can now make API calls</li> </ol>"},{"location":"admin-dashboard/overview/#workflow-2-monitor-team-usage","title":"Workflow 2: Monitor Team Usage","text":"<ol> <li>View Team List</li> <li>Navigate to Teams</li> <li> <p>See all teams with current status</p> </li> <li> <p>Check Team Details</p> </li> <li>Click on a team</li> <li>View credits remaining</li> <li>See recent jobs</li> <li> <p>Check usage statistics</p> </li> <li> <p>Add Credits (if needed)</p> </li> <li>Click \"Add Credits\"</li> <li>Enter amount</li> <li>Confirm transaction</li> </ol>"},{"location":"admin-dashboard/overview/#workflow-3-suspend-a-team","title":"Workflow 3: Suspend a Team","text":"<ol> <li>Navigate to Team</li> <li>Find the team in Teams list</li> <li> <p>Click team name</p> </li> <li> <p>Suspend Team</p> </li> <li>Click \"Suspend\" button</li> <li>Confirm suspension</li> <li> <p>Team can no longer make API calls</p> </li> <li> <p>Resume Team (when ready)</p> </li> <li>Click \"Resume\" button</li> <li>Team access restored</li> </ol> <p> Learn more about suspend/resume</p>"},{"location":"admin-dashboard/overview/#dashboard-sections","title":"Dashboard Sections","text":""},{"location":"admin-dashboard/overview/#organizations-section","title":"Organizations Section","text":"<p>Path: <code>/organizations</code></p> <ul> <li>List all organizations</li> <li>Create new organizations</li> <li>View organization details</li> <li>List teams in organization</li> <li>Organization usage statistics</li> </ul>"},{"location":"admin-dashboard/overview/#teams-section","title":"Teams Section","text":"<p>Path: <code>/teams</code></p> <ul> <li>List all teams</li> <li>Create new teams</li> <li>View team details</li> <li>Manage virtual keys</li> <li>Add/remove credits</li> <li>Suspend/resume teams</li> <li>View team usage</li> </ul>"},{"location":"admin-dashboard/overview/#model-access-section","title":"Model Access Section","text":"<p>Path: <code>/model-access</code></p> <ul> <li>List access groups</li> <li>Create access groups</li> <li>Assign model aliases</li> <li>Manage permissions</li> </ul>"},{"location":"admin-dashboard/overview/#model-aliases-section","title":"Model Aliases Section","text":"<p>Path: <code>/model-aliases</code></p> <ul> <li>List all model aliases</li> <li>Create new aliases</li> <li>Configure pricing</li> <li>Provider settings</li> </ul>"},{"location":"admin-dashboard/overview/#monitoring-section","title":"Monitoring Section","text":"<p>Path: <code>/monitoring</code></p> <ul> <li>Dashboard overview</li> <li>Usage statistics</li> <li>Cost tracking</li> <li>Job monitoring</li> <li>Team activity</li> </ul>"},{"location":"admin-dashboard/overview/#configuration","title":"Configuration","text":""},{"location":"admin-dashboard/overview/#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env.local</code> in the <code>admin-panel/</code> directory:</p> <pre><code># SaaS API URL (required)\nNEXT_PUBLIC_API_URL=http://localhost:8003\n\n# Note: MASTER_KEY is entered by users at login,\n# not stored in this file\n</code></pre>"},{"location":"admin-dashboard/overview/#api-connection","title":"API Connection","text":"<p>The dashboard connects to the SaaS API to: - Fetch data (organizations, teams, etc.) - Create/update/delete resources - Monitor usage and costs</p> <p>All API calls are authenticated (implementation depends on your auth strategy).</p>"},{"location":"admin-dashboard/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"admin-dashboard/overview/#authentication","title":"Authentication","text":"<p>The Admin Dashboard uses MASTER_KEY authentication to protect administrative endpoints.</p> <p>How it works: 1. Users enter their MASTER_KEY on the login page 2. Dashboard validates key by making test request to <code>/api/model-groups</code> 3. Valid key is stored in localStorage 4. All API requests include <code>X-Admin-Key</code> header with the key</p> <p>Setting up:</p> <p>Local Development: <pre><code># .env\nMASTER_KEY=sk-admin-local-dev-change-in-production\n</code></pre></p> <p>Production (Railway): <pre><code># Railway Variables\nMASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\n</code></pre></p> <p>Generate secure keys: <pre><code>openssl rand -hex 32\n# Format: sk-admin-&lt;generated-hex&gt;\n</code></pre></p> <p> Full Authentication Guide</p>"},{"location":"admin-dashboard/overview/#network-security","title":"Network Security","text":"<p>Production Best Practices:</p> <ol> <li>HTTPS Only - Always use HTTPS in production</li> <li>VPN Access - Restrict dashboard to VPN network</li> <li>IP Whitelist - Limit access to specific IPs</li> <li>Rate Limiting - Prevent brute force attacks</li> <li>Audit Logging - Log all admin actions</li> <li>CORS Configuration - Add production admin panel URL to CORS</li> </ol> <p>CORS Configuration Required</p> <p>The admin panel is browser-based, so it requires CORS configuration in the SaaS API.</p> <p>You must add your production admin panel URL to the <code>allow_origins</code> list in <code>src/saas_api.py</code>.</p> <p> Learn how to configure CORS</p>"},{"location":"admin-dashboard/overview/#data-protection","title":"Data Protection","text":"<ul> <li>Don't expose virtual keys in logs</li> <li>Mask sensitive data in UI</li> <li>Implement RBAC if multiple admins</li> <li>Regular security audits</li> </ul>"},{"location":"admin-dashboard/overview/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/overview/#dashboard-wont-start","title":"Dashboard Won't Start","text":"<p>Problem: <code>npm run dev</code> fails</p> <p>Solutions: <pre><code># Clear node_modules and reinstall\nrm -rf node_modules package-lock.json\nnpm install\n\n# Check Node.js version (should be 16+)\nnode --version\n\n# Try with clean cache\nnpm cache clean --force\nnpm install\n</code></pre></p>"},{"location":"admin-dashboard/overview/#cant-connect-to-api","title":"Can't Connect to API","text":"<p>Problem: Dashboard shows \"API connection failed\"</p> <p>Solutions: 1. Verify SaaS API is running: <code>curl http://localhost:8003/health</code> 2. Check <code>NEXT_PUBLIC_API_URL</code> in <code>.env.local</code> 3. Check browser console for CORS errors 4. Verify firewall isn't blocking port 8003</p>"},{"location":"admin-dashboard/overview/#data-not-loading","title":"Data Not Loading","text":"<p>Problem: Lists show empty or \"Loading...\"</p> <p>Solutions: 1. Check browser network tab for API errors 2. Verify database has data 3. Check SaaS API logs for errors 4. Ensure PostgreSQL is running</p>"},{"location":"admin-dashboard/overview/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Action <code>Ctrl/Cmd + K</code> Search <code>G + O</code> Go to Organizations <code>G + T</code> Go to Teams <code>G + M</code> Go to Monitoring <code>N</code> Create New (on list pages) <code>?</code> Show help"},{"location":"admin-dashboard/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the dashboard overview:</p> <ol> <li>Set Up Organizations - Create your first organization</li> <li>Create Teams - Set up teams with virtual keys</li> <li>Configure Model Access - Control model access</li> <li>Manage Credits - Allocate credits to teams</li> <li>Monitor Usage - Track usage and costs</li> </ol>"},{"location":"admin-dashboard/overview/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Reference - Backend API documentation</li> <li>Integration Guide - How teams integrate</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"admin-dashboard/suspend-pause/","title":"Team Suspend &amp; Pause Management","text":"<p>Learn how to suspend and pause teams to control access to your SaaS LiteLLM platform.</p>"},{"location":"admin-dashboard/suspend-pause/#overview","title":"Overview","text":"<p>Team status management allows you to temporarily or permanently restrict team access without deleting their data. This is useful for:</p> <ul> <li>Handling non-payment situations</li> <li>Enforcing terms of service violations</li> <li>Managing trial period expiration</li> <li>Performing maintenance on team resources</li> <li>Implementing temporary service holds</li> </ul>"},{"location":"admin-dashboard/suspend-pause/#understanding-team-status","title":"Understanding Team Status","text":""},{"location":"admin-dashboard/suspend-pause/#active-status","title":"Active Status","text":"<p>Status: <code>active</code></p> <p>Teams with active status have full access to: - Make API calls using their virtual key - Consume credits from their allocation - Access all assigned model groups - View usage statistics and job history</p>"},{"location":"admin-dashboard/suspend-pause/#suspended-status","title":"Suspended Status","text":"<p>Status: <code>suspended</code></p> <p>Teams with suspended status are blocked from all API access:</p> <ul> <li>API Calls: Rejected with 403 Forbidden</li> <li>Credit Usage: No credits can be consumed</li> <li>Data Access: Historical data remains available</li> <li>Virtual Keys: Keys remain valid but non-functional</li> <li>Billing: Team can't incur new charges</li> </ul> <p>Use Cases: - Non-payment or overdue invoices - Terms of service violations - Security concerns or suspected abuse - Account closure pending data retention - Compliance or legal holds</p>"},{"location":"admin-dashboard/suspend-pause/#paused-status","title":"Paused Status","text":"<p>Status: <code>paused</code></p> <p>Teams with paused status have temporary service interruption:</p> <ul> <li>API Calls: Rejected with 429 Too Many Requests</li> <li>Credit Usage: No credits consumed during pause</li> <li>Data Access: Full access to historical data</li> <li>Virtual Keys: Keys remain valid but temporarily inactive</li> <li>Billing: Team won't incur charges while paused</li> </ul> <p>Use Cases: - Scheduled maintenance windows - Customer-requested service pause - Temporary budget holds - Trial period between billing cycles - Seasonal or periodic usage patterns</p>"},{"location":"admin-dashboard/suspend-pause/#key-differences-suspend-vs-pause","title":"Key Differences: Suspend vs Pause","text":"Feature Active Suspended Paused API Access \u2705 Yes \u274c No \u274c No HTTP Status 200 403 Forbidden 429 Too Many Requests Credit Consumption \u2705 Yes \u274c No \u274c No View History \u2705 Yes \u2705 Yes \u2705 Yes Intent Normal operation Permanent/serious block Temporary hold Resumption N/A Admin action required Can be automated Error Message N/A \"Team suspended\" \"Team paused\""},{"location":"admin-dashboard/suspend-pause/#implementation","title":"Implementation","text":""},{"location":"admin-dashboard/suspend-pause/#database-schema","title":"Database Schema","text":"<p>The team status is stored in the <code>organizations</code> table (which stores team metadata through organization relationships):</p> <pre><code>-- Organization status field (from organizations.py)\nCREATE TABLE organizations (\n    organization_id VARCHAR(255) PRIMARY KEY,\n    name VARCHAR(500) NOT NULL,\n    status VARCHAR(50) DEFAULT 'active',  -- 'active', 'suspended', 'paused'\n    metadata JSON DEFAULT '{}',\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n</code></pre> <p>The status field accepts: - <code>'active'</code> - Full access (default) - <code>'suspended'</code> - Blocked access - <code>'paused'</code> - Temporary hold</p>"},{"location":"admin-dashboard/suspend-pause/#status-checking-flow","title":"Status Checking Flow","text":"<pre><code>graph TD\n    A[API Request] --&gt; B{Virtual Key Valid?}\n    B --&gt;|No| C[401 Unauthorized]\n    B --&gt;|Yes| D[Get Team Details]\n    D --&gt; E{Check Organization Status}\n    E --&gt;|Active| F[Process Request]\n    E --&gt;|Suspended| G[403 Forbidden]\n    E --&gt;|Paused| H[429 Too Many Requests]\n\n    style F fill:#4CAF50\n    style G fill:#F44336\n    style H fill:#FF9800</code></pre>"},{"location":"admin-dashboard/suspend-pause/#api-endpoints","title":"API Endpoints","text":""},{"location":"admin-dashboard/suspend-pause/#check-team-status","title":"Check Team Status","text":"<pre><code>GET /api/teams/{team_id}\nAuthorization: Bearer {virtual_key}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"team_id\": \"team_abc123\",\n  \"organization_id\": \"org_xyz789\",\n  \"credits\": {\n    \"credits_allocated\": 1000,\n    \"credits_used\": 245,\n    \"credits_remaining\": 755\n  },\n  \"model_groups\": [\"gpt-models\", \"claude-models\"]\n}\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#check-organization-status","title":"Check Organization Status","text":"<pre><code>GET /api/organizations/{organization_id}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"organization_id\": \"org_xyz789\",\n  \"name\": \"Acme Corporation\",\n  \"status\": \"active\",\n  \"metadata\": {},\n  \"created_at\": \"2025-10-01T12:00:00Z\",\n  \"updated_at\": \"2025-10-15T08:30:00Z\"\n}\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#update-organization-status-admin-only","title":"Update Organization Status (Admin Only)","text":"<p>Endpoint: Currently managed via direct database access. Future API:</p> <pre><code>PUT /api/organizations/{organization_id}/status\nContent-Type: application/json\nAuthorization: Bearer {admin_key}\n\n{\n  \"status\": \"suspended\",\n  \"reason\": \"Payment overdue\"\n}\n</code></pre> <p>Expected Response:</p> <pre><code>{\n  \"organization_id\": \"org_xyz789\",\n  \"status\": \"suspended\",\n  \"previous_status\": \"active\",\n  \"updated_at\": \"2025-10-15T14:30:00Z\",\n  \"reason\": \"Payment overdue\"\n}\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#usage-examples","title":"Usage Examples","text":""},{"location":"admin-dashboard/suspend-pause/#example-1-suspend-team-for-non-payment","title":"Example 1: Suspend Team for Non-Payment","text":"<p>Scenario: Customer hasn't paid their invoice for 30 days.</p> <p>Steps:</p> <ol> <li>Check current status:</li> </ol> <pre><code>curl -X GET http://localhost:8003/api/organizations/org_customer123\n</code></pre> <ol> <li>Update via database (until API endpoint exists):</li> </ol> <pre><code>UPDATE organizations\nSET status = 'suspended',\n    metadata = jsonb_set(\n        metadata::jsonb,\n        '{suspension_reason}',\n        '\"Payment overdue\"'::jsonb\n    ),\n    updated_at = NOW()\nWHERE organization_id = 'org_customer123';\n</code></pre> <ol> <li>Verify team API calls fail:</li> </ol> <pre><code># Team attempts to create a job\ncurl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-team-key-abc123\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"team_abc123\",\n    \"job_type\": \"document_analysis\"\n  }'\n\n# Expected response:\n# HTTP 403 Forbidden\n# {\n#   \"detail\": \"Team access suspended - contact support\"\n# }\n</code></pre> <ol> <li>Resume when payment received:</li> </ol> <pre><code>UPDATE organizations\nSET status = 'active',\n    metadata = jsonb_set(\n        metadata::jsonb,\n        '{suspension_reason}',\n        'null'::jsonb\n    ),\n    updated_at = NOW()\nWHERE organization_id = 'org_customer123';\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#example-2-pause-team-for-maintenance","title":"Example 2: Pause Team for Maintenance","text":"<p>Scenario: Performing model group reconfiguration for a customer.</p> <p>Steps:</p> <ol> <li> <p>Notify customer about upcoming maintenance window</p> </li> <li> <p>Pause team access:</p> </li> </ol> <pre><code>UPDATE organizations\nSET status = 'paused',\n    metadata = jsonb_set(\n        metadata::jsonb,\n        '{pause_reason}',\n        '\"Scheduled maintenance\"'::jsonb\n    ),\n    metadata = jsonb_set(\n        metadata::jsonb,\n        '{pause_until}',\n        '\"2025-10-16T10:00:00Z\"'::jsonb\n    ),\n    updated_at = NOW()\nWHERE organization_id = 'org_customer456';\n</code></pre> <ol> <li>Perform maintenance:</li> </ol> <pre><code># Update team's model groups\ncurl -X PUT http://localhost:8003/api/teams/team_def456/model-groups \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_groups\": [\"gpt-models\", \"claude-models\", \"new-model-group\"]\n  }'\n</code></pre> <ol> <li>Resume team access:</li> </ol> <pre><code>UPDATE organizations\nSET status = 'active',\n    metadata = metadata::jsonb - 'pause_reason' - 'pause_until',\n    updated_at = NOW()\nWHERE organization_id = 'org_customer456';\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#example-3-automated-pause-on-credit-exhaustion","title":"Example 3: Automated Pause on Credit Exhaustion","text":"<p>Scenario: Automatically pause teams when credits run out (with grace period).</p> <p>Implementation:</p> <pre><code># services/credit_monitor.py\nfrom sqlalchemy.orm import Session\nfrom models.credits import TeamCredits\nfrom models.organizations import Organization\n\ndef check_and_pause_low_credit_teams(db: Session, threshold: int = 0):\n    \"\"\"\n    Pause teams that have exhausted their credits\n    \"\"\"\n    # Find teams with no credits remaining\n    low_credit_teams = db.query(TeamCredits).filter(\n        TeamCredits.credits_remaining &lt;= threshold\n    ).all()\n\n    for team_credits in low_credit_teams:\n        # Get the organization\n        org = db.query(Organization).filter(\n            Organization.organization_id == team_credits.organization_id\n        ).first()\n\n        if org and org.status == 'active':\n            # Pause the organization\n            org.status = 'paused'\n            org.org_metadata['pause_reason'] = 'Credit exhausted'\n            org.org_metadata['paused_at'] = datetime.utcnow().isoformat()\n\n            db.commit()\n\n            # Send notification\n            send_notification(\n                team_id=team_credits.team_id,\n                message=\"Team paused - credits exhausted. Add credits to resume.\"\n            )\n\n# Schedule this to run periodically\n# Example: Every 5 minutes via cron or scheduler\n</code></pre> <p>Cron job:</p> <pre><code>*/5 * * * * python /path/to/check_credits.py\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#example-4-bulk-status-update","title":"Example 4: Bulk Status Update","text":"<p>Scenario: Suspend multiple teams for a compliance audit.</p> <pre><code>-- Suspend all teams in a specific organization\nUPDATE organizations\nSET status = 'suspended',\n    metadata = jsonb_set(\n        metadata::jsonb,\n        '{suspension_reason}',\n        '\"Compliance audit in progress\"'::jsonb\n    ),\n    updated_at = NOW()\nWHERE organization_id IN (\n    'org_audit1',\n    'org_audit2',\n    'org_audit3'\n);\n\n-- Resume all suspended teams after audit\nUPDATE organizations\nSET status = 'active',\n    metadata = metadata::jsonb - 'suspension_reason',\n    updated_at = NOW()\nWHERE status = 'suspended'\n  AND metadata-&gt;&gt;'suspension_reason' = 'Compliance audit in progress';\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#admin-dashboard-integration","title":"Admin Dashboard Integration","text":""},{"location":"admin-dashboard/suspend-pause/#dashboard-ui-components","title":"Dashboard UI Components","text":"<p>Team Status Badge:</p> <pre><code>// components/TeamStatusBadge.jsx\nexport function TeamStatusBadge({ status }) {\n  const statusConfig = {\n    active: { color: 'green', text: 'Active', icon: '\u2713' },\n    suspended: { color: 'red', text: 'Suspended', icon: '\u2298' },\n    paused: { color: 'orange', text: 'Paused', icon: '\u23f8' }\n  };\n\n  const config = statusConfig[status] || statusConfig.active;\n\n  return (\n    &lt;span className={`badge badge-${config.color}`}&gt;\n      {config.icon} {config.text}\n    &lt;/span&gt;\n  );\n}\n</code></pre> <p>Status Update Form:</p> <pre><code>// components/UpdateTeamStatus.jsx\nexport function UpdateTeamStatus({ organizationId, currentStatus }) {\n  const [status, setStatus] = useState(currentStatus);\n  const [reason, setReason] = useState('');\n\n  const handleSubmit = async () =&gt; {\n    await fetch(`/api/organizations/${organizationId}/status`, {\n      method: 'PUT',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ status, reason })\n    });\n\n    // Refresh data\n    window.location.reload();\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;select value={status} onChange={e =&gt; setStatus(e.target.value)}&gt;\n        &lt;option value=\"active\"&gt;Active&lt;/option&gt;\n        &lt;option value=\"paused\"&gt;Paused&lt;/option&gt;\n        &lt;option value=\"suspended\"&gt;Suspended&lt;/option&gt;\n      &lt;/select&gt;\n\n      &lt;input\n        type=\"text\"\n        placeholder=\"Reason (optional)\"\n        value={reason}\n        onChange={e =&gt; setReason(e.target.value)}\n      /&gt;\n\n      &lt;button type=\"submit\"&gt;Update Status&lt;/button&gt;\n    &lt;/form&gt;\n  );\n}\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#best-practices","title":"Best Practices","text":""},{"location":"admin-dashboard/suspend-pause/#1-always-document-status-changes","title":"1. Always Document Status Changes","text":"<p>Store reason and timestamp in organization metadata:</p> <pre><code>{\n  \"status_history\": [\n    {\n      \"status\": \"suspended\",\n      \"reason\": \"Payment overdue\",\n      \"changed_by\": \"admin@company.com\",\n      \"changed_at\": \"2025-10-15T14:30:00Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#2-notify-teams-before-status-changes","title":"2. Notify Teams Before Status Changes","text":"<p>Send email/webhook notifications:</p> <pre><code>def suspend_team_with_notification(org_id: str, reason: str):\n    # Send warning email\n    send_email(\n        to=get_org_email(org_id),\n        subject=\"Team Suspension Notice\",\n        body=f\"Your team will be suspended. Reason: {reason}\"\n    )\n\n    # Wait for 24 hours grace period\n    schedule_task(\n        task=apply_suspension,\n        args=[org_id, reason],\n        delay=86400  # 24 hours\n    )\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#3-implement-graduated-responses","title":"3. Implement Graduated Responses","text":"<pre><code>def handle_overdue_payment(days_overdue: int, org_id: str):\n    if days_overdue &gt;= 30:\n        suspend_team(org_id, \"Payment 30+ days overdue\")\n    elif days_overdue &gt;= 14:\n        pause_team(org_id, \"Payment 14+ days overdue\")\n    elif days_overdue &gt;= 7:\n        send_warning(org_id, \"Payment 7 days overdue\")\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#4-provide-self-service-resume","title":"4. Provide Self-Service Resume","text":"<p>Allow customers to resume paused teams:</p> <pre><code>@app.post(\"/api/organizations/{org_id}/resume\")\nasync def resume_team(org_id: str, payment_proof: str):\n    # Verify payment\n    if verify_payment(payment_proof):\n        org = db.query(Organization).filter(\n            Organization.organization_id == org_id\n        ).first()\n\n        if org.status == 'paused':\n            org.status = 'active'\n            db.commit()\n            return {\"status\": \"resumed\"}\n\n    raise HTTPException(400, \"Cannot resume team\")\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#5-monitor-status-changes","title":"5. Monitor Status Changes","text":"<p>Log all status changes for audit:</p> <pre><code>def audit_log_status_change(\n    org_id: str,\n    old_status: str,\n    new_status: str,\n    reason: str,\n    admin_id: str\n):\n    log_entry = {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"organization_id\": org_id,\n        \"old_status\": old_status,\n        \"new_status\": new_status,\n        \"reason\": reason,\n        \"changed_by\": admin_id\n    }\n\n    # Store in audit log table\n    db.add(AuditLog(**log_entry))\n    db.commit()\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/suspend-pause/#team-cant-resume-after-payment","title":"Team Can't Resume After Payment","text":"<p>Problem: Updated organization status to 'active' but team still can't access API.</p> <p>Solutions:</p> <ol> <li>Check team's organization mapping:</li> </ol> <pre><code>SELECT tc.team_id, tc.organization_id, o.status\nFROM team_credits tc\nJOIN organizations o ON tc.organization_id = o.organization_id\nWHERE tc.team_id = 'team_abc123';\n</code></pre> <ol> <li>Verify virtual key is still valid:</li> </ol> <pre><code>curl -X GET http://localhost:8002/key/info \\\n  -H \"Authorization: Bearer {master_key}\" \\\n  -d '{\"keys\": [\"sk-team-key-abc123\"]}'\n</code></pre> <ol> <li>Check LiteLLM team budget:</li> </ol> <pre><code>curl -X GET http://localhost:8002/team/info?team_id=team_abc123 \\\n  -H \"Authorization: Bearer {master_key}\"\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#status-changes-not-reflected-in-api","title":"Status Changes Not Reflected in API","text":"<p>Problem: Database shows 'suspended' but API still allows requests.</p> <p>Cause: Authentication middleware not checking organization status.</p> <p>Solution: Implement status check in authentication:</p> <pre><code># auth/dependencies.py\nasync def verify_virtual_key(\n    authorization: str = Header(None),\n    db: Session = Depends(get_db)\n):\n    # ... existing key verification ...\n\n    # Get team's organization\n    team_credits = db.query(TeamCredits).filter(\n        TeamCredits.virtual_key == virtual_key\n    ).first()\n\n    if team_credits:\n        org = db.query(Organization).filter(\n            Organization.organization_id == team_credits.organization_id\n        ).first()\n\n        if org.status == 'suspended':\n            raise HTTPException(\n                status_code=403,\n                detail=\"Team access suspended - contact support\"\n            )\n        elif org.status == 'paused':\n            raise HTTPException(\n                status_code=429,\n                detail=\"Team access temporarily paused\"\n            )\n\n    return team_credits.team_id\n</code></pre>"},{"location":"admin-dashboard/suspend-pause/#next-steps","title":"Next Steps","text":"<p>Now that you understand team status management:</p> <ol> <li>Monitor Teams - Track team usage and health</li> <li>Manage Credits - Understand credit allocation</li> <li>View Usage Analytics - Analyze consumption patterns</li> </ol>"},{"location":"admin-dashboard/suspend-pause/#additional-resources","title":"Additional Resources","text":"<ul> <li>Team Management Guide - Full team administration</li> <li>Organization Management - Organization hierarchy</li> <li>API Reference - Team API endpoints</li> </ul>"},{"location":"admin-dashboard/teams/","title":"Teams","text":"<p>Learn how to create and manage teams - the primary way your clients access the SaaS LiteLLM API.</p>"},{"location":"admin-dashboard/teams/#what-are-teams","title":"What are Teams?","text":"<p>Teams are the core access unit in SaaS LiteLLM. Each team gets:</p> <ul> <li>\u2705 Unique virtual API key for authentication</li> <li>\u2705 Credit allocation for usage</li> <li>\u2705 Model access permissions via access groups</li> <li>\u2705 Independent rate limits (TPM/RPM)</li> <li>\u2705 Usage tracking and cost monitoring</li> </ul> <p>Key Point: Your clients use teams to make API calls. One organization can have multiple teams (e.g., dev, staging, production).</p>"},{"location":"admin-dashboard/teams/#creating-a-team-for-a-client","title":"Creating a Team for a Client","text":"<p> Teams management interface - create teams, manage credits, and virtual keys</p>"},{"location":"admin-dashboard/teams/#quick-start","title":"Quick Start","text":"<ol> <li>Navigate to Teams \u2192 Click \"Create Team\"</li> <li>Fill in details:</li> <li>Team ID: <code>client-prod</code></li> <li>Organization: Select client's organization</li> <li>Access Groups: <code>[\"gpt-models\"]</code></li> <li>Credits: <code>1000</code></li> <li>Click \"Create\"</li> <li>Copy the virtual key and share with client securely</li> </ol> <p>That's it! Your client can now make API calls.</p>"},{"location":"admin-dashboard/teams/#via-api","title":"Via API","text":"<pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-prod\",\n    \"team_alias\": \"ACME Production\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000,\n    \"budget_mode\": \"job_based\",\n    \"credits_per_dollar\": 10.0,\n    \"tokens_per_credit\": 10000\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"organization_id\": \"org_acme\",\n  \"team_alias\": \"ACME Production\",\n  \"virtual_key\": \"sk-1234567890abcdef1234567890abcdef\",\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 1000,\n  \"status\": \"active\",\n  \"access_groups\": [\"gpt-models\"],\n  \"budget_mode\": \"job_based\",\n  \"credits_per_dollar\": 10.0,\n  \"tokens_per_credit\": 10000\n}\n</code></pre></p> <p>Save the Virtual Key</p> <p>The virtual key is only shown once during team creation. Make sure to copy it and share it securely with your client!</p>"},{"location":"admin-dashboard/teams/#budget-modes","title":"Budget Modes","text":"<p>Teams support three flexible billing modes to match different use cases:</p>"},{"location":"admin-dashboard/teams/#job-based-billing-default","title":"Job-Based Billing (Default)","text":"<p>Best for: Predictable flat-rate billing</p> <pre><code>{\n  \"budget_mode\": \"job_based\"\n}\n</code></pre> <ul> <li>1 credit = 1 completed job (regardless of token usage)</li> <li>Simple and predictable for clients</li> <li>Perfect for fixed-price operations</li> </ul> <p>Example: Document analysis service where each document costs 1 credit</p>"},{"location":"admin-dashboard/teams/#usd-based-billing","title":"USD-Based Billing","text":"<p>Best for: Cost-plus pricing tied to actual LLM costs</p> <pre><code>{\n  \"budget_mode\": \"consumption_usd\",\n  \"credits_per_dollar\": 10.0\n}\n</code></pre> <ul> <li>Credits deducted based on actual USD cost of LLM calls</li> <li>Useful for chat applications with variable costs</li> <li>Set your margin with <code>credits_per_dollar</code> (e.g., 10 = $0.10 per credit)</li> </ul> <p>Example: Chat app where a conversation costing $0.05 deducts 0.5 credits (at 10 credits per dollar)</p> <p>Calculation: <pre><code>Credits deducted = actual_cost_usd \u00d7 credits_per_dollar\n</code></pre></p>"},{"location":"admin-dashboard/teams/#token-based-billing","title":"Token-Based Billing","text":"<p>Best for: Usage-based pricing by token consumption</p> <pre><code>{\n  \"budget_mode\": \"consumption_tokens\",\n  \"tokens_per_credit\": 10000\n}\n</code></pre> <ul> <li>Credits deducted based on tokens consumed</li> <li>Most granular billing option</li> <li>Perfect for API resellers tracking token usage</li> </ul> <p>Example: API service where 10,000 tokens = 1 credit</p> <p>Calculation: <pre><code>Credits deducted = total_tokens \u00f7 tokens_per_credit\n</code></pre></p>"},{"location":"admin-dashboard/teams/#choosing-a-budget-mode","title":"Choosing a Budget Mode","text":"Mode Use Case Billing Client Predictability job_based Fixed operations 1 credit per job High consumption_usd Variable LLM costs Based on $ cost Medium consumption_tokens Token tracking Based on tokens Low (variable input/output)"},{"location":"admin-dashboard/teams/#setting-budget-mode","title":"Setting Budget Mode","text":"<p>Budget mode is configured during team creation and defines how credits are calculated for that team.</p> <p>Example: Chat Application (USD-based) <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -d '{\n    \"team_id\": \"chat-app-prod\",\n    \"budget_mode\": \"consumption_usd\",\n    \"credits_per_dollar\": 20.0,\n    \"credits_allocated\": 1000\n  }'\n</code></pre></p> <p>With this configuration: - 1000 credits = $50 worth of LLM usage - Each $0.10 LLM call deducts 2 credits - Client can estimate costs based on conversation volume</p> <p>Example: Token Service (Token-based) <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -d '{\n    \"team_id\": \"token-service\",\n    \"budget_mode\": \"consumption_tokens\",\n    \"tokens_per_credit\": 5000,\n    \"credits_allocated\": 2000\n  }'\n</code></pre></p> <p>With this configuration: - 2000 credits = 10M tokens available - Job using 15,000 tokens deducts 3 credits - Perfect for API resellers tracking exact token usage</p>"},{"location":"admin-dashboard/teams/#team-properties","title":"Team Properties","text":"Property Type Description <code>team_id</code> string Unique identifier (e.g., \"acme-prod\") <code>organization_id</code> string Parent organization <code>team_alias</code> string Display name (e.g., \"ACME Production\") <code>virtual_key</code> string API key for authentication (starts with \"sk-\") <code>credits_allocated</code> integer Total credits allocated <code>credits_remaining</code> integer Credits still available <code>access_groups</code> array Model access groups (e.g., [\"gpt-models\"]) <code>budget_mode</code> string Billing mode: job_based, consumption_usd, consumption_tokens <code>credits_per_dollar</code> float Conversion rate for USD-based billing (default: 10.0) <code>tokens_per_credit</code> integer Tokens per credit for token-based billing (default: 10000) <code>status</code> string active, suspended, paused <code>created_at</code> timestamp When team was created"},{"location":"admin-dashboard/teams/#viewing-teams","title":"Viewing Teams","text":""},{"location":"admin-dashboard/teams/#list-all-teams","title":"List All Teams","text":"<p>Via Dashboard: - Navigate to Teams - See all teams with status, credits, organization</p> <p>Via API: <pre><code>curl http://localhost:8003/api/teams\n</code></pre></p>"},{"location":"admin-dashboard/teams/#view-team-details","title":"View Team Details","text":"<p>Via Dashboard: - Click on team name - See full details, virtual key, usage stats</p> <p>Via API: <pre><code>curl http://localhost:8003/api/teams/acme-prod\n</code></pre></p> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"organization_id\": \"org_acme\",\n  \"team_alias\": \"ACME Production\",\n  \"virtual_key\": \"sk-1234567890abcdef1234567890abcdef\",\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 750,\n  \"access_groups\": [\"gpt-models\"],\n  \"status\": \"active\",\n  \"usage_summary\": {\n    \"total_jobs\": 250,\n    \"total_cost_usd\": 45.67\n  }\n}\n</code></pre></p>"},{"location":"admin-dashboard/teams/#managing-credits","title":"Managing Credits","text":""},{"location":"admin-dashboard/teams/#add-credits","title":"Add Credits","text":"<p>When a client needs more credits:</p> <p>Via Dashboard: 1. Navigate to team 2. Click \"Add Credits\" 3. Enter amount (e.g., 500) 4. Click \"Add\"</p> <p>Via API: <pre><code>curl -X POST http://localhost:8003/api/credits/add \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-prod\",\n    \"amount\": 500,\n    \"description\": \"Monthly credit top-up\"\n  }'\n</code></pre></p>"},{"location":"admin-dashboard/teams/#check-credit-balance","title":"Check Credit Balance","text":"<p>Via API: <pre><code>curl \"http://localhost:8003/api/credits/balance?team_id=acme-prod\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"credits_remaining\": 750,\n  \"credits_allocated\": 1000,\n  \"credits_used\": 250\n}\n</code></pre></p> <p> Learn more about credits</p>"},{"location":"admin-dashboard/teams/#model-access-groups","title":"Model Access Groups","text":"<p>Control which models a team can access:</p>"},{"location":"admin-dashboard/teams/#assign-access-groups","title":"Assign Access Groups","text":"<p>During Creation: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"access_groups\": [\"gpt-models\", \"claude-models\"]\n}\n</code></pre></p> <p>After Creation (Update): <pre><code>curl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"access_groups\": [\"gpt-models\", \"claude-models\", \"gemini-models\"]\n  }'\n</code></pre></p>"},{"location":"admin-dashboard/teams/#common-access-group-setups","title":"Common Access Group Setups","text":"<p>Basic (GPT Only): <pre><code>{\n  \"access_groups\": [\"gpt-models\"]\n}\n</code></pre></p> <p>Premium (Multiple Providers): <pre><code>{\n  \"access_groups\": [\"gpt-models\", \"claude-models\", \"gemini-models\"]\n}\n</code></pre></p> <p>Custom (Specific Models): <pre><code>{\n  \"access_groups\": [\"fast-models\", \"smart-models\"]\n}\n</code></pre></p> <p> Learn more about model access groups</p>"},{"location":"admin-dashboard/teams/#team-status","title":"Team Status","text":""},{"location":"admin-dashboard/teams/#active","title":"Active","text":"<ul> <li>\u2705 Can make API calls</li> <li>\u2705 Credits are deducted</li> <li>\u2705 Normal operation</li> </ul>"},{"location":"admin-dashboard/teams/#suspended","title":"Suspended","text":"<ul> <li>\u274c Cannot make API calls</li> <li>\u274c All requests return 403 error</li> <li>\u23f8\ufe0f Billing stopped</li> </ul> <p>Use when: Client hasn't paid, exceeded limits, or temporary account freeze</p>"},{"location":"admin-dashboard/teams/#paused","title":"Paused","text":"<ul> <li>\u274c Cannot make API calls</li> <li>\u23f8\ufe0f Temporary pause (different from suspend)</li> <li>\ud83d\udd04 Can be quickly resumed</li> </ul> <p>Use when: Client requested temporary pause, maintenance, etc.</p> <p> Learn more about suspend/pause</p>"},{"location":"admin-dashboard/teams/#suspendingresuming-teams","title":"Suspending/Resuming Teams","text":""},{"location":"admin-dashboard/teams/#suspend-a-team","title":"Suspend a Team","text":"<p>Via Dashboard: 1. Navigate to team 2. Click \"Suspend\" 3. Confirm</p> <p>Via API: <pre><code>curl -X POST http://localhost:8003/api/teams/acme-prod/suspend \\\n  -H \"Content-Type: application/json\"\n</code></pre></p>"},{"location":"admin-dashboard/teams/#resume-a-team","title":"Resume a Team","text":"<p>Via Dashboard: 1. Navigate to suspended team 2. Click \"Resume\" 3. Team is immediately active</p> <p>Via API: <pre><code>curl -X POST http://localhost:8003/api/teams/acme-prod/resume \\\n  -H \"Content-Type: application/json\"\n</code></pre></p>"},{"location":"admin-dashboard/teams/#updating-teams","title":"Updating Teams","text":""},{"location":"admin-dashboard/teams/#update-team-details","title":"Update Team Details","text":"<pre><code>curl -X PUT http://localhost:8003/api/teams/acme-prod \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_alias\": \"ACME Production (Updated)\",\n    \"access_groups\": [\"gpt-models\", \"claude-models\"],\n    \"metadata\": {\n      \"environment\": \"production\",\n      \"contact\": \"tech@acme.com\"\n    }\n  }'\n</code></pre>"},{"location":"admin-dashboard/teams/#team-usage-statistics","title":"Team Usage Statistics","text":""},{"location":"admin-dashboard/teams/#view-team-usage","title":"View Team Usage","text":"<p>Via API: <pre><code>curl \"http://localhost:8003/api/teams/acme-prod/usage?period=2024-10\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"team_id\": \"acme-prod\",\n  \"period\": \"2024-10\",\n  \"summary\": {\n    \"total_jobs\": 250,\n    \"successful_jobs\": 245,\n    \"failed_jobs\": 5,\n    \"total_cost_usd\": 45.67,\n    \"credits_used\": 250,\n    \"avg_cost_per_job\": 0.18\n  },\n  \"by_job_type\": {\n    \"document_analysis\": 120,\n    \"chat_session\": 130\n  }\n}\n</code></pre></p>"},{"location":"admin-dashboard/teams/#sharing-virtual-keys-with-clients","title":"Sharing Virtual Keys with Clients","text":""},{"location":"admin-dashboard/teams/#best-practices","title":"Best Practices","text":"<ol> <li>Secure Transmission</li> <li>Use encrypted channels (password-protected email, secure portal)</li> <li>Never send via plain text email or chat</li> <li> <p>Consider one-time secret links (e.g., onetimesecret.com)</p> </li> <li> <p>Documentation</p> </li> <li>Share integration docs with the key</li> <li>Provide example code</li> <li> <p>Link to your API documentation</p> </li> <li> <p>Support</p> </li> <li>Provide contact for technical support</li> <li>Set up monitoring for new teams</li> <li>Check in after first successful API call</li> </ol>"},{"location":"admin-dashboard/teams/#example-email-template","title":"Example Email Template","text":"<pre><code>Subject: Your SaaS LiteLLM API Access\n\nHi [Client Name],\n\nYour API access has been set up! Here are your credentials:\n\nVirtual Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTeam ID: acme-prod\nCredits Allocated: 1,000\n\nGETTING STARTED:\n1. Review our integration guide: https://docs.yourcompany.com/integration/overview\n2. Try our quickstart: https://docs.yourcompany.com/getting-started/quickstart\n3. See examples: https://docs.yourcompany.com/examples/basic-usage\n\nAPI ENDPOINTS:\n- Production API: https://api.yourcompany.com/api\n- API Documentation: https://api.yourcompany.com/redoc\n\nSUPPORT:\n- Technical Support: support@yourcompany.com\n- Your Account Manager: manager@yourcompany.com\n\nIMPORTANT: Keep your virtual key secure. Don't share it or commit it to version control.\n\nQuestions? Reply to this email or contact support@yourcompany.com\n\nBest regards,\nYour Company Team\n</code></pre>"},{"location":"admin-dashboard/teams/#common-client-onboarding-workflow","title":"Common Client Onboarding Workflow","text":"<pre><code># 1. Create organization for client\ncurl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"name\": \"New Client Inc\"\n  }'\n\n# 2. Create production team\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"team_id\": \"newclient-prod\",\n    \"team_alias\": \"Production\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n# Save the virtual_key from response!\n\n# 3. (Optional) Create dev/staging team\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_newclient\",\n    \"team_id\": \"newclient-dev\",\n    \"team_alias\": \"Development\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 500\n  }'\n\n# 4. Share virtual keys with client securely\n# 5. Monitor first API calls\n# 6. Check in with client after 24 hours\n</code></pre>"},{"location":"admin-dashboard/teams/#team-best-practices","title":"Team Best Practices","text":""},{"location":"admin-dashboard/teams/#naming-conventions","title":"Naming Conventions","text":"<p>Team IDs: - Use format: <code>{org}-{environment}</code> (e.g., \"acme-prod\", \"acme-dev\") - Keep lowercase with hyphens - Make it descriptive</p> <p>Team Aliases: - Use readable names: \"ACME Production\", \"ACME Development\" - Include environment if multiple teams</p>"},{"location":"admin-dashboard/teams/#security","title":"Security","text":"<ol> <li>Virtual Keys:</li> <li>Treat like passwords</li> <li>Rotate periodically (create new team)</li> <li> <p>Monitor for unusual usage</p> </li> <li> <p>Access Control:</p> </li> <li>Use least privilege (only needed models)</li> <li>Separate dev and prod teams</li> <li> <p>Different keys per environment</p> </li> <li> <p>Monitoring:</p> </li> <li>Set up alerts for high usage</li> <li>Monitor failed requests</li> <li>Track credit depletion rate</li> </ol>"},{"location":"admin-dashboard/teams/#credit-management","title":"Credit Management","text":"<ol> <li>Initial Allocation:</li> <li>Start conservative (1000 credits)</li> <li>Monitor usage first week</li> <li> <p>Adjust based on actual usage</p> </li> <li> <p>Top-ups:</p> </li> <li>Set up low-credit alerts (20% remaining)</li> <li>Automate top-ups for good customers</li> <li> <p>Prepaid vs. postpaid options</p> </li> <li> <p>Overage Protection:</p> </li> <li>Hard limits (team suspended at 0 credits)</li> <li>Soft limits (alert but don't suspend)</li> <li>Grace period for good customers</li> </ol>"},{"location":"admin-dashboard/teams/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-dashboard/teams/#virtual-key-not-working","title":"Virtual Key Not Working","text":"<p>Problem: Client reports 401 errors</p> <p>Solutions: 1. Verify key was copied correctly (no extra spaces) 2. Check team status is \"active\" (not suspended) 3. Verify <code>Authorization: Bearer sk-...</code> format 4. Check team exists in database</p>"},{"location":"admin-dashboard/teams/#out-of-credits","title":"Out of Credits","text":"<p>Problem: Client getting 403 \"Insufficient credits\"</p> <p>Solutions: 1. Check credit balance 2. Add more credits 3. Review usage patterns 4. Consider upgrade to higher tier</p>"},{"location":"admin-dashboard/teams/#cant-access-model","title":"Can't Access Model","text":"<p>Problem: Client gets \"Model access denied\"</p> <p>Solutions: 1. Check team's access groups 2. Verify model alias exists 3. Add required access group to team 4. Check model is active</p>"},{"location":"admin-dashboard/teams/#next-steps","title":"Next Steps","text":"<p>Now that you understand teams:</p> <ol> <li>Allocate Credits - Give teams credits to use</li> <li>Configure Model Access - Control which models teams can access</li> <li>Share Integration Docs - Help clients integrate</li> <li>Monitor Usage - Track team activity</li> </ol>"},{"location":"admin-dashboard/teams/#quick-reference","title":"Quick Reference","text":""},{"location":"admin-dashboard/teams/#create-team","title":"Create Team","text":"<pre><code>POST /api/teams/create\n{\n  \"organization_id\": \"org_client\",\n  \"team_id\": \"client-prod\",\n  \"team_alias\": \"Production\",\n  \"access_groups\": [\"gpt-models\"],\n  \"credits_allocated\": 1000\n}\n</code></pre>"},{"location":"admin-dashboard/teams/#add-credits_1","title":"Add Credits","text":"<pre><code>POST /api/credits/add\n{\n  \"team_id\": \"client-prod\",\n  \"amount\": 500\n}\n</code></pre>"},{"location":"admin-dashboard/teams/#suspend-team","title":"Suspend Team","text":"<pre><code>POST /api/teams/client-prod/suspend\n</code></pre>"},{"location":"admin-dashboard/teams/#resume-team","title":"Resume Team","text":"<pre><code>POST /api/teams/client-prod/resume\n</code></pre>"},{"location":"admin-dashboard/teams/#check-usage","title":"Check Usage","text":"<pre><code>GET /api/teams/client-prod/usage?period=2024-10\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/","title":"Admin Dashboard User Management - Implementation Plan","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#overview","title":"Overview","text":"<p>Replace MASTER_KEY-only authentication with proper user management, role-based access control (RBAC), and secure password authentication.</p>"},{"location":"admin-dashboard/user-management-implementation-plan/#current-issues","title":"Current Issues","text":"<ol> <li>\u274c Single MASTER_KEY shared by everyone</li> <li>\u274c No user accounts or role-based permissions</li> <li>\u274c No audit trail of who performed actions</li> <li>\u274c Can't revoke access without changing MASTER_KEY</li> <li>\u274c Not suitable for team collaboration</li> <li>\u274c No proper onboarding flow</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#proposed-solution","title":"Proposed Solution","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"Role Permissions Owner - Full access to everything- Manage admins (create, delete, change roles)- Manage users- Manage organizations, teams, models, credits- View audit logs Admin - Manage users (create, delete regular users)- Manage organizations, teams, models, credits- Cannot change admin roles- View audit logs User - View organizations, teams, models- View credits (read-only)- Cannot create/edit/delete- View own audit trail"},{"location":"admin-dashboard/user-management-implementation-plan/#user-model","title":"User Model","text":"<pre><code>interface AdminUser {\n  user_id: string;           // UUID\n  email: string;             // Unique, used as username\n  display_name: string;      // Full name for display\n  password_hash: string;     // bcrypt hashed password\n  role: 'owner' | 'admin' | 'user';\n  is_active: boolean;        // Can be suspended\n  created_at: timestamp;\n  created_by: string;        // user_id of creator\n  last_login: timestamp;\n  metadata: {\n    avatar_url?: string;\n    phone?: string;\n    department?: string;\n  };\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#database-schema","title":"Database Schema","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#table-admin_users","title":"Table: <code>admin_users</code>","text":"<pre><code>CREATE TABLE admin_users (\n    user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    display_name VARCHAR(255) NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    role VARCHAR(20) NOT NULL CHECK (role IN ('owner', 'admin', 'user')),\n    is_active BOOLEAN DEFAULT true,\n    created_at TIMESTAMP DEFAULT NOW(),\n    created_by UUID REFERENCES admin_users(user_id),\n    last_login TIMESTAMP,\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE INDEX idx_admin_users_email ON admin_users(email);\nCREATE INDEX idx_admin_users_role ON admin_users(role);\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#table-admin_sessions","title":"Table: <code>admin_sessions</code>","text":"<pre><code>CREATE TABLE admin_sessions (\n    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id UUID REFERENCES admin_users(user_id) ON DELETE CASCADE,\n    token_hash VARCHAR(255) NOT NULL,\n    expires_at TIMESTAMP NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW(),\n    ip_address VARCHAR(45),\n    user_agent TEXT,\n    is_revoked BOOLEAN DEFAULT false\n);\n\nCREATE INDEX idx_admin_sessions_user_id ON admin_sessions(user_id);\nCREATE INDEX idx_admin_sessions_token_hash ON admin_sessions(token_hash);\nCREATE INDEX idx_admin_sessions_expires_at ON admin_sessions(expires_at);\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#table-admin_audit_log","title":"Table: <code>admin_audit_log</code>","text":"<pre><code>CREATE TABLE admin_audit_log (\n    audit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id UUID REFERENCES admin_users(user_id),\n    action VARCHAR(100) NOT NULL,\n    resource_type VARCHAR(50),  -- 'organization', 'team', 'user', etc.\n    resource_id VARCHAR(255),\n    details JSONB,\n    ip_address VARCHAR(45),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_admin_audit_log_user_id ON admin_audit_log(user_id);\nCREATE INDEX idx_admin_audit_log_created_at ON admin_audit_log(created_at DESC);\nCREATE INDEX idx_admin_audit_log_resource ON admin_audit_log(resource_type, resource_id);\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#api-endpoints","title":"API Endpoints","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#authentication-endpoints","title":"Authentication Endpoints","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#1-check-setup-status","title":"1. Check Setup Status","text":"<pre><code>GET /api/admin/setup/status\n\nResponse:\n{\n  \"setup_complete\": boolean,\n  \"owner_exists\": boolean\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#2-initial-setup-first-time-only","title":"2. Initial Setup (First-Time Only)","text":"<pre><code>POST /api/admin/setup\n\nHeaders:\n  X-Admin-Key: &lt;MASTER_KEY&gt;\n\nRequest:\n{\n  \"email\": \"owner@company.com\",\n  \"display_name\": \"John Doe\",\n  \"password\": \"SecurePassword123!\"\n}\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"email\": \"owner@company.com\",\n  \"display_name\": \"John Doe\",\n  \"role\": \"owner\",\n  \"token\": \"eyJ...\",  // JWT token\n  \"message\": \"Owner account created successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#3-login","title":"3. Login","text":"<pre><code>POST /api/admin/login\n\nRequest:\n{\n  \"email\": \"user@company.com\",\n  \"password\": \"password\"\n}\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"email\": \"user@company.com\",\n  \"display_name\": \"John Doe\",\n  \"role\": \"admin\",\n  \"token\": \"eyJ...\",  // JWT token\n  \"expires_at\": \"2024-01-15T12:00:00Z\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#4-logout","title":"4. Logout","text":"<pre><code>POST /api/admin/logout\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nResponse:\n{\n  \"message\": \"Logged out successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#5-refresh-token","title":"5. Refresh Token","text":"<pre><code>POST /api/admin/refresh\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nResponse:\n{\n  \"token\": \"eyJ...\",  // New JWT\n  \"expires_at\": \"2024-01-15T12:00:00Z\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#user-management-endpoints","title":"User Management Endpoints","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#6-list-users","title":"6. List Users","text":"<pre><code>GET /api/admin/users\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: admin, owner\n\nResponse:\n{\n  \"users\": [\n    {\n      \"user_id\": \"uuid\",\n      \"email\": \"user@company.com\",\n      \"display_name\": \"John Doe\",\n      \"role\": \"admin\",\n      \"is_active\": true,\n      \"created_at\": \"2024-01-01T10:00:00Z\",\n      \"last_login\": \"2024-01-15T08:30:00Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#7-create-user","title":"7. Create User","text":"<pre><code>POST /api/admin/users\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: admin (can create 'user'), owner (can create 'admin' or 'user')\n\nRequest:\n{\n  \"email\": \"newuser@company.com\",\n  \"display_name\": \"Jane Smith\",\n  \"password\": \"TempPassword123!\",\n  \"role\": \"user\",\n  \"send_email\": true  // Send welcome email with password reset link\n}\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"email\": \"newuser@company.com\",\n  \"display_name\": \"Jane Smith\",\n  \"role\": \"user\",\n  \"temp_password\": \"TempPassword123!\",  // Only if send_email=false\n  \"message\": \"User created successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#8-update-user","title":"8. Update User","text":"<pre><code>PUT /api/admin/users/{user_id}\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: admin (other users), owner (any user), self (own profile)\n\nRequest:\n{\n  \"display_name\": \"Jane Smith-Johnson\",\n  \"is_active\": true,\n  \"metadata\": {\n    \"department\": \"Engineering\"\n  }\n}\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"message\": \"User updated successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#9-change-user-role","title":"9. Change User Role","text":"<pre><code>PUT /api/admin/users/{user_id}/role\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: owner only\n\nRequest:\n{\n  \"role\": \"admin\"\n}\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"role\": \"admin\",\n  \"message\": \"User role updated successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#10-delete-user","title":"10. Delete User","text":"<pre><code>DELETE /api/admin/users/{user_id}\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: admin (can delete 'user'), owner (can delete 'admin' or 'user')\n\nResponse:\n{\n  \"message\": \"User deleted successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#11-change-password","title":"11. Change Password","text":"<pre><code>POST /api/admin/users/{user_id}/password\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: self (own password), owner (any user)\n\nRequest:\n{\n  \"current_password\": \"OldPassword123!\",  // Required for self\n  \"new_password\": \"NewPassword456!\"\n}\n\nResponse:\n{\n  \"message\": \"Password changed successfully\"\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#12-get-current-user-profile","title":"12. Get Current User Profile","text":"<pre><code>GET /api/admin/profile\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nResponse:\n{\n  \"user_id\": \"uuid\",\n  \"email\": \"user@company.com\",\n  \"display_name\": \"John Doe\",\n  \"role\": \"admin\",\n  \"is_active\": true,\n  \"created_at\": \"2024-01-01T10:00:00Z\",\n  \"last_login\": \"2024-01-15T08:30:00Z\",\n  \"metadata\": {}\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#audit-log-endpoints","title":"Audit Log Endpoints","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#13-get-audit-logs","title":"13. Get Audit Logs","text":"<pre><code>GET /api/admin/audit-logs?user_id={id}&amp;resource_type={type}&amp;limit=100\n\nHeaders:\n  Authorization: Bearer &lt;JWT&gt;\n\nPermissions: admin, owner (all logs), user (own logs only)\n\nResponse:\n{\n  \"logs\": [\n    {\n      \"audit_id\": \"uuid\",\n      \"user_email\": \"admin@company.com\",\n      \"user_display_name\": \"Admin User\",\n      \"action\": \"created_team\",\n      \"resource_type\": \"team\",\n      \"resource_id\": \"acme-corp\",\n      \"details\": {\n        \"team_name\": \"ACME Corp\",\n        \"credits_allocated\": 1000\n      },\n      \"ip_address\": \"192.168.1.100\",\n      \"created_at\": \"2024-01-15T10:30:00Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#authentication-flow","title":"Authentication Flow","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#first-time-setup","title":"First-Time Setup","text":"<pre><code>User visits admin dashboard\n  \u2193\nCheck /api/admin/setup/status\n  \u2193\nIf setup_complete = false:\n  \u2193\nShow \"Initial Setup\" page\n  \u2193\nUser enters:\n  - MASTER_KEY (to prove they have access)\n  - Email\n  - Display name\n  - Password\n  \u2193\nPOST /api/admin/setup with MASTER_KEY header\n  \u2193\nBackend:\n  - Validates MASTER_KEY\n  - Checks no owner exists\n  - Creates owner account with hashed password\n  - Generates JWT token\n  - Returns token\n  \u2193\nFrontend stores JWT in localStorage\n  \u2193\nRedirect to dashboard\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#normal-login","title":"Normal Login","text":"<pre><code>User visits admin dashboard\n  \u2193\nCheck /api/admin/setup/status\n  \u2193\nIf setup_complete = true:\n  \u2193\nShow login page\n  \u2193\nUser enters email + password\n  \u2193\nPOST /api/admin/login\n  \u2193\nBackend:\n  - Validates email/password\n  - Generates JWT token\n  - Updates last_login\n  - Creates audit log entry\n  - Returns token\n  \u2193\nFrontend stores JWT in localStorage\n  \u2193\nRedirect to dashboard\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#api-request-with-jwt","title":"API Request with JWT","text":"<pre><code>Frontend makes API request\n  \u2193\nAdd header: Authorization: Bearer &lt;JWT&gt;\n  \u2193\nBackend middleware:\n  - Validates JWT signature\n  - Checks expiration\n  - Extracts user_id and role\n  - Checks user is_active\n  - Checks permissions for endpoint\n  \u2193\nIf valid: Process request\nIf invalid: Return 401 Unauthorized\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#frontend-changes","title":"Frontend Changes","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#1-new-pages","title":"1. New Pages","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#login-login-page","title":"<code>/login</code> - Login Page","text":"<ul> <li>Check setup status</li> <li>If not setup: Show initial setup form</li> <li>If setup: Show login form</li> <li>Handle JWT storage</li> </ul>"},{"location":"admin-dashboard/user-management-implementation-plan/#setup-initial-setup-page","title":"<code>/setup</code> - Initial Setup Page","text":"<ul> <li>Only accessible if no owner exists</li> <li>Requires MASTER_KEY + owner credentials</li> <li>Creates first owner account</li> </ul>"},{"location":"admin-dashboard/user-management-implementation-plan/#users-user-management-page","title":"<code>/users</code> - User Management Page","text":"<ul> <li>List all users (admin+)</li> <li>Create new user (admin+)</li> <li>Edit user (admin+ or self)</li> <li>Delete user (owner for admins, admin for users)</li> <li>Change role (owner only)</li> </ul>"},{"location":"admin-dashboard/user-management-implementation-plan/#profile-user-profile-page","title":"<code>/profile</code> - User Profile Page","text":"<ul> <li>View own profile</li> <li>Edit display name</li> <li>Change password</li> <li>View own audit log</li> </ul>"},{"location":"admin-dashboard/user-management-implementation-plan/#audit-logs-audit-logs-page-admin","title":"<code>/audit-logs</code> - Audit Logs Page (Admin+)","text":"<ul> <li>View all admin actions</li> <li>Filter by user, action, resource</li> <li>Export logs</li> </ul>"},{"location":"admin-dashboard/user-management-implementation-plan/#2-update-existing-pages","title":"2. Update Existing Pages","text":"<p>All existing pages (Organizations, Teams, Model Groups, etc.) need: - Update API client to use JWT instead of MASTER_KEY - Add role-based UI elements (hide/show based on permissions) - Add audit logging to all create/update/delete operations</p>"},{"location":"admin-dashboard/user-management-implementation-plan/#3-components","title":"3. Components","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#protectedroutetsx","title":"<code>ProtectedRoute.tsx</code>","text":"<pre><code>// Update to check JWT and role\nfunction ProtectedRoute({\n  children,\n  requiredRole\n}: {\n  children: React.ReactNode,\n  requiredRole?: 'owner' | 'admin' | 'user'\n}) {\n  const user = getCurrentUser();\n\n  if (!user) {\n    redirect('/login');\n  }\n\n  if (requiredRole &amp;&amp; !hasRole(user.role, requiredRole)) {\n    return &lt;div&gt;Access Denied&lt;/div&gt;;\n  }\n\n  return children;\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#roleguardtsx","title":"<code>RoleGuard.tsx</code>","text":"<pre><code>// Conditional rendering based on role\nfunction RoleGuard({\n  children,\n  requiredRole\n}: {\n  children: React.ReactNode,\n  requiredRole: 'owner' | 'admin' | 'user'\n}) {\n  const user = getCurrentUser();\n\n  if (!hasRole(user.role, requiredRole)) {\n    return null;\n  }\n\n  return children;\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#4-update-libapi-clientts","title":"4. Update <code>lib/api-client.ts</code>","text":"<pre><code>// Get JWT from localStorage\nfunction getAuthToken(): string | null {\n  if (typeof window === 'undefined') return null;\n  return localStorage.getItem('authToken');\n}\n\n// Updated request function\nasync function request(endpoint: string, options: RequestInit = {}) {\n  const token = getAuthToken();\n\n  const response = await fetch(`${API_URL}${endpoint}`, {\n    ...options,\n    headers: {\n      'Content-Type': 'application/json',\n      ...(token ? { 'Authorization': `Bearer ${token}` } : {}),\n      ...options.headers,\n    },\n  });\n\n  if (!response.ok) {\n    if (response.status === 401) {\n      // Token expired or invalid\n      localStorage.removeItem('authToken');\n      localStorage.removeItem('user');\n      window.location.href = '/login';\n    }\n    // ... rest of error handling\n  }\n\n  return response.json();\n}\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#backend-implementation","title":"Backend Implementation","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#1-dependencies","title":"1. Dependencies","text":"<pre><code># requirements.txt additions\npasslib[bcrypt]&gt;=1.7.4\npython-jose[cryptography]&gt;=3.3.0\npython-multipart&gt;=0.0.6\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#2-password-hashing","title":"2. Password Hashing","text":"<pre><code># src/auth/password.py\nfrom passlib.context import CryptContext\n\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\ndef hash_password(password: str) -&gt; str:\n    \"\"\"Hash a password using bcrypt\"\"\"\n    return pwd_context.hash(password)\n\ndef verify_password(plain_password: str, hashed_password: str) -&gt; bool:\n    \"\"\"Verify a password against a hash\"\"\"\n    return pwd_context.verify(plain_password, hashed_password)\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#3-jwt-token-generation","title":"3. JWT Token Generation","text":"<pre><code># src/auth/jwt.py\nfrom jose import JWTError, jwt\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nSECRET_KEY = settings.jwt_secret_key  # New setting\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24  # 24 hours\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\ndef verify_token(token: str) -&gt; dict:\n    \"\"\"Verify JWT token and return payload\"\"\"\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        return payload\n    except JWTError:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#4-new-authentication-dependency","title":"4. New Authentication Dependency","text":"<pre><code># src/auth/dependencies.py - Add new function\nfrom fastapi import Depends, Header\nfrom jose import JWTError\nfrom .jwt import verify_token\n\nasync def get_current_user(\n    authorization: Optional[str] = Header(None),\n    db: Session = Depends(get_db)\n) -&gt; AdminUser:\n    \"\"\"\n    Get current authenticated admin user from JWT token.\n\n    Expects: Authorization: Bearer &lt;JWT&gt;\n    Returns: AdminUser object\n    Raises: 401 if invalid/expired token\n    \"\"\"\n    if not authorization:\n        raise HTTPException(status_code=401, detail=\"Not authenticated\")\n\n    try:\n        scheme, token = authorization.split()\n        if scheme.lower() != \"bearer\":\n            raise HTTPException(status_code=401, detail=\"Invalid authentication scheme\")\n    except ValueError:\n        raise HTTPException(status_code=401, detail=\"Invalid authorization header\")\n\n    try:\n        payload = verify_token(token)\n        user_id = payload.get(\"sub\")\n        if not user_id:\n            raise HTTPException(status_code=401, detail=\"Invalid token\")\n    except JWTError:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n\n    user = db.query(AdminUser).filter(AdminUser.user_id == user_id).first()\n    if not user:\n        raise HTTPException(status_code=401, detail=\"User not found\")\n\n    if not user.is_active:\n        raise HTTPException(status_code=401, detail=\"User account is inactive\")\n\n    return user\n\ndef require_role(required_role: str):\n    \"\"\"\n    Dependency factory for role-based access control.\n\n    Usage:\n        @router.post(\"/endpoint\", dependencies=[Depends(require_role(\"admin\"))])\n    \"\"\"\n    async def check_role(current_user: AdminUser = Depends(get_current_user)):\n        role_hierarchy = {\"user\": 1, \"admin\": 2, \"owner\": 3}\n\n        if role_hierarchy[current_user.role] &lt; role_hierarchy[required_role]:\n            raise HTTPException(\n                status_code=403,\n                detail=f\"Insufficient permissions. Required role: {required_role}\"\n            )\n\n        return current_user\n\n    return check_role\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#5-audit-logging-middleware","title":"5. Audit Logging Middleware","text":"<pre><code># src/middleware/audit.py\nfrom fastapi import Request\nfrom ..models.admin_users import AuditLog\n\nasync def log_admin_action(\n    db: Session,\n    user: AdminUser,\n    action: str,\n    resource_type: Optional[str] = None,\n    resource_id: Optional[str] = None,\n    details: Optional[dict] = None,\n    request: Optional[Request] = None\n):\n    \"\"\"Log an admin action to the audit log\"\"\"\n    audit_entry = AuditLog(\n        user_id=user.user_id,\n        action=action,\n        resource_type=resource_type,\n        resource_id=resource_id,\n        details=details or {},\n        ip_address=request.client.host if request else None\n    )\n    db.add(audit_entry)\n    db.commit()\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#migration-path","title":"Migration Path","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#phase-1-database-setup","title":"Phase 1: Database Setup","text":"<ol> <li>Create migration for <code>admin_users</code>, <code>admin_sessions</code>, <code>admin_audit_log</code> tables</li> <li>Run migration</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#phase-2-backend-implementation","title":"Phase 2: Backend Implementation","text":"<ol> <li>Implement password hashing</li> <li>Implement JWT token generation/validation</li> <li>Create admin user models</li> <li>Create setup endpoint</li> <li>Create login/logout endpoints</li> <li>Create user management endpoints</li> <li>Update existing endpoints to use JWT auth</li> <li>Add audit logging to all endpoints</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#phase-3-frontend-implementation","title":"Phase 3: Frontend Implementation","text":"<ol> <li>Update login page (setup vs. normal login)</li> <li>Create user management page</li> <li>Create profile page</li> <li>Create audit log page</li> <li>Update API client to use JWT</li> <li>Add role guards to existing pages</li> <li>Update all components to check permissions</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#phase-4-testing","title":"Phase 4: Testing","text":"<ol> <li>Test initial setup flow</li> <li>Test login/logout</li> <li>Test role-based permissions</li> <li>Test user management (CRUD)</li> <li>Test audit logging</li> <li>Test password changes</li> <li>Test token expiration/refresh</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#phase-5-deployment","title":"Phase 5: Deployment","text":"<ol> <li>Deploy backend changes</li> <li>Deploy frontend changes</li> <li>Run initial setup to create owner account</li> <li>Document for team</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#security-considerations","title":"Security Considerations","text":"<ol> <li>Password Requirements</li> <li>Minimum 12 characters</li> <li>At least one uppercase, lowercase, number, special character</li> <li> <p>Cannot be common passwords (check against list)</p> </li> <li> <p>Token Security</p> </li> <li>JWTs expire after 24 hours</li> <li>Refresh tokens for long sessions</li> <li>Revoke tokens on logout</li> <li> <p>Store tokens securely (httpOnly cookies preferred over localStorage)</p> </li> <li> <p>Rate Limiting</p> </li> <li>Limit login attempts (5 per 15 minutes per IP)</li> <li>Lock accounts after 10 failed attempts</li> <li> <p>Require email verification for password reset</p> </li> <li> <p>Audit Logging</p> </li> <li>Log all admin actions</li> <li>Include IP address and user agent</li> <li>Cannot be deleted by users</li> <li> <p>Owner can export logs</p> </li> <li> <p>RBAC Enforcement</p> </li> <li>Always check permissions on backend</li> <li>Never trust frontend role checks alone</li> <li>Use dependency injection for role verification</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#example-usage","title":"Example Usage","text":""},{"location":"admin-dashboard/user-management-implementation-plan/#creating-owner-first-time","title":"Creating Owner (First Time)","text":"<pre><code># Step 1: Check setup status\ncurl https://admin-panel.yourcompany.com/api/admin/setup/status\n\n# Response: {\"setup_complete\": false, \"owner_exists\": false}\n\n# Step 2: Create owner account\ncurl -X POST https://api.yourcompany.com/api/admin/setup \\\n  -H \"X-Admin-Key: sk-admin-MASTER-KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"owner@yourcompany.com\",\n    \"display_name\": \"John Doe\",\n    \"password\": \"SecurePassword123!\"\n  }'\n\n# Response: {\n#   \"user_id\": \"uuid\",\n#   \"email\": \"owner@yourcompany.com\",\n#   \"display_name\": \"John Doe\",\n#   \"role\": \"owner\",\n#   \"token\": \"eyJ...\"\n# }\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#normal-login_1","title":"Normal Login","text":"<pre><code>curl -X POST https://api.yourcompany.com/api/admin/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@yourcompany.com\",\n    \"password\": \"MyPassword123!\"\n  }'\n\n# Response: {\n#   \"user_id\": \"uuid\",\n#   \"token\": \"eyJ...\",\n#   \"role\": \"admin\"\n# }\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#creating-a-new-user-as-admin","title":"Creating a New User (as Admin)","text":"<pre><code>curl -X POST https://api.yourcompany.com/api/admin/users \\\n  -H \"Authorization: Bearer eyJ...\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"newuser@yourcompany.com\",\n    \"display_name\": \"Jane Smith\",\n    \"password\": \"TempPassword123!\",\n    \"role\": \"user\"\n  }'\n</code></pre>"},{"location":"admin-dashboard/user-management-implementation-plan/#backward-compatibility","title":"Backward Compatibility","text":"<p>During transition period, support both authentication methods:</p> <ol> <li>MASTER_KEY (deprecated) - Direct header authentication</li> <li>JWT (new) - Token-based authentication</li> </ol> <p>Endpoints check for either: - <code>X-Admin-Key</code> header (old way) - Maps to \"owner\" role - <code>Authorization: Bearer</code> header (new way) - Uses JWT with role</p> <p>After all users migrated, remove MASTER_KEY support.</p>"},{"location":"admin-dashboard/user-management-implementation-plan/#next-steps","title":"Next Steps","text":"<ol> <li>Review and approve this plan</li> <li>Estimate implementation time (likely 2-3 days for full implementation)</li> <li>Prioritize phases (can implement incrementally)</li> <li>Security review (password policies, token expiration, etc.)</li> <li>Begin implementation starting with database schema</li> </ol>"},{"location":"admin-dashboard/user-management-implementation-plan/#questions-to-resolve","title":"Questions to Resolve","text":"<ol> <li>Password reset flow - Email-based or admin-forced reset?</li> <li>Session timeout - 24 hours or shorter?</li> <li>Multi-factor authentication (MFA) - Required for owners/admins?</li> <li>API key management - Separate from user passwords for programmatic access?</li> <li>SSO/SAML integration - Future requirement?</li> </ol>"},{"location":"api-reference/admin-users/","title":"Admin Users API Reference","text":"<p>Complete API reference for admin user management, JWT authentication, and audit logging endpoints.</p>"},{"location":"api-reference/admin-users/#authentication","title":"Authentication","text":"<p>All admin user endpoints (except <code>/setup</code> and <code>/login</code>) require authentication via:</p> <ol> <li> <p>JWT Bearer Token (Preferred):    <pre><code>Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\n</code></pre></p> </li> <li> <p>Legacy X-Admin-Key (Management endpoints only):    <pre><code>X-Admin-Key: sk-admin-your-master-key\n</code></pre></p> </li> </ol> <p>JWT vs Legacy Auth</p> <ul> <li>User management endpoints (admin-users, audit logs) require JWT authentication</li> <li>Management endpoints (organizations, teams, credits) support both JWT and X-Admin-Key</li> </ul>"},{"location":"api-reference/admin-users/#setup-authentication-endpoints","title":"Setup &amp; Authentication Endpoints","text":""},{"location":"api-reference/admin-users/#check-setup-status","title":"Check Setup Status","text":"<p>Check if initial setup is needed (i.e., if any admin users exist).</p> <p>Endpoint: <code>GET /api/admin-users/setup/status</code></p> <p>Authentication: None required</p> <p>Response: <pre><code>{\n  \"needs_setup\": true,\n  \"has_users\": false\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Success</p>"},{"location":"api-reference/admin-users/#setup-owner-account","title":"Setup Owner Account","text":"<p>Create the first owner account. Only works when no users exist.</p> <p>Endpoint: <code>POST /api/admin-users/setup</code></p> <p>Authentication: None required (only works when no users exist)</p> <p>Request Body: <pre><code>{\n  \"email\": \"admin@example.com\",\n  \"display_name\": \"Admin User\",\n  \"password\": \"SecurePassword123!\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"bearer\",\n  \"user\": {\n    \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"role\": \"owner\",\n    \"is_active\": true,\n    \"created_at\": \"2025-10-16T10:30:00Z\",\n    \"last_login\": \"2025-10-16T10:30:00Z\",\n    \"metadata\": {}\n  }\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Owner account created successfully - <code>400</code>: Setup already completed (users exist) - <code>422</code>: Invalid email or password (minimum 8 characters)</p> <p>Example: <pre><code>curl -X POST http://localhost:8004/api/admin-users/setup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"password\": \"SecurePassword123!\"\n  }'\n</code></pre></p>"},{"location":"api-reference/admin-users/#login","title":"Login","text":"<p>Authenticate with email and password to receive a JWT token.</p> <p>Endpoint: <code>POST /api/admin-users/login</code></p> <p>Authentication: None required</p> <p>Request Body: <pre><code>{\n  \"email\": \"admin@example.com\",\n  \"password\": \"SecurePassword123!\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"bearer\",\n  \"user\": {\n    \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"role\": \"owner\",\n    \"is_active\": true,\n    \"created_at\": \"2025-10-16T10:30:00Z\",\n    \"last_login\": \"2025-10-16T11:45:00Z\",\n    \"metadata\": {}\n  }\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Login successful - <code>401</code>: Invalid email or password - <code>403</code>: Account is inactive</p> <p>Example: <pre><code>curl -X POST http://localhost:8004/api/admin-users/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"admin@example.com\",\n    \"password\": \"SecurePassword123!\"\n  }'\n</code></pre></p>"},{"location":"api-reference/admin-users/#logout","title":"Logout","text":"<p>Revoke the current JWT session.</p> <p>Endpoint: <code>POST /api/admin-users/logout</code></p> <p>Authentication: JWT Bearer token required</p> <p>Request: No body required</p> <p>Response: <pre><code>{\n  \"message\": \"Logged out successfully\"\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Session revoked successfully - <code>401</code>: Invalid or missing JWT token</p> <p>Example: <pre><code>curl -X POST http://localhost:8004/api/admin-users/logout \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre></p>"},{"location":"api-reference/admin-users/#get-current-user","title":"Get Current User","text":"<p>Get information about the currently authenticated user.</p> <p>Endpoint: <code>GET /api/admin-users/me</code></p> <p>Authentication: JWT Bearer token required</p> <p>Response: <pre><code>{\n  \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n  \"email\": \"admin@example.com\",\n  \"display_name\": \"Admin User\",\n  \"role\": \"owner\",\n  \"is_active\": true,\n  \"created_at\": \"2025-10-16T10:30:00Z\",\n  \"last_login\": \"2025-10-16T11:45:00Z\",\n  \"metadata\": {}\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Success - <code>401</code>: Invalid or missing JWT token</p> <p>Example: <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users/me\n</code></pre></p>"},{"location":"api-reference/admin-users/#user-management-endpoints","title":"User Management Endpoints","text":""},{"location":"api-reference/admin-users/#list-admin-users","title":"List Admin Users","text":"<p>List all admin users. Requires owner or admin role.</p> <p>Endpoint: <code>GET /api/admin-users</code></p> <p>Authentication: JWT Bearer token required (owner or admin role)</p> <p>Response: <pre><code>[\n  {\n    \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"email\": \"admin@example.com\",\n    \"display_name\": \"Admin User\",\n    \"role\": \"owner\",\n    \"is_active\": true,\n    \"created_at\": \"2025-10-16T10:30:00Z\",\n    \"last_login\": \"2025-10-16T11:45:00Z\",\n    \"metadata\": {}\n  },\n  {\n    \"user_id\": \"f1e753ec-7e92-4767-8de8-e9a5bed22840\",\n    \"email\": \"user@example.com\",\n    \"display_name\": \"Regular User\",\n    \"role\": \"user\",\n    \"is_active\": true,\n    \"created_at\": \"2025-10-16T12:00:00Z\",\n    \"last_login\": \"2025-10-16T13:00:00Z\",\n    \"metadata\": {}\n  }\n]\n</code></pre></p> <p>Status Codes: - <code>200</code>: Success - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions (requires owner or admin role)</p> <p>Example: <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8004/api/admin-users\n</code></pre></p>"},{"location":"api-reference/admin-users/#create-admin-user","title":"Create Admin User","text":"<p>Create a new admin user. Requires owner or admin role.</p> <p>Endpoint: <code>POST /api/admin-users</code></p> <p>Authentication: JWT Bearer token required (owner or admin role)</p> <p>Request Body: <pre><code>{\n  \"email\": \"newuser@example.com\",\n  \"display_name\": \"New User\",\n  \"password\": \"SecurePass456!\",\n  \"role\": \"user\",\n  \"metadata\": {\n    \"department\": \"Engineering\"\n  }\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"user_id\": \"b2c3d4e5-6789-4abc-def0-123456789abc\",\n  \"email\": \"newuser@example.com\",\n  \"display_name\": \"New User\",\n  \"role\": \"user\",\n  \"is_active\": true,\n  \"created_at\": \"2025-10-16T14:00:00Z\",\n  \"last_login\": null,\n  \"metadata\": {\n    \"department\": \"Engineering\"\n  }\n}\n</code></pre></p> <p>Role Restrictions: - Owner: Can create any role (owner, admin, user) - Admin: Can only create \"user\" role</p> <p>Status Codes: - <code>200</code>: User created successfully - <code>400</code>: Email already registered - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions - <code>422</code>: Invalid email or password (minimum 8 characters)</p> <p>Example: <pre><code>curl -X POST http://localhost:8004/api/admin-users \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"newuser@example.com\",\n    \"display_name\": \"New User\",\n    \"password\": \"SecurePass456!\",\n    \"role\": \"user\"\n  }'\n</code></pre></p>"},{"location":"api-reference/admin-users/#update-admin-user","title":"Update Admin User","text":"<p>Update an existing admin user. Requires owner or admin role.</p> <p>Endpoint: <code>PUT /api/admin-users/{user_id}</code></p> <p>Authentication: JWT Bearer token required (owner or admin role)</p> <p>Request Body (all fields optional): <pre><code>{\n  \"display_name\": \"Updated Name\",\n  \"role\": \"admin\",\n  \"is_active\": false,\n  \"metadata\": {\n    \"department\": \"Management\"\n  }\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"user_id\": \"b2c3d4e5-6789-4abc-def0-123456789abc\",\n  \"email\": \"newuser@example.com\",\n  \"display_name\": \"Updated Name\",\n  \"role\": \"admin\",\n  \"is_active\": false,\n  \"created_at\": \"2025-10-16T14:00:00Z\",\n  \"last_login\": \"2025-10-16T15:00:00Z\",\n  \"metadata\": {\n    \"department\": \"Management\"\n  }\n}\n</code></pre></p> <p>Role Restrictions: - Owner: Can update any user - Admin: Cannot update owner or admin users</p> <p>Status Codes: - <code>200</code>: User updated successfully - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions - <code>404</code>: User not found</p> <p>Example: <pre><code>curl -X PUT http://localhost:8004/api/admin-users/$USER_ID \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"display_name\": \"Updated Name\",\n    \"is_active\": false\n  }'\n</code></pre></p>"},{"location":"api-reference/admin-users/#delete-admin-user","title":"Delete Admin User","text":"<p>Delete an admin user. Requires owner or admin role.</p> <p>Endpoint: <code>DELETE /api/admin-users/{user_id}</code></p> <p>Authentication: JWT Bearer token required (owner or admin role)</p> <p>Response: <pre><code>{\n  \"message\": \"User deleted successfully\"\n}\n</code></pre></p> <p>Role Restrictions: - Owner: Can delete any user except themselves - Admin: Cannot delete owner or admin users</p> <p>Status Codes: - <code>200</code>: User deleted successfully - <code>400</code>: Cannot delete yourself - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions - <code>404</code>: User not found</p> <p>Example: <pre><code>curl -X DELETE http://localhost:8004/api/admin-users/$USER_ID \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre></p>"},{"location":"api-reference/admin-users/#change-password","title":"Change Password","text":"<p>Change password for the current user or another user (owner only).</p> <p>Endpoint: <code>POST /api/admin-users/{user_id}/change-password</code></p> <p>Authentication: JWT Bearer token required</p> <p>Request Body: <pre><code>{\n  \"current_password\": \"OldPassword123!\",\n  \"new_password\": \"NewPassword456!\"\n}\n</code></pre></p> <p>Note: <code>current_password</code> is only required when changing your own password.</p> <p>Response: <pre><code>{\n  \"message\": \"Password changed successfully\"\n}\n</code></pre></p> <p>Status Codes: - <code>200</code>: Password changed successfully - <code>400</code>: Invalid current password - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions - <code>404</code>: User not found - <code>422</code>: Invalid new password (minimum 8 characters)</p> <p>Example (change own password): <pre><code>curl -X POST http://localhost:8004/api/admin-users/me/change-password \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"current_password\": \"OldPassword123!\",\n    \"new_password\": \"NewPassword456!\"\n  }'\n</code></pre></p> <p>Example (owner changing another user's password): <pre><code>curl -X POST http://localhost:8004/api/admin-users/$USER_ID/change-password \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"new_password\": \"NewPassword456!\"\n  }'\n</code></pre></p>"},{"location":"api-reference/admin-users/#audit-log-endpoints","title":"Audit Log Endpoints","text":""},{"location":"api-reference/admin-users/#view-audit-logs","title":"View Audit Logs","text":"<p>View audit logs of admin actions. Requires owner or admin role.</p> <p>Endpoint: <code>GET /api/admin-users/audit-logs</code></p> <p>Authentication: JWT Bearer token required (owner or admin role)</p> <p>Query Parameters: - <code>limit</code> (optional): Maximum number of logs to return (default: 50, max: 100) - <code>user_id</code> (optional): Filter logs by user ID - <code>action</code> (optional): Filter logs by action type</p> <p>Response: <pre><code>[\n  {\n    \"audit_id\": \"c3d4e5f6-7890-4abc-def0-234567890def\",\n    \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"action\": \"created_user\",\n    \"resource_type\": \"admin_user\",\n    \"resource_id\": \"b2c3d4e5-6789-4abc-def0-123456789abc\",\n    \"details\": {\n      \"email\": \"newuser@example.com\",\n      \"role\": \"user\"\n    },\n    \"ip_address\": \"127.0.0.1\",\n    \"created_at\": \"2025-10-16T14:00:00Z\"\n  },\n  {\n    \"audit_id\": \"d4e5f6a7-8901-4bcd-ef01-345678901ef0\",\n    \"user_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"action\": \"login\",\n    \"resource_type\": \"admin_user\",\n    \"resource_id\": \"ae89197e-e24a-4e55-a9e2-70ba9a273730\",\n    \"details\": {\n      \"success\": true\n    },\n    \"ip_address\": \"127.0.0.1\",\n    \"created_at\": \"2025-10-16T11:45:00Z\"\n  }\n]\n</code></pre></p> <p>Logged Actions: - <code>setup_owner</code>: Initial owner account creation - <code>login</code>: User login - <code>logout</code>: User logout - <code>created_user</code>: New user created - <code>updated_user</code>: User updated - <code>deleted_user</code>: User deleted - <code>changed_password</code>: Password changed</p> <p>Status Codes: - <code>200</code>: Success - <code>401</code>: Invalid or missing JWT token - <code>403</code>: Insufficient permissions (requires owner or admin role)</p> <p>Example: <pre><code># Get last 50 audit logs\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8004/api/admin-users/audit-logs?limit=50\"\n\n# Get logs for specific user\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8004/api/admin-users/audit-logs?user_id=$USER_ID\"\n\n# Get logs for specific action\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8004/api/admin-users/audit-logs?action=login\"\n</code></pre></p>"},{"location":"api-reference/admin-users/#error-responses","title":"Error Responses","text":"<p>All endpoints return consistent error responses:</p> <pre><code>{\n  \"detail\": \"Error message describing what went wrong\"\n}\n</code></pre> <p>Common Error Codes: - <code>400</code>: Bad Request (invalid input, duplicate email, etc.) - <code>401</code>: Unauthorized (missing/invalid authentication) - <code>403</code>: Forbidden (insufficient permissions) - <code>404</code>: Not Found (user doesn't exist) - <code>422</code>: Unprocessable Entity (validation error) - <code>500</code>: Internal Server Error</p>"},{"location":"api-reference/admin-users/#role-based-access-control","title":"Role-Based Access Control","text":""},{"location":"api-reference/admin-users/#role-hierarchy","title":"Role Hierarchy","text":"<ol> <li>Owner: Full access to all features</li> <li>Create/modify/delete any user (including other owners and admins)</li> <li>Change any user's password</li> <li>View all audit logs</li> <li> <p>All management operations</p> </li> <li> <p>Admin: Management access with restrictions</p> </li> <li>Create/modify/delete \"user\" role accounts only</li> <li>Cannot modify owner or admin users</li> <li>View all audit logs</li> <li> <p>All management operations</p> </li> <li> <p>User: Read-only access</p> </li> <li>View own profile</li> <li>Change own password</li> <li>View dashboard (read-only)</li> <li>Cannot create/modify/delete users</li> </ol>"},{"location":"api-reference/admin-users/#endpoint-access-matrix","title":"Endpoint Access Matrix","text":"Endpoint Owner Admin User GET /api/admin-users/me \u2705 \u2705 \u2705 GET /api/admin-users \u2705 \u2705 \u274c POST /api/admin-users \u2705 (any role) \u2705 (user role only) \u274c PUT /api/admin-users/{id} \u2705 (any user) \u2705 (user role only) \u274c DELETE /api/admin-users/{id} \u2705 (any user except self) \u2705 (user role only) \u274c POST /api/admin-users/{id}/change-password \u2705 (any user) \u2705 (own password) \u2705 (own password) GET /api/admin-users/audit-logs \u2705 \u2705 \u274c"},{"location":"api-reference/admin-users/#testing","title":"Testing","text":""},{"location":"api-reference/admin-users/#integration-tests","title":"Integration Tests","text":"<p>Comprehensive integration tests verify all endpoints:</p> <p>Test Script: <code>scripts/test_jwt_integration.py</code></p> <p>Run Tests: <pre><code>python3 scripts/test_jwt_integration.py\n</code></pre></p> <p>Test Coverage: - Setup and login flows - JWT and legacy authentication - All CRUD operations - Role-based permissions - Security validation - Audit logging</p> <p>See Integration Tests Documentation for details.</p>"},{"location":"api-reference/admin-users/#related-documentation","title":"Related Documentation","text":"<ul> <li>Admin Dashboard Authentication Guide - User guide for authentication</li> <li>Integration Tests - How to test the API</li> <li>Environment Variables - Configuration options</li> </ul>"},{"location":"api-reference/jobs/","title":"Jobs API","text":"<p>The Jobs API provides endpoints for managing job lifecycle in the SaaS LiteLLM platform. Jobs are logical groupings of related LLM calls that represent a single business operation or workflow.</p>"},{"location":"api-reference/jobs/#overview","title":"Overview","text":"<p>Jobs enable simplified billing where 1 job = 1 credit, regardless of how many LLM calls are made within that job. This provides a business-centric approach to cost management.</p> <p>Base URL: <code>/api/jobs</code></p> <p>Authentication: All endpoints require a Bearer token (virtual API key) in the <code>Authorization</code> header.</p>"},{"location":"api-reference/jobs/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/jobs/#create-job","title":"Create Job","text":"<p>Create a new job for tracking multiple LLM calls.</p> <p>Endpoint: <code>POST /api/jobs/create</code></p> <p>Authentication: Required (virtual key)</p> <p>Request Body:</p> <pre><code>{\n  \"team_id\": \"string\",\n  \"user_id\": \"string (optional)\",\n  \"job_type\": \"string\",\n  \"metadata\": {\n    \"key\": \"value\"\n  }\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>team_id</code> string Yes Team identifier (must match authenticated team) <code>user_id</code> string No Optional user identifier for tracking <code>job_type</code> string Yes Type of job (e.g., \"document_analysis\", \"chat_session\") <code>metadata</code> object No Custom metadata for the job <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2025-10-14T12:00:00.000Z\"\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>job_id</code> string (UUID) Unique identifier for the job <code>status</code> string Initial status (always \"pending\") <code>created_at</code> string (ISO 8601) Job creation timestamp <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"user_id\": \"john@acme.com\",\n    \"job_type\": \"resume_analysis\",\n    \"metadata\": {\n      \"document_id\": \"doc_123\",\n      \"document_name\": \"resume.pdf\"\n    }\n  }'\n</code></pre> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"user_id\": \"john@acme.com\",\n        \"job_type\": \"resume_analysis\",\n        \"metadata\": {\n            \"document_id\": \"doc_123\",\n            \"document_name\": \"resume.pdf\"\n        }\n    }\n)\n\njob = response.json()\nprint(f\"Created job: {job['job_id']}\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst VIRTUAL_KEY = \"sk-your-virtual-key\";\n\nconst response = await fetch(`${API_URL}/jobs/create`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    team_id: 'acme-corp',\n    user_id: 'john@acme.com',\n    job_type: 'resume_analysis',\n    metadata: {\n      document_id: 'doc_123',\n      document_name: 'resume.pdf'\n    }\n  })\n});\n\nconst job = await response.json();\nconsole.log(`Created job: ${job.job_id}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Virtual key does not belong to the specified team 422 Validation Error Invalid request data 500 Internal Server Error Server error <p>Example Error Response:</p> <pre><code>{\n  \"detail\": \"API key does not belong to team 'acme-corp'\"\n}\n</code></pre>"},{"location":"api-reference/jobs/#get-job","title":"Get Job","text":"<p>Retrieve details about a specific job.</p> <p>Endpoint: <code>GET /api/jobs/{job_id}</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"team_id\": \"acme-corp\",\n  \"user_id\": \"john@acme.com\",\n  \"job_type\": \"resume_analysis\",\n  \"status\": \"in_progress\",\n  \"created_at\": \"2025-10-14T12:00:00.000Z\",\n  \"started_at\": \"2025-10-14T12:00:05.000Z\",\n  \"completed_at\": null,\n  \"model_groups_used\": [\"ResumeAgent\"],\n  \"credit_applied\": false,\n  \"metadata\": {\n    \"document_id\": \"doc_123\",\n    \"document_name\": \"resume.pdf\"\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>job_id</code> string (UUID) Unique job identifier <code>team_id</code> string Team that owns this job <code>user_id</code> string User who created the job (optional) <code>job_type</code> string Type of job <code>status</code> string Current status (pending, in_progress, completed, failed) <code>created_at</code> string (ISO 8601) Job creation timestamp <code>started_at</code> string (ISO 8601) When first LLM call was made (null if pending) <code>completed_at</code> string (ISO 8601) When job was completed (null if not completed) <code>model_groups_used</code> array List of model groups used in this job <code>credit_applied</code> boolean Whether a credit was deducted for this job <code>metadata</code> object Custom metadata <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET http://localhost:8003/api/jobs/550e8400-e29b-41d4-a716-446655440000 \\\n  -H \"Authorization: Bearer sk-your-virtual-key\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/jobs/{job_id}\",\n    headers=headers\n)\n\njob = response.json()\nprint(f\"Job status: {job['status']}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/jobs/${jobId}`, {\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`\n  }\n});\n\nconst job = await response.json();\nconsole.log(`Job status: ${job.status}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team 404 Not Found Job not found"},{"location":"api-reference/jobs/#complete-job","title":"Complete Job","text":"<p>Mark a job as complete and trigger credit deduction (if applicable).</p> <p>Endpoint: <code>POST /api/jobs/{job_id}/complete</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Request Body:</p> <pre><code>{\n  \"status\": \"completed\",\n  \"metadata\": {\n    \"result\": \"success\",\n    \"output_file\": \"analysis_123.json\"\n  },\n  \"error_message\": \"Optional error message if status is 'failed'\"\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>status</code> string Yes Job final status: \"completed\" or \"failed\" <code>metadata</code> object No Additional metadata to merge with existing metadata <code>error_message</code> string No Error message (only if status is \"failed\") <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"completed_at\": \"2025-10-14T12:05:23.000Z\",\n  \"costs\": {\n    \"total_calls\": 3,\n    \"successful_calls\": 3,\n    \"failed_calls\": 0,\n    \"total_tokens\": 1350,\n    \"total_cost_usd\": 0.0045,\n    \"avg_latency_ms\": 1200,\n    \"credit_applied\": true,\n    \"credits_remaining\": 999\n  },\n  \"calls\": [\n    {\n      \"call_id\": \"call-uuid-1\",\n      \"purpose\": \"parse\",\n      \"model_group\": \"ResumeAgent\",\n      \"tokens\": 450,\n      \"latency_ms\": 1250,\n      \"error\": null\n    },\n    {\n      \"call_id\": \"call-uuid-2\",\n      \"purpose\": \"analyze\",\n      \"model_group\": \"ResumeAgent\",\n      \"tokens\": 480,\n      \"latency_ms\": 1180,\n      \"error\": null\n    },\n    {\n      \"call_id\": \"call-uuid-3\",\n      \"purpose\": \"summarize\",\n      \"model_group\": \"ResumeAgent\",\n      \"tokens\": 420,\n      \"latency_ms\": 1170,\n      \"error\": null\n    }\n  ]\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>job_id</code> string (UUID) Job identifier <code>status</code> string Final job status <code>completed_at</code> string (ISO 8601) Completion timestamp <code>costs</code> object Aggregated cost information <code>costs.total_calls</code> integer Total number of LLM calls <code>costs.successful_calls</code> integer Number of successful calls <code>costs.failed_calls</code> integer Number of failed calls <code>costs.total_tokens</code> integer Total tokens used across all calls <code>costs.total_cost_usd</code> number Total cost in USD (for internal tracking) <code>costs.avg_latency_ms</code> integer Average latency in milliseconds <code>costs.credit_applied</code> boolean Whether a credit was deducted <code>costs.credits_remaining</code> integer Credits remaining for the team <code>calls</code> array List of all LLM calls made in this job <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/jobs/550e8400-e29b-41d4-a716-446655440000/complete \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"status\": \"completed\",\n    \"metadata\": {\n      \"result\": \"success\",\n      \"output_file\": \"analysis_123.json\"\n    }\n  }'\n</code></pre> <pre><code>response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\n        \"status\": \"completed\",\n        \"metadata\": {\n            \"result\": \"success\",\n            \"output_file\": \"analysis_123.json\"\n        }\n    }\n)\n\nresult = response.json()\nprint(f\"Total calls: {result['costs']['total_calls']}\")\nprint(f\"Total cost: ${result['costs']['total_cost_usd']:.4f}\")\nprint(f\"Credit deducted: {result['costs']['credit_applied']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/jobs/${jobId}/complete`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    status: 'completed',\n    metadata: {\n      result: 'success',\n      output_file: 'analysis_123.json'\n    }\n  })\n});\n\nconst result = await response.json();\nconsole.log(`Total calls: ${result.costs.total_calls}`);\nconsole.log(`Credit deducted: ${result.costs.credit_applied}`);\nconsole.log(`Credits remaining: ${result.costs.credits_remaining}`);\n</code></pre> <p>Credit Deduction Rules:</p> <p>Credits are ONLY deducted when ALL of these conditions are met:</p> <ol> <li>Job status is \"completed\" (not \"failed\")</li> <li>All LLM calls succeeded (no failed calls)</li> <li>Credit hasn't already been applied</li> </ol> <p>Example: Completing Failed Job (No Credit Deduction):</p> <pre><code>curl -X POST http://localhost:8003/api/jobs/{job_id}/complete \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"status\": \"failed\",\n    \"error_message\": \"Document parsing failed\",\n    \"metadata\": {\n      \"error_type\": \"ParsingError\"\n    }\n  }'\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team 404 Not Found Job not found 422 Validation Error Invalid status value"},{"location":"api-reference/jobs/#update-job-metadata","title":"Update Job Metadata","text":"<p>Append metadata to a job during execution. Useful for enriching job context as work progresses.</p> <p>Endpoint: <code>PATCH /api/jobs/{job_id}/metadata</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Request Body:</p> <pre><code>{\n  \"metadata\": {\n    \"conversation_turn\": 3,\n    \"user_sentiment\": \"positive\",\n    \"tokens_so_far\": 450\n  }\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>metadata</code> object Yes Metadata to merge with existing job metadata <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"metadata\": {\n    \"document_id\": \"doc_123\",\n    \"conversation_turn\": 3,\n    \"user_sentiment\": \"positive\",\n    \"tokens_so_far\": 450\n  },\n  \"updated_at\": \"2025-10-14T12:03:45.000Z\"\n}\n</code></pre> <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X PATCH http://localhost:8003/api/jobs/550e8400-e29b-41d4-a716-446655440000/metadata \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"metadata\": {\n      \"conversation_turn\": 3,\n      \"user_sentiment\": \"positive\"\n    }\n  }'\n</code></pre> <pre><code>response = requests.patch(\n    f\"{API_URL}/jobs/{job_id}/metadata\",\n    headers=headers,\n    json={\n        \"metadata\": {\n            \"conversation_turn\": 3,\n            \"user_sentiment\": \"positive\"\n        }\n    }\n)\n\nresult = response.json()\nprint(f\"Updated metadata: {result['metadata']}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/jobs/${jobId}/metadata`, {\n  method: 'PATCH',\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    metadata: {\n      conversation_turn: 3,\n      user_sentiment: 'positive'\n    }\n  })\n});\n\nconst result = await response.json();\nconsole.log(`Updated metadata:`, result.metadata);\n</code></pre> <p>Use Cases:</p> <ol> <li>Chat Applications</li> <li>Track conversation history</li> <li>Record turn numbers</li> <li> <p>Store user sentiment or feedback</p> </li> <li> <p>Multi-Step Workflows</p> </li> <li>Record intermediate results</li> <li>Track progress through pipeline</li> <li> <p>Store decision points</p> </li> <li> <p>Agent Workflows</p> </li> <li>Log tool calls and results</li> <li>Track reasoning steps</li> <li>Record agent state changes</li> </ol> <p>Example: Chat Application Tracking</p> <pre><code># Create job for chat session\njob_response = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"chat_session\",\n        \"metadata\": {\n            \"session_id\": \"sess_123\",\n            \"user_id\": \"user_456\"\n        }\n    }\n)\njob_id = job_response.json()[\"job_id\"]\n\n# User turn 1\nllm_response_1 = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model\": \"gpt-4\",\n        \"messages\": messages_turn_1,\n        \"call_metadata\": {\n            \"turn\": 1,\n            \"user_message\": \"How do I deploy to production?\"\n        }\n    }\n)\n\n# Update job metadata after turn 1\nrequests.patch(\n    f\"{API_URL}/jobs/{job_id}/metadata\",\n    headers=headers,\n    json={\n        \"metadata\": {\n            \"turns_completed\": 1,\n            \"last_topic\": \"deployment\"\n        }\n    }\n)\n\n# User turn 2\nllm_response_2 = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model\": \"gpt-4\",\n        \"messages\": messages_turn_2,\n        \"call_metadata\": {\n            \"turn\": 2,\n            \"user_message\": \"What about environment variables?\"\n        }\n    }\n)\n\n# Update metadata after turn 2\nrequests.patch(\n    f\"{API_URL}/jobs/{job_id}/metadata\",\n    headers=headers,\n    json={\n        \"metadata\": {\n            \"turns_completed\": 2,\n            \"last_topic\": \"environment_configuration\"\n        }\n    }\n)\n\n# Complete the chat session\nrequests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\n        \"status\": \"completed\",\n        \"metadata\": {\n            \"satisfaction_rating\": 5,\n            \"resolved\": True\n        }\n    }\n)\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team 404 Not Found Job not found 422 Validation Error Invalid metadata format <p>Notes:</p> <ul> <li>Metadata is merged with existing job metadata (not replaced)</li> <li>Use dot notation or nested objects to organize metadata</li> <li>Metadata persists through job lifecycle and is returned in job details</li> <li>Maximum metadata size: 10KB per job</li> </ul>"},{"location":"api-reference/jobs/#single-call-job-create-call-and-complete","title":"Single-Call Job (Create, Call, and Complete)","text":"<p>Create a job, make a single LLM call, and complete the job in one request. This is a convenience endpoint for simple workflows that only need one LLM call.</p> <p>Endpoint: <code>POST /api/jobs/create-and-call</code></p> <p>Authentication: Required (virtual key)</p> <p>Performance Benefits: - ~66% latency reduction (1 API call vs 3) - Single network round-trip - Automatic job lifecycle management</p> <p>Request Body:</p> <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"job_type\": \"chat_response\",\n  \"model\": \"gpt-4\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is Python?\"\n    }\n  ],\n  \"user_id\": \"optional-user-id\",\n  \"job_metadata\": {\n    \"session_id\": \"sess_123\"\n  },\n  \"purpose\": \"chat\",\n  \"temperature\": 0.7,\n  \"max_tokens\": 500\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>team_id</code> string Yes Team identifier (must match authenticated team) <code>job_type</code> string Yes Type of job (e.g., \"chat_response\", \"text_generation\") <code>model</code> string Yes Model alias or model group name <code>messages</code> array Yes OpenAI-compatible messages array <code>user_id</code> string No Optional user identifier for tracking <code>job_metadata</code> object No Custom metadata for the job <code>purpose</code> string No Optional label for the LLM call (e.g., \"chat\", \"generation\") <code>temperature</code> number No Sampling temperature (0.0-2.0, default: 0.7) <code>max_tokens</code> integer No Maximum tokens to generate <code>response_format</code> object No Structured output format (e.g., {\"type\": \"json_object\"}) <code>tools</code> array No Function calling tools <code>tool_choice</code> any No Tool choice strategy (\"auto\", \"none\", or specific tool) <code>top_p</code> number No Nucleus sampling parameter <code>frequency_penalty</code> number No Frequency penalty (-2.0 to 2.0) <code>presence_penalty</code> number No Presence penalty (-2.0 to 2.0) <code>stop</code> array No Stop sequences <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"response\": {\n    \"content\": \"Python is a high-level programming language...\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 256,\n    \"latency_ms\": 1340,\n    \"model\": \"gpt-4\"\n  },\n  \"costs\": {\n    \"total_calls\": 1,\n    \"successful_calls\": 1,\n    \"failed_calls\": 0,\n    \"total_tokens\": 256,\n    \"total_cost_usd\": 0.0128,\n    \"avg_latency_ms\": 1340,\n    \"credit_applied\": true,\n    \"credits_remaining\": 999\n  },\n  \"completed_at\": \"2025-10-14T12:00:05.340Z\"\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>job_id</code> string (UUID) Unique job identifier <code>status</code> string Always \"completed\" for successful calls <code>response.content</code> string The generated response content <code>response.finish_reason</code> string Why generation stopped: \"stop\", \"length\", or \"content_filter\" <code>metadata.tokens_used</code> integer Total tokens used (prompt + completion) <code>metadata.latency_ms</code> integer Call latency in milliseconds <code>metadata.model</code> string Model alias or group that was requested <code>costs</code> object Aggregated cost information <code>costs.credit_applied</code> boolean Whether a credit was deducted (always true for successful calls) <code>costs.credits_remaining</code> integer Credits remaining for the team <code>completed_at</code> string (ISO 8601) Job completion timestamp <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/jobs/create-and-call \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"chat_response\",\n    \"model\": \"gpt-4\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is Python?\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }'\n</code></pre> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/create-and-call\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"chat_response\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ],\n        \"temperature\": 0.7,\n        \"max_tokens\": 500\n    }\n)\n\nresult = response.json()\nprint(f\"Response: {result['response']['content']}\")\nprint(f\"Tokens used: {result['metadata']['tokens_used']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst VIRTUAL_KEY = \"sk-your-virtual-key\";\n\nconst response = await fetch(`${API_URL}/jobs/create-and-call`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    team_id: 'acme-corp',\n    job_type: 'chat_response',\n    model: 'gpt-4',\n    messages: [\n      {role: 'user', content: 'What is Python?'}\n    ],\n    temperature: 0.7,\n    max_tokens: 500\n  })\n});\n\nconst result = await response.json();\nconsole.log(`Response: ${result.response.content}`);\nconsole.log(`Tokens used: ${result.metadata.tokens_used}`);\nconsole.log(`Credits remaining: ${result.costs.credits_remaining}`);\n</code></pre> <p>When to Use This Endpoint:</p> <p>\u2705 Best for: - Chat applications with single-turn responses - Simple text generation tasks - Quick question/answer scenarios - Any workflow with exactly one LLM call</p> <p>\u274c Not ideal for: - Complex workflows requiring multiple LLM calls - Agentic workflows with branching logic - Batch processing multiple documents - Scenarios where you need to inspect intermediate results</p> <p>Performance Comparison:</p> Workflow Type API Calls Latency Use Case Single-Call 1 ~1.5s Chat apps, simple tasks Multi-Step 3+ ~4.5s+ Complex workflows <p>Error Handling:</p> <p>If the LLM call fails, the job is automatically marked as failed (no credit deducted):</p> <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/create-and-call\",\n        headers=headers,\n        json={\n            \"team_id\": \"acme-corp\",\n            \"job_type\": \"chat\",\n            \"model\": \"gpt-4\",\n            \"messages\": messages\n        },\n        timeout=30\n    )\n    response.raise_for_status()\n    result = response.json()\n\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 403:\n        print(f\"Access error: {e.response.json()['detail']}\")\n    elif e.response.status_code == 500:\n        print(f\"LLM call failed: {e.response.json()['detail']}\")\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Virtual key does not belong to team, or model access denied 422 Validation Error Invalid request data 500 Internal Server Error LLM call failed (job marked as failed, no credit charged)"},{"location":"api-reference/jobs/#single-call-job-with-streaming-create-call-and-complete","title":"Single-Call Job with Streaming (Create, Call, and Complete)","text":"<p>Create a job, stream LLM response in real-time via Server-Sent Events (SSE), and complete the job automatically. This is the recommended endpoint for chat applications and any scenario requiring real-time token streaming.</p> <p>Endpoint: <code>POST /api/jobs/create-and-call-stream</code></p> <p>Authentication: Required (virtual key)</p> <p>Performance Benefits: - Real-time streaming responses (lowest time-to-first-token) - Single API call (automatic job lifecycle) - Server-Sent Events (SSE) protocol - Automatic credit deduction on completion</p> <p>Request Body:</p> <p>Same as <code>/api/jobs/create-and-call</code> - see above for full parameter list.</p> <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"job_type\": \"chat_response\",\n  \"model\": \"gpt-4\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Tell me a short story\"\n    }\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 500\n}\n</code></pre> <p>Response:</p> <p>Server-Sent Events (SSE) stream with <code>Content-Type: text/event-stream</code></p> <p>SSE Event Format:</p> <p>Each event follows the SSE format: <code>data: {JSON}\\n\\n</code></p> <pre><code>data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Once\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" upon\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"finish_reason\":null}]}\n\ndata: [DONE]\n</code></pre> <p>Stream Events:</p> Event Description Chunk with <code>delta.role</code> First chunk, contains role (\"assistant\") Chunk with <code>delta.content</code> Text content chunk to append to response Chunk with <code>finish_reason</code> Final chunk, reason: \"stop\", \"length\", or \"content_filter\" <code>[DONE]</code> Stream complete (job auto-completed, credits deducted) <p>Example Request:</p> cURLPython (requests)Python (httpx async)JavaScript (fetch) <pre><code>curl -N -X POST http://localhost:8003/api/jobs/create-and-call-stream \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"chat_response\",\n    \"model\": \"gpt-4\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Tell me a short story\"}\n    ],\n    \"temperature\": 0.7\n  }'\n</code></pre> <p>Note: <code>-N</code> flag disables buffering for real-time streaming</p> <pre><code>import requests\nimport json\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/create-and-call-stream\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"chat_response\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Tell me a short story\"}\n        ],\n        \"temperature\": 0.7\n    },\n    stream=True  # Important: enable streaming\n)\n\n# Process Server-Sent Events\naccumulated = \"\"\nfor line in response.iter_lines():\n    if line:\n        line = line.decode('utf-8')\n        if line.startswith('data: '):\n            data_str = line[6:]  # Remove 'data: ' prefix\n\n            if data_str == '[DONE]':\n                print(\"\\n\\nStream complete!\")\n                break\n\n            try:\n                chunk = json.loads(data_str)\n                if chunk.get(\"choices\"):\n                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    if content:\n                        accumulated += content\n                        print(content, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                continue\n\nprint(f\"\\n\\nFull response: {accumulated}\")\n</code></pre> <pre><code>import httpx\nimport json\nimport asyncio\n\nasync def stream_chat():\n    API_URL = \"http://localhost:8003/api\"\n    VIRTUAL_KEY = \"sk-your-virtual-key\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\n            \"POST\",\n            f\"{API_URL}/jobs/create-and-call-stream\",\n            headers=headers,\n            json={\n                \"team_id\": \"acme-corp\",\n                \"job_type\": \"chat\",\n                \"model\": \"gpt-4\",\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Tell me a short story\"}\n                ]\n            },\n            timeout=60.0\n        ) as response:\n            accumulated = \"\"\n            async for line in response.aiter_lines():\n                if line.startswith('data: '):\n                    data_str = line[6:]\n\n                    if data_str == '[DONE]':\n                        print(\"\\n\\nStream complete!\")\n                        break\n\n                    try:\n                        chunk = json.loads(data_str)\n                        if chunk.get(\"choices\"):\n                            delta = chunk[\"choices\"][0].get(\"delta\", {})\n                            content = delta.get(\"content\", \"\")\n                            if content:\n                                accumulated += content\n                                print(content, end=\"\", flush=True)\n                    except json.JSONDecodeError:\n                        continue\n\n            return accumulated\n\n# Run the async function\nresponse = asyncio.run(stream_chat())\nprint(f\"\\n\\nFinal response: {response}\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst VIRTUAL_KEY = \"sk-your-virtual-key\";\n\nasync function streamChat() {\n  const response = await fetch(`${API_URL}/jobs/create-and-call-stream`, {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${VIRTUAL_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      team_id: 'acme-corp',\n      job_type: 'chat_response',\n      model: 'gpt-4',\n      messages: [\n        {role: 'user', content: 'Tell me a short story'}\n      ],\n      temperature: 0.7\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n  let accumulated = '';\n\n  while (true) {\n    const {done, value} = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n\n        if (data === '[DONE]') {\n          console.log('\\n\\nStream complete!');\n          return accumulated;\n        }\n\n        try {\n          const parsed = JSON.parse(data);\n          if (parsed.choices &amp;&amp; parsed.choices[0].delta.content) {\n            const content = parsed.choices[0].delta.content;\n            accumulated += content;\n            process.stdout.write(content); // Real-time output\n          }\n        } catch (e) {\n          // Skip invalid JSON\n        }\n      }\n    }\n  }\n\n  return accumulated;\n}\n\n// Use the function\nstreamChat().then(response =&gt; {\n  console.log(`\\n\\nFull response: ${response}`);\n});\n</code></pre> <p>When to Use This Endpoint:</p> <p>\u2705 Best for: - Chat applications (real-time user experience) - Any scenario requiring streaming responses - Single LLM call workflows - Lowest time-to-first-token latency</p> <p>\u274c Not ideal for: - Complex workflows with multiple LLM calls - When you need non-streaming responses (use <code>/api/jobs/create-and-call</code>) - Scenarios where streaming is not supported by the client</p> <p>Streaming vs Non-Streaming Comparison:</p> Endpoint Response Type Time to First Token Use Case <code>/create-and-call-stream</code> SSE Stream ~300ms Chat apps, real-time UX <code>/create-and-call</code> JSON ~1500ms Simple responses, batch processing <p>Error Handling:</p> <p>Errors are sent as SSE events:</p> <pre><code>data: {\"error\": \"Model not found\"}\n\ndata: [DONE]\n</code></pre> <p>Example error handling in Python:</p> <pre><code>for line in response.iter_lines():\n    if line:\n        line = line.decode('utf-8')\n        if line.startswith('data: '):\n            data_str = line[6:]\n\n            if data_str == '[DONE]':\n                break\n\n            try:\n                chunk = json.loads(data_str)\n\n                # Check for errors\n                if \"error\" in chunk:\n                    print(f\"Error: {chunk['error']}\")\n                    break\n\n                # Process normal chunk\n                if chunk.get(\"choices\"):\n                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    if content:\n                        print(content, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                continue\n</code></pre> <p>Automatic Job Completion:</p> <p>When the stream completes: 1. Job is automatically marked as \"completed\" 2. Credits are deducted based on budget mode (see Teams API) 3. LLM call is stored in database 4. Cost summary is calculated</p> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Virtual key does not belong to team, or model access denied 422 Validation Error Invalid request data 500 Internal Server Error LLM streaming failed (job marked as failed, no credit charged) <p>SSE Client Requirements:</p> <ul> <li>Set <code>stream=True</code> in requests library (Python)</li> <li>Use <code>response.body.getReader()</code> in JavaScript fetch</li> <li>Handle line-by-line parsing (<code>\\n\\n</code> delimiters)</li> <li>Parse JSON after removing <code>data:</code> prefix</li> </ul> <p>See Also:</p> <ul> <li>Streaming Guide - Detailed streaming documentation</li> <li>Non-Streaming Single-Call - JSON response version</li> <li>Multi-Step Streaming - For complex workflows</li> </ul>"},{"location":"api-reference/jobs/#get-job-costs","title":"Get Job Costs","text":"<p>Get detailed cost breakdown for a job (internal analytics).</p> <p>Endpoint: <code>GET /api/jobs/{job_id}/costs</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Response (200 OK):</p> <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"team_id\": \"acme-corp\",\n  \"job_type\": \"resume_analysis\",\n  \"status\": \"completed\",\n  \"costs\": {\n    \"total_cost_usd\": 0.0045,\n    \"breakdown\": [\n      {\n        \"call_id\": \"call-uuid-1\",\n        \"model\": \"gpt-4-turbo\",\n        \"purpose\": \"parse\",\n        \"prompt_tokens\": 200,\n        \"completion_tokens\": 250,\n        \"cost_usd\": 0.0015,\n        \"created_at\": \"2025-10-14T12:00:10.000Z\"\n      },\n      {\n        \"call_id\": \"call-uuid-2\",\n        \"model\": \"gpt-4-turbo\",\n        \"purpose\": \"analyze\",\n        \"prompt_tokens\": 220,\n        \"completion_tokens\": 260,\n        \"cost_usd\": 0.0016,\n        \"created_at\": \"2025-10-14T12:00:15.000Z\"\n      },\n      {\n        \"call_id\": \"call-uuid-3\",\n        \"model\": \"gpt-4-turbo\",\n        \"purpose\": \"summarize\",\n        \"prompt_tokens\": 180,\n        \"completion_tokens\": 240,\n        \"cost_usd\": 0.0014,\n        \"created_at\": \"2025-10-14T12:00:20.000Z\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Example Request:</p> cURLPython <pre><code>curl -X GET http://localhost:8003/api/jobs/{job_id}/costs \\\n  -H \"Authorization: Bearer sk-your-virtual-key\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/jobs/{job_id}/costs\",\n    headers=headers\n)\n\ncosts = response.json()\nprint(f\"Total cost: ${costs['costs']['total_cost_usd']:.4f}\")\nfor call in costs['costs']['breakdown']:\n    print(f\"  {call['purpose']}: ${call['cost_usd']:.4f}\")\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team 404 Not Found Job not found"},{"location":"api-reference/jobs/#job-lifecycle","title":"Job Lifecycle","text":"<p>Jobs progress through the following states:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; pending: Create Job\n    pending --&gt; in_progress: First LLM Call\n    in_progress --&gt; in_progress: Additional LLM Calls\n    in_progress --&gt; completed: Complete (status: completed)\n    in_progress --&gt; failed: Complete (status: failed)\n    completed --&gt; [*]: 1 credit deducted\n    failed --&gt; [*]: No credit deducted</code></pre>"},{"location":"api-reference/jobs/#job-states","title":"Job States","text":"State Description Credit Impact <code>pending</code> Job created, no LLM calls yet None <code>in_progress</code> At least one LLM call made None (until completion) <code>completed</code> Job successfully completed 1 credit deducted <code>failed</code> Job failed or cancelled No credit deducted"},{"location":"api-reference/jobs/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete example showing the full job workflow:</p> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create job\njob_response = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"user_id\": \"john@acme.com\",\n        \"job_type\": \"document_analysis\",\n        \"metadata\": {\n            \"document_id\": \"doc_123\",\n            \"document_name\": \"report.pdf\"\n        }\n    }\n)\njob = job_response.json()\njob_id = job[\"job_id\"]\nprint(f\"Created job: {job_id}\")\n\n# 2. Make LLM calls (see LLM Calls API documentation)\n# ...\n\n# 3. Complete job\ncomplete_response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\n        \"status\": \"completed\",\n        \"metadata\": {\n            \"result\": \"success\"\n        }\n    }\n)\n\nresult = complete_response.json()\nprint(f\"Job completed!\")\nprint(f\"Total calls: {result['costs']['total_calls']}\")\nprint(f\"Total cost: ${result['costs']['total_cost_usd']:.4f}\")\nprint(f\"Credit deducted: {result['costs']['credit_applied']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre>"},{"location":"api-reference/jobs/#rate-limiting","title":"Rate Limiting","text":"<p>Jobs API endpoints are subject to rate limiting per team:</p> <ul> <li>Requests per minute (RPM): Configurable per team</li> <li>Default: 100 RPM</li> </ul> <p>When rate limited, you'll receive a <code>429 Too Many Requests</code> response. Implement exponential backoff for retries.</p>"},{"location":"api-reference/jobs/#best-practices","title":"Best Practices","text":"<ol> <li>Always complete jobs - Even if failed, complete the job to prevent \"zombie jobs\"</li> <li>Use meaningful job types - Use descriptive types like \"resume_parsing\" not \"job1\"</li> <li>Add contextual metadata - Include relevant context for analytics and debugging</li> <li>Handle errors gracefully - Complete failed jobs with status \"failed\"</li> <li>Monitor credit balance - Check credits before creating jobs</li> </ol>"},{"location":"api-reference/jobs/#see-also","title":"See Also","text":"<ul> <li>LLM Calls API - Make LLM calls within jobs</li> <li>Job Workflow Guide - Detailed workflow documentation</li> <li>Teams API - Manage teams and credits</li> <li>Authentication Guide - API authentication details</li> </ul>"},{"location":"api-reference/llm-calls/","title":"LLM Calls API","text":"<p>The LLM Calls API provides endpoints for making both non-streaming and streaming LLM calls within job contexts. All calls are tracked, aggregated, and billed at the job level.</p>"},{"location":"api-reference/llm-calls/#overview","title":"Overview","text":"<p>LLM calls are always made within the context of a job. The API supports:</p> <ul> <li>Non-streaming calls - Standard request/response pattern</li> <li>Streaming calls - Real-time Server-Sent Events (SSE) streaming</li> <li>Model group resolution - Automatic model selection based on team permissions</li> <li>OpenAI-compatible format - Standard messages format</li> <li>Cost tracking - Automatic tracking of tokens and costs</li> </ul> <p>Base URL: <code>/api/jobs/{job_id}</code></p> <p>Authentication: All endpoints require a Bearer token (virtual API key) in the <code>Authorization</code> header.</p>"},{"location":"api-reference/llm-calls/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/llm-calls/#non-streaming-llm-call","title":"Non-Streaming LLM Call","text":"<p>Make a standard LLM call within a job context.</p> <p>Endpoint: <code>POST /api/jobs/{job_id}/llm-call</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Request Body:</p> <pre><code>{\n  \"model_group\": \"ResumeAgent\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Parse this resume...\"\n    }\n  ],\n  \"purpose\": \"resume_parsing\",\n  \"temperature\": 0.7,\n  \"max_tokens\": 1000\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>model_group</code> string Yes Name of model group (e.g., \"ResumeAgent\", \"ChatAgent\") <code>messages</code> array Yes OpenAI-compatible messages array <code>messages[].role</code> string Yes Message role: \"system\", \"user\", or \"assistant\" <code>messages[].content</code> string Yes Message content <code>purpose</code> string No Optional label for tracking (e.g., \"parsing\", \"analysis\") <code>temperature</code> number No Sampling temperature (0.0-2.0, default: 0.7) <code>max_tokens</code> integer No Maximum tokens to generate (optional) <p>Response (200 OK):</p> <pre><code>{\n  \"call_id\": \"call-uuid-123\",\n  \"response\": {\n    \"content\": \"Here is the parsed resume information...\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 450,\n    \"latency_ms\": 1250,\n    \"model_group\": \"ResumeAgent\"\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>call_id</code> string (UUID) Unique identifier for this LLM call <code>response.content</code> string The generated response content <code>response.finish_reason</code> string Why generation stopped: \"stop\", \"length\", or \"content_filter\" <code>metadata.tokens_used</code> integer Total tokens used (prompt + completion) <code>metadata.latency_ms</code> integer Call latency in milliseconds <code>metadata.model_group</code> string Model group that was used <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/jobs/{job_id}/llm-call \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_group\": \"ResumeAgent\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a resume parsing assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Extract key skills from this resume: ...\"\n      }\n    ],\n    \"purpose\": \"skill_extraction\",\n    \"temperature\": 0.3\n  }'\n</code></pre> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model_group\": \"ResumeAgent\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a resume parsing assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract key skills from this resume: ...\"\n            }\n        ],\n        \"purpose\": \"skill_extraction\",\n        \"temperature\": 0.3\n    }\n)\n\nresult = response.json()\nprint(result['response']['content'])\nprint(f\"Tokens used: {result['metadata']['tokens_used']}\")\nprint(f\"Latency: {result['metadata']['latency_ms']}ms\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst VIRTUAL_KEY = \"sk-your-virtual-key\";\n\nconst response = await fetch(`${API_URL}/jobs/${jobId}/llm-call`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${VIRTUAL_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model_group: 'ResumeAgent',\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a resume parsing assistant.'\n      },\n      {\n        role: 'user',\n        content: 'Extract key skills from this resume: ...'\n      }\n    ],\n    purpose: 'skill_extraction',\n    temperature: 0.3\n  })\n});\n\nconst result = await response.json();\nconsole.log(result.response.content);\nconsole.log(`Tokens used: ${result.metadata.tokens_used}`);\nconsole.log(`Latency: ${result.metadata.latency_ms}ms`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team, or model group not allowed 404 Not Found Job not found 422 Validation Error Invalid request data 500 Internal Server Error LLM call failed or server error <p>Example Error Response:</p> <pre><code>{\n  \"detail\": \"Team 'acme-corp' does not have access to model group 'GPT4Agent'\"\n}\n</code></pre>"},{"location":"api-reference/llm-calls/#streaming-llm-call","title":"Streaming LLM Call","text":"<p>Make a streaming LLM call with real-time Server-Sent Events (SSE).</p> <p>Endpoint: <code>POST /api/jobs/{job_id}/llm-call-stream</code></p> <p>Authentication: Required (virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>job_id</code> string (UUID) The job identifier <p>Request Body:</p> <pre><code>{\n  \"model_group\": \"ChatAgent\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Tell me a story\"\n    }\n  ],\n  \"purpose\": \"chat\",\n  \"temperature\": 0.8,\n  \"max_tokens\": 500\n}\n</code></pre> <p>Request Fields: (Same as non-streaming call)</p> Field Type Required Description <code>model_group</code> string Yes Name of model group <code>messages</code> array Yes OpenAI-compatible messages array <code>purpose</code> string No Optional label for tracking <code>temperature</code> number No Sampling temperature (0.0-2.0, default: 0.7) <code>max_tokens</code> integer No Maximum tokens to generate <p>Response Headers:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: text/event-stream\nCache-Control: no-cache\nX-Accel-Buffering: no\nConnection: keep-alive\n</code></pre> <p>Response Format (Server-Sent Events):</p> <pre><code>data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1697896000,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Once\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1697896000,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" upon\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1697896000,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1697896000,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" time\"},\"finish_reason\":null}]}\n\ndata: [DONE]\n</code></pre> <p>SSE Chunk Format:</p> <p>Each chunk is a JSON object prefixed with <code>data:</code>:</p> <pre><code>{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1697896000,\n  \"model\": \"gpt-4-turbo\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello\"\n      },\n      \"finish_reason\": null\n    }\n  ]\n}\n</code></pre> <p>Chunk Fields:</p> Field Type Description <code>id</code> string Unique completion ID <code>object</code> string Always \"chat.completion.chunk\" for streaming <code>created</code> integer Unix timestamp <code>model</code> string Actual model used (resolved from model group) <code>choices[].index</code> integer Choice index (always 0) <code>choices[].delta.role</code> string Role (only present in first chunk: \"assistant\") <code>choices[].delta.content</code> string Incremental text content <code>choices[].finish_reason</code> string null during streaming, \"stop\"/\"length\" at end <p>Final Chunk:</p> <p>The last chunk includes usage metadata:</p> <pre><code>{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1697896000,\n  \"model\": \"gpt-4-turbo\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {},\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 125,\n    \"completion_tokens\": 450,\n    \"total_tokens\": 575\n  }\n}\n</code></pre> <p>Stream Termination:</p> <p>The stream ends with:</p> <pre><code>data: [DONE]\n</code></pre> <p>Example Request:</p> Python (Raw)Python (Typed Client)JavaScript <pre><code>import requests\nimport json\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make streaming request\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call-stream\",\n    headers=headers,\n    json={\n        \"model_group\": \"ChatAgent\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Tell me a story\"}\n        ],\n        \"temperature\": 0.8\n    },\n    stream=True  # Important: enable streaming\n)\n\n# Process Server-Sent Events\naccumulated = \"\"\nfor line in response.iter_lines():\n    if line:\n        line = line.decode('utf-8')\n        if line.startswith('data: '):\n            data_str = line[6:]  # Remove 'data: ' prefix\n\n            if data_str == '[DONE]':\n                print(\"\\n\\nStream complete!\")\n                break\n\n            try:\n                chunk = json.loads(data_str)\n                if chunk.get(\"choices\"):\n                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    if content:\n                        accumulated += content\n                        print(content, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                continue\n\nprint(f\"\\n\\nFull response: {accumulated}\")\n</code></pre> <pre><code>from examples.typed_client import SaaSLLMClient\n\nasync def streaming_example():\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key\"\n    ) as client:\n        # Create job\n        job_id = await client.create_job(\"chat\")\n\n        # Stream response\n        accumulated = \"\"\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Tell me a story\"}\n            ],\n            temperature=0.8\n        ):\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n                content = delta.get(\"content\", \"\")\n                if content:\n                    accumulated += content\n                    print(content, end=\"\", flush=True)\n\n        print(f\"\\n\\nFull response: {accumulated}\")\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"Credits remaining: {result.credits_remaining}\")\n\nimport asyncio\nasyncio.run(streaming_example())\n</code></pre> <pre><code>async function streamChat(jobId, messages) {\n  const response = await fetch(`${API_URL}/jobs/${jobId}/llm-call-stream`, {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${VIRTUAL_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model_group: 'ChatAgent',\n      messages: messages,\n      temperature: 0.8\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  let accumulated = '';\n\n  while (true) {\n    const {done, value} = await reader.read();\n    if (done) break;\n\n    const text = decoder.decode(value);\n    const lines = text.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.substring(6);\n\n        if (data === '[DONE]') {\n          console.log('\\nStream complete');\n          return accumulated;\n        }\n\n        try {\n          const chunk = JSON.parse(data);\n          const content = chunk.choices?.[0]?.delta?.content || '';\n          if (content) {\n            accumulated += content;\n            process.stdout.write(content);  // Node.js\n            // Or: document.getElementById('output').textContent += content;  // Browser\n          }\n        } catch (e) {\n          // Ignore parse errors\n        }\n      }\n    }\n  }\n\n  return accumulated;\n}\n</code></pre> <p>Error Handling:</p> <p>Errors during streaming are sent as error chunks:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Model timeout exceeded\",\n    \"type\": \"timeout_error\",\n    \"code\": \"model_timeout\"\n  }\n}\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Job does not belong to your team, or model group not allowed 404 Not Found Job not found 422 Validation Error Invalid request data 500 Internal Server Error Stream failed or server error"},{"location":"api-reference/llm-calls/#model-parameters","title":"Model Parameters","text":""},{"location":"api-reference/llm-calls/#temperature","title":"Temperature","text":"<p>Controls randomness in the output.</p> <ul> <li>Range: 0.0 to 2.0</li> <li>Default: 0.7</li> <li>Lower values (0.0-0.5): More deterministic, focused responses</li> <li>Higher values (0.8-1.5): More creative, varied responses</li> </ul> <p>Examples:</p> <pre><code># Factual, precise response\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model_group\": \"AnalysisAgent\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"What is photosynthesis?\"}],\n        \"temperature\": 0.2  # Low temperature for facts\n    }\n)\n\n# Creative, varied response\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model_group\": \"ChatAgent\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Write a creative story\"}],\n        \"temperature\": 1.2  # High temperature for creativity\n    }\n)\n</code></pre>"},{"location":"api-reference/llm-calls/#max-tokens","title":"Max Tokens","text":"<p>Limits the maximum number of tokens to generate.</p> <ul> <li>Type: Integer</li> <li>Default: Varies by model (typically 1000-4000)</li> <li>Use cases: Limit response length, control costs</li> </ul> <p>Examples:</p> <pre><code># Short summary\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model_group\": \"SummaryAgent\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Summarize this article...\"}],\n        \"max_tokens\": 150  # Limit to ~150 tokens\n    }\n)\n\n# Long-form content\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"model_group\": \"WritingAgent\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Write a detailed article...\"}],\n        \"max_tokens\": 2000  # Allow up to 2000 tokens\n    }\n)\n</code></pre>"},{"location":"api-reference/llm-calls/#messages-format","title":"Messages Format","text":"<p>The <code>messages</code> array follows the OpenAI format:</p> <p>System Message:</p> <p>Sets the assistant's behavior and context.</p> <pre><code>{\n  \"role\": \"system\",\n  \"content\": \"You are a helpful assistant that specializes in resume analysis.\"\n}\n</code></pre> <p>User Message:</p> <p>User input or query.</p> <pre><code>{\n  \"role\": \"user\",\n  \"content\": \"Parse this resume and extract key skills.\"\n}\n</code></pre> <p>Assistant Message:</p> <p>Previous assistant responses (for multi-turn conversations).</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"content\": \"I've identified the following skills: Python, SQL, Machine Learning...\"\n}\n</code></pre> <p>Complete Multi-Turn Example:</p> <pre><code>messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a Python tutor.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is a list comprehension?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"A list comprehension is a concise way to create lists in Python...\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you show me an example?\"\n    }\n]\n</code></pre>"},{"location":"api-reference/llm-calls/#model-group-resolution","title":"Model Group Resolution","text":"<p>Model groups abstract the actual model selection, allowing you to:</p> <ul> <li>Change models without code changes - Update model group configuration</li> <li>Control costs - Use different models for different teams</li> <li>Manage permissions - Restrict which teams can use which models</li> <li>Implement fallbacks - Automatically fallback to alternative models</li> </ul> <p>Example:</p> <p>If \"ResumeAgent\" model group is configured with: - Primary: <code>gpt-4-turbo</code> - Fallbacks: <code>gpt-3.5-turbo</code>, <code>claude-3-sonnet</code></p> <p>Your call to \"ResumeAgent\" will: 1. Attempt <code>gpt-4-turbo</code> first 2. Fallback to <code>gpt-3.5-turbo</code> if primary fails 3. Fallback to <code>claude-3-sonnet</code> if both fail</p> <p>See Model Groups API for configuration details.</p>"},{"location":"api-reference/llm-calls/#complete-example-multi-step-job","title":"Complete Example: Multi-Step Job","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create job\njob = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"document_analysis\"\n    }\n).json()\n\njob_id = job[\"job_id\"]\nprint(f\"Created job: {job_id}\")\n\n# 2. Make multiple LLM calls\nsteps = [\n    (\"Parse document\", \"Extract key information from this document...\"),\n    (\"Classify content\", \"Classify the content type...\"),\n    (\"Generate summary\", \"Generate a concise summary...\")\n]\n\nfor purpose, prompt in steps:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"model_group\": \"AnalysisAgent\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"purpose\": purpose,\n            \"temperature\": 0.3\n        }\n    ).json()\n\n    print(f\"{purpose}: {response['response']['content'][:50]}...\")\n    print(f\"  Tokens: {response['metadata']['tokens_used']}\")\n\n# 3. Complete job\nresult = requests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\"status\": \"completed\"}\n).json()\n\nprint(f\"\\nJob completed!\")\nprint(f\"Total calls: {result['costs']['total_calls']}\")\nprint(f\"Total tokens: {result['costs']['total_tokens']}\")\nprint(f\"Total cost: ${result['costs']['total_cost_usd']:.4f}\")\nprint(f\"Credit deducted: {result['costs']['credit_applied']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre>"},{"location":"api-reference/llm-calls/#streaming-vs-non-streaming-comparison","title":"Streaming vs Non-Streaming Comparison","text":"Feature Non-Streaming Streaming Latency (perceived) High (~2000ms TTFT) Low (~300-500ms TTFT) User Experience Wait for complete response Progressive display Implementation Simpler More complex Use Case Batch processing Interactive apps Buffering Full response buffered Zero buffering Credits 1 per completed job 1 per completed job Cost Same Same <p>When to use non-streaming: - Batch processing jobs - Background tasks - Simple integrations - When full response is needed before processing</p> <p>When to use streaming: - Chat interfaces - Real-time user interactions - Long-form content generation - Lower perceived latency requirement</p>"},{"location":"api-reference/llm-calls/#rate-limiting","title":"Rate Limiting","text":"<p>LLM calls are subject to rate limiting:</p> <ul> <li>Requests per minute (RPM): Configurable per team</li> <li>Tokens per minute (TPM): Configurable per team</li> <li>Default: 60 RPM, 60,000 TPM</li> </ul> <p>When rate limited, you'll receive a <code>429 Too Many Requests</code> response.</p>"},{"location":"api-reference/llm-calls/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate model groups - Select the right model group for your use case</li> <li>Set reasonable temperatures - Lower for facts, higher for creativity</li> <li>Limit max_tokens - Control response length and costs</li> <li>Add purpose labels - Track different types of calls for analytics</li> <li>Handle errors gracefully - Implement retry logic with exponential backoff</li> <li>Use streaming for UX - Provide better user experience with real-time feedback</li> <li>Always complete jobs - Ensure jobs are completed to trigger proper billing</li> </ol>"},{"location":"api-reference/llm-calls/#see-also","title":"See Also","text":"<ul> <li>Jobs API - Create and manage jobs</li> <li>Job Workflow Guide - Complete workflow documentation</li> <li>Streaming Guide - Detailed streaming implementation</li> <li>Non-Streaming Guide - Standard call patterns</li> <li>Model Groups - Configure model groups</li> </ul>"},{"location":"api-reference/organizations/","title":"Organizations API","text":"<p>The Organizations API provides endpoints for managing organizations in the SaaS LiteLLM platform. Organizations are the top-level entity in the hierarchy and contain multiple teams.</p>"},{"location":"api-reference/organizations/#overview","title":"Overview","text":"<p>Organizations enable you to:</p> <ul> <li>Group teams hierarchically - Organize multiple teams under one organization</li> <li>Track organization-wide usage - View aggregated usage across all teams</li> <li>Manage billing - Bill at the organization level</li> <li>Organize customers - Each customer/company can be an organization</li> </ul> <p>Hierarchy:</p> <pre><code>Organization (e.g., \"ACME Corp\")\n\u251c\u2500\u2500 Team 1 (e.g., \"Engineering\")\n\u2502   \u251c\u2500\u2500 Job 1\n\u2502   \u251c\u2500\u2500 Job 2\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 Team 2 (e.g., \"Sales\")\n\u2502   \u251c\u2500\u2500 Job 1\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 Team 3 (e.g., \"Marketing\")\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Base URL: <code>/api/organizations</code></p> <p>Authentication: All endpoints require a Bearer token (virtual API key or admin key) in the <code>Authorization</code> header.</p>"},{"location":"api-reference/organizations/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/organizations/#create-organization","title":"Create Organization","text":"<p>Create a new organization.</p> <p>Endpoint: <code>POST /api/organizations/create</code></p> <p>Authentication: Required (admin/platform key)</p> <p>Request Body:</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"size\": \"Enterprise\",\n    \"country\": \"USA\",\n    \"contact_email\": \"admin@acme.com\"\n  }\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>organization_id</code> string Yes Unique organization identifier (e.g., \"org_acme\") <code>name</code> string Yes Organization display name <code>metadata</code> object No Custom metadata for the organization <p>Response (200 OK):</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"status\": \"active\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"size\": \"Enterprise\",\n    \"country\": \"USA\",\n    \"contact_email\": \"admin@acme.com\"\n  },\n  \"created_at\": \"2025-10-14T12:00:00.000Z\",\n  \"updated_at\": \"2025-10-14T12:00:00.000Z\"\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>organization_id</code> string Unique organization identifier <code>name</code> string Organization display name <code>status</code> string Organization status (always \"active\" on creation) <code>metadata</code> object Custom metadata <code>created_at</code> string (ISO 8601) Creation timestamp <code>updated_at</code> string (ISO 8601) Last update timestamp <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Authorization: Bearer sk-platform-admin-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"name\": \"ACME Corporation\",\n    \"metadata\": {\n      \"industry\": \"Technology\",\n      \"size\": \"Enterprise\",\n      \"country\": \"USA\",\n      \"contact_email\": \"admin@acme.com\"\n    }\n  }'\n</code></pre> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nADMIN_KEY = \"sk-platform-admin-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/organizations/create\",\n    headers=headers,\n    json={\n        \"organization_id\": \"org_acme\",\n        \"name\": \"ACME Corporation\",\n        \"metadata\": {\n            \"industry\": \"Technology\",\n            \"size\": \"Enterprise\",\n            \"country\": \"USA\",\n            \"contact_email\": \"admin@acme.com\"\n        }\n    }\n)\n\norg = response.json()\nprint(f\"Organization created: {org['name']}\")\nprint(f\"ID: {org['organization_id']}\")\nprint(f\"Status: {org['status']}\")\nprint(f\"Created: {org['created_at']}\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst ADMIN_KEY = \"sk-platform-admin-key\";\n\nconst response = await fetch(`${API_URL}/organizations/create`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${ADMIN_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    organization_id: 'org_acme',\n    name: 'ACME Corporation',\n    metadata: {\n      industry: 'Technology',\n      size: 'Enterprise',\n      country: 'USA',\n      contact_email: 'admin@acme.com'\n    }\n  })\n});\n\nconst org = await response.json();\nconsole.log(`Organization created: ${org.name}`);\nconsole.log(`ID: ${org.organization_id}`);\nconsole.log(`Status: ${org.status}`);\nconsole.log(`Created: ${org.created_at}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 400 Bad Request Organization with this ID already exists 401 Unauthorized Invalid or missing admin key 422 Validation Error Invalid request data 500 Internal Server Error Server error <p>Example Error Response:</p> <pre><code>{\n  \"detail\": \"Organization with ID 'org_acme' already exists\"\n}\n</code></pre>"},{"location":"api-reference/organizations/#get-organization","title":"Get Organization","text":"<p>Retrieve details about a specific organization.</p> <p>Endpoint: <code>GET /api/organizations/{organization_id}</code></p> <p>Authentication: Required (admin key or team key within organization)</p> <p>Path Parameters:</p> Parameter Type Description <code>organization_id</code> string The organization identifier <p>Response (200 OK):</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"name\": \"ACME Corporation\",\n  \"status\": \"active\",\n  \"metadata\": {\n    \"industry\": \"Technology\",\n    \"size\": \"Enterprise\",\n    \"country\": \"USA\",\n    \"contact_email\": \"admin@acme.com\"\n  },\n  \"created_at\": \"2025-10-14T12:00:00.000Z\",\n  \"updated_at\": \"2025-10-14T12:00:00.000Z\"\n}\n</code></pre> <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET http://localhost:8003/api/organizations/org_acme \\\n  -H \"Authorization: Bearer sk-platform-admin-key\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/organizations/org_acme\",\n    headers={\"Authorization\": f\"Bearer {ADMIN_KEY}\"}\n)\n\norg = response.json()\nprint(f\"Organization: {org['name']}\")\nprint(f\"ID: {org['organization_id']}\")\nprint(f\"Status: {org['status']}\")\nprint(f\"Industry: {org['metadata'].get('industry', 'N/A')}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/organizations/org_acme`, {\n  headers: {\n    'Authorization': `Bearer ${ADMIN_KEY}`\n  }\n});\n\nconst org = await response.json();\nconsole.log(`Organization: ${org.name}`);\nconsole.log(`ID: ${org.organization_id}`);\nconsole.log(`Status: ${org.status}`);\nconsole.log(`Industry: ${org.metadata.industry || 'N/A'}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing key 404 Not Found Organization not found"},{"location":"api-reference/organizations/#list-organization-teams","title":"List Organization Teams","text":"<p>List all teams belonging to an organization.</p> <p>Endpoint: <code>GET /api/organizations/{organization_id}/teams</code></p> <p>Authentication: Required (admin key or team key within organization)</p> <p>Path Parameters:</p> Parameter Type Description <code>organization_id</code> string The organization identifier <p>Response (200 OK):</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"team_count\": 3,\n  \"teams\": [\n    \"acme-engineering\",\n    \"acme-sales\",\n    \"acme-marketing\"\n  ]\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>organization_id</code> string Organization identifier <code>team_count</code> integer Number of teams in the organization <code>teams</code> array List of team IDs <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET http://localhost:8003/api/organizations/org_acme/teams \\\n  -H \"Authorization: Bearer sk-platform-admin-key\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/organizations/org_acme/teams\",\n    headers={\"Authorization\": f\"Bearer {ADMIN_KEY}\"}\n)\n\ndata = response.json()\nprint(f\"Organization: {data['organization_id']}\")\nprint(f\"Total teams: {data['team_count']}\")\nprint(\"Teams:\")\nfor team_id in data['teams']:\n    print(f\"  - {team_id}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/organizations/org_acme/teams`, {\n  headers: {\n    'Authorization': `Bearer ${ADMIN_KEY}`\n  }\n});\n\nconst data = await response.json();\nconsole.log(`Organization: ${data.organization_id}`);\nconsole.log(`Total teams: ${data.team_count}`);\nconsole.log('Teams:');\ndata.teams.forEach(teamId =&gt; {\n  console.log(`  - ${teamId}`);\n});\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing key 404 Not Found Organization not found"},{"location":"api-reference/organizations/#get-organization-usage","title":"Get Organization Usage","text":"<p>Get organization-wide usage statistics for a specified period.</p> <p>Endpoint: <code>GET /api/organizations/{organization_id}/usage</code></p> <p>Authentication: Required (admin key or team key within organization)</p> <p>Path Parameters:</p> Parameter Type Description <code>organization_id</code> string The organization identifier <p>Query Parameters:</p> Parameter Type Required Description <code>period</code> string Yes Period in format \"YYYY-MM\" (e.g., \"2025-10\") <p>Response (200 OK):</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"period\": \"2025-10\",\n  \"summary\": {\n    \"total_jobs\": 750,\n    \"completed_jobs\": 710,\n    \"failed_jobs\": 40,\n    \"credits_used\": 710,\n    \"total_cost_usd\": 35.25,\n    \"total_tokens\": 375000\n  },\n  \"teams\": {\n    \"acme-engineering\": {\n      \"jobs\": 450,\n      \"credits_used\": 425\n    },\n    \"acme-sales\": {\n      \"jobs\": 200,\n      \"credits_used\": 195\n    },\n    \"acme-marketing\": {\n      \"jobs\": 100,\n      \"credits_used\": 90\n    }\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>organization_id</code> string Organization identifier <code>period</code> string Requested period <code>summary.total_jobs</code> integer Total jobs across all teams <code>summary.completed_jobs</code> integer Successfully completed jobs <code>summary.failed_jobs</code> integer Failed jobs <code>summary.credits_used</code> integer Total credits consumed <code>summary.total_cost_usd</code> number Total cost in USD (internal tracking) <code>summary.total_tokens</code> integer Total tokens used <code>teams</code> object Per-team breakdown <code>teams[team_id].jobs</code> integer Number of jobs for this team <code>teams[team_id].credits_used</code> integer Credits used by this team <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET \"http://localhost:8003/api/organizations/org_acme/usage?period=2025-10\" \\\n  -H \"Authorization: Bearer sk-platform-admin-key\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/organizations/org_acme/usage\",\n    headers={\"Authorization\": f\"Bearer {ADMIN_KEY}\"},\n    params={\"period\": \"2025-10\"}\n)\n\nusage = response.json()\nprint(f\"Organization: {usage['organization_id']}\")\nprint(f\"Period: {usage['period']}\")\nprint(f\"\\nSummary:\")\nprint(f\"  Total jobs: {usage['summary']['total_jobs']}\")\nprint(f\"  Completed: {usage['summary']['completed_jobs']}\")\nprint(f\"  Failed: {usage['summary']['failed_jobs']}\")\nprint(f\"  Success rate: {usage['summary']['completed_jobs'] / usage['summary']['total_jobs'] * 100:.1f}%\")\nprint(f\"  Credits used: {usage['summary']['credits_used']}\")\nprint(f\"  Total cost: ${usage['summary']['total_cost_usd']:.2f}\")\nprint(f\"  Total tokens: {usage['summary']['total_tokens']:,}\")\n\nprint(f\"\\nTeam breakdown:\")\nfor team_id, stats in usage['teams'].items():\n    print(f\"  {team_id}:\")\n    print(f\"    Jobs: {stats['jobs']}\")\n    print(f\"    Credits used: {stats['credits_used']}\")\n</code></pre> <pre><code>const response = await fetch(\n  `${API_URL}/organizations/org_acme/usage?period=2025-10`,\n  {\n    headers: {\n      'Authorization': `Bearer ${ADMIN_KEY}`\n    }\n  }\n);\n\nconst usage = await response.json();\nconsole.log(`Organization: ${usage.organization_id}`);\nconsole.log(`Period: ${usage.period}`);\nconsole.log('\\nSummary:');\nconsole.log(`  Total jobs: ${usage.summary.total_jobs}`);\nconsole.log(`  Completed: ${usage.summary.completed_jobs}`);\nconsole.log(`  Failed: ${usage.summary.failed_jobs}`);\nconsole.log(`  Success rate: ${(usage.summary.completed_jobs / usage.summary.total_jobs * 100).toFixed(1)}%`);\nconsole.log(`  Credits used: ${usage.summary.credits_used}`);\nconsole.log(`  Total cost: $${usage.summary.total_cost_usd.toFixed(2)}`);\nconsole.log(`  Total tokens: ${usage.summary.total_tokens.toLocaleString()}`);\n\nconsole.log('\\nTeam breakdown:');\nfor (const [teamId, stats] of Object.entries(usage.teams)) {\n  console.log(`  ${teamId}:`);\n  console.log(`    Jobs: ${stats.jobs}`);\n  console.log(`    Credits used: ${stats.credits_used}`);\n}\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing key 404 Not Found Organization not found 422 Validation Error Invalid period format"},{"location":"api-reference/organizations/#organization-hierarchy","title":"Organization Hierarchy","text":"<p>Organizations follow a hierarchical structure:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Organization: org_acme                  \u2502\n\u2502 Name: ACME Corporation                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Teams:                                  \u2502\n\u2502                                         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502 \u2502 Team: acme-engineering          \u2502    \u2502\n\u2502 \u2502 Model Groups: GPT4Agent         \u2502    \u2502\n\u2502 \u2502 Credits: 1000                   \u2502    \u2502\n\u2502 \u2502 Virtual Key: sk-eng-123...      \u2502    \u2502\n\u2502 \u2502                                 \u2502    \u2502\n\u2502 \u2502 Jobs: 450                       \u2502    \u2502\n\u2502 \u2502   \u251c\u2500 resume_analysis: 300       \u2502    \u2502\n\u2502 \u2502   \u251c\u2500 document_parsing: 100      \u2502    \u2502\n\u2502 \u2502   \u2514\u2500 chat_session: 50           \u2502    \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502 \u2502 Team: acme-sales                \u2502    \u2502\n\u2502 \u2502 Model Groups: GPT35Agent        \u2502    \u2502\n\u2502 \u2502 Credits: 500                    \u2502    \u2502\n\u2502 \u2502 Virtual Key: sk-sales-456...    \u2502    \u2502\n\u2502 \u2502                                 \u2502    \u2502\n\u2502 \u2502 Jobs: 200                       \u2502    \u2502\n\u2502 \u2502   \u251c\u2500 lead_qualification: 150    \u2502    \u2502\n\u2502 \u2502   \u2514\u2500 email_generation: 50       \u2502    \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502 \u2502 Team: acme-marketing            \u2502    \u2502\n\u2502 \u2502 Model Groups: Claude3Agent      \u2502    \u2502\n\u2502 \u2502 Credits: 300                    \u2502    \u2502\n\u2502 \u2502 Virtual Key: sk-mkt-789...      \u2502    \u2502\n\u2502 \u2502                                 \u2502    \u2502\n\u2502 \u2502 Jobs: 100                       \u2502    \u2502\n\u2502 \u2502   \u2514\u2500 content_generation: 100    \u2502    \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api-reference/organizations/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nADMIN_KEY = \"sk-platform-admin-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create organization\nprint(\"Creating organization...\")\norg_response = requests.post(\n    f\"{API_URL}/organizations/create\",\n    headers=headers,\n    json={\n        \"organization_id\": \"org_acme\",\n        \"name\": \"ACME Corporation\",\n        \"metadata\": {\n            \"industry\": \"Technology\",\n            \"size\": \"Enterprise\",\n            \"country\": \"USA\"\n        }\n    }\n)\n\norg = org_response.json()\nprint(f\"\u2713 Created: {org['name']} ({org['organization_id']})\")\n\n# 2. Create teams within the organization\nteams_to_create = [\n    {\n        \"team_id\": \"acme-engineering\",\n        \"team_alias\": \"Engineering Team\",\n        \"model_groups\": [\"GPT4Agent\"],\n        \"credits\": 1000\n    },\n    {\n        \"team_id\": \"acme-sales\",\n        \"team_alias\": \"Sales Team\",\n        \"model_groups\": [\"GPT35Agent\"],\n        \"credits\": 500\n    },\n    {\n        \"team_id\": \"acme-marketing\",\n        \"team_alias\": \"Marketing Team\",\n        \"model_groups\": [\"Claude3Agent\"],\n        \"credits\": 300\n    }\n]\n\nprint(\"\\nCreating teams...\")\nfor team_config in teams_to_create:\n    team_response = requests.post(\n        f\"{API_URL}/teams/create\",\n        headers=headers,\n        json={\n            \"organization_id\": \"org_acme\",\n            \"team_id\": team_config[\"team_id\"],\n            \"team_alias\": team_config[\"team_alias\"],\n            \"model_groups\": team_config[\"model_groups\"],\n            \"credits_allocated\": team_config[\"credits\"]\n        }\n    )\n\n    team = team_response.json()\n    print(f\"\u2713 Created: {team['team_id']} (Credits: {team['credits_allocated']})\")\n\n# 3. List organization teams\nprint(\"\\nListing organization teams...\")\nteams_response = requests.get(\n    f\"{API_URL}/organizations/org_acme/teams\",\n    headers=headers\n)\n\nteams_data = teams_response.json()\nprint(f\"\u2713 Organization has {teams_data['team_count']} teams:\")\nfor team_id in teams_data['teams']:\n    print(f\"  - {team_id}\")\n\n# 4. Get organization details\nprint(\"\\nGetting organization details...\")\norg_details = requests.get(\n    f\"{API_URL}/organizations/org_acme\",\n    headers=headers\n).json()\n\nprint(f\"\u2713 Organization: {org_details['name']}\")\nprint(f\"  Status: {org_details['status']}\")\nprint(f\"  Industry: {org_details['metadata'].get('industry', 'N/A')}\")\nprint(f\"  Created: {org_details['created_at']}\")\n\n# 5. Get organization usage (if data exists)\nprint(\"\\nGetting organization usage for current period...\")\ntry:\n    usage_response = requests.get(\n        f\"{API_URL}/organizations/org_acme/usage\",\n        headers=headers,\n        params={\"period\": \"2025-10\"}\n    )\n\n    if usage_response.status_code == 200:\n        usage = usage_response.json()\n        print(f\"\u2713 Period: {usage['period']}\")\n        print(f\"  Total jobs: {usage['summary']['total_jobs']}\")\n        print(f\"  Credits used: {usage['summary']['credits_used']}\")\n        print(f\"  Total cost: ${usage['summary']['total_cost_usd']:.2f}\")\n    else:\n        print(\"  No usage data yet for this period\")\nexcept:\n    print(\"  No usage data yet\")\n\nprint(\"\\n\u2705 Organization setup complete!\")\n</code></pre> <p>Output:</p> <pre><code>Creating organization...\n\u2713 Created: ACME Corporation (org_acme)\n\nCreating teams...\n\u2713 Created: acme-engineering (Credits: 1000)\n\u2713 Created: acme-sales (Credits: 500)\n\u2713 Created: acme-marketing (Credits: 300)\n\nListing organization teams...\n\u2713 Organization has 3 teams:\n  - acme-engineering\n  - acme-sales\n  - acme-marketing\n\nGetting organization details...\n\u2713 Organization: ACME Corporation\n  Status: active\n  Industry: Technology\n  Created: 2025-10-14T12:00:00.000Z\n\nGetting organization usage for current period...\n\u2713 Period: 2025-10\n  Total jobs: 750\n  Credits used: 710\n  Total cost: $35.25\n\n\u2705 Organization setup complete!\n</code></pre>"},{"location":"api-reference/organizations/#use-cases","title":"Use Cases","text":""},{"location":"api-reference/organizations/#multi-tenant-saas-platform","title":"Multi-Tenant SaaS Platform","text":"<p>Each customer is an organization:</p> <pre><code># Customer A\ncreate_organization(\"org_customer_a\", \"Customer A Inc.\")\ncreate_team(\"org_customer_a\", \"customer-a-prod\", [\"GPT4Agent\"])\ncreate_team(\"org_customer_a\", \"customer-a-dev\", [\"GPT35Agent\"])\n\n# Customer B\ncreate_organization(\"org_customer_b\", \"Customer B LLC\")\ncreate_team(\"org_customer_b\", \"customer-b-prod\", [\"Claude3Agent\"])\ncreate_team(\"org_customer_b\", \"customer-b-dev\", [\"GPT35Agent\"])\n</code></pre>"},{"location":"api-reference/organizations/#departmental-organization","title":"Departmental Organization","text":"<p>Single company, multiple departments:</p> <pre><code>create_organization(\"org_acme\", \"ACME Corporation\")\ncreate_team(\"org_acme\", \"engineering\", [\"GPT4Agent\", \"Claude3Agent\"])\ncreate_team(\"org_acme\", \"sales\", [\"GPT35Agent\"])\ncreate_team(\"org_acme\", \"marketing\", [\"Claude3Agent\"])\ncreate_team(\"org_acme\", \"support\", [\"GPT35Agent\"])\n</code></pre>"},{"location":"api-reference/organizations/#environment-separation","title":"Environment Separation","text":"<p>Separate production and development:</p> <pre><code>create_organization(\"org_acme\", \"ACME Corporation\")\ncreate_team(\"org_acme\", \"production\", [\"GPT4Agent\"])\ncreate_team(\"org_acme\", \"staging\", [\"GPT4Agent\"])\ncreate_team(\"org_acme\", \"development\", [\"GPT35Agent\"])\n</code></pre>"},{"location":"api-reference/organizations/#rate-limiting","title":"Rate Limiting","text":"<p>Organizations API endpoints have rate limits:</p> <ul> <li>GET endpoints: 100 requests per minute</li> <li>POST/PUT endpoints: 30 requests per minute</li> </ul>"},{"location":"api-reference/organizations/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive organization IDs - Use prefixes like \"org_\" for clarity</li> <li>Add comprehensive metadata - Include industry, size, contact info for billing</li> <li>Monitor organization-wide usage - Track usage across all teams</li> <li>Set up team hierarchy early - Create organization before creating teams</li> <li>Use meaningful names - Human-readable names help with reporting</li> <li>Track periods consistently - Use YYYY-MM format for usage queries</li> <li>Implement usage alerts - Monitor organization usage to prevent overages</li> </ol>"},{"location":"api-reference/organizations/#see-also","title":"See Also","text":"<ul> <li>Teams API - Manage teams within organizations</li> <li>Jobs API - Create and manage jobs</li> <li>Credits API - Manage team credits</li> <li>Authentication Guide - API authentication details</li> <li>Getting Started - Initial setup guide</li> </ul>"},{"location":"api-reference/overview/","title":"API Reference Overview","text":"<p>Complete reference for the SaaS LiteLLM API endpoints.</p>"},{"location":"api-reference/overview/#overview","title":"Overview","text":"<p>The SaaS LiteLLM API provides REST endpoints for managing jobs, making LLM calls, managing teams, and tracking usage.</p> <p>Base URL (Local): <code>http://localhost:8003/api</code> Base URL (Production): <code>https://your-domain.com/api</code></p> <p>Authentication: Bearer token (virtual key) in <code>Authorization</code> header</p>"},{"location":"api-reference/overview/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>For complete, interactive API documentation with \"Try it out\" functionality:</p> <ul> <li> <p> ReDoc</p> <p>Beautiful, responsive API documentation</p> <p> http://localhost:8003/redoc</p> </li> <li> <p> Swagger UI</p> <p>Interactive API testing interface</p> <p> http://localhost:8003/docs</p> </li> </ul>"},{"location":"api-reference/overview/#api-categories","title":"API Categories","text":""},{"location":"api-reference/overview/#jobs-api","title":"Jobs API","text":"<p>Manage job lifecycle for cost tracking:</p> Endpoint Method Description <code>/api/jobs/create</code> POST Create a new job <code>/api/jobs/{job_id}</code> GET Get job details <code>/api/jobs/{job_id}/complete</code> POST Complete a job <code>/api/jobs/{job_id}/costs</code> GET Get job cost breakdown <p> Jobs API Details</p>"},{"location":"api-reference/overview/#llm-calls-api","title":"LLM Calls API","text":"<p>Make LLM calls within jobs:</p> Endpoint Method Description <code>/api/jobs/{job_id}/llm-call</code> POST Non-streaming LLM call <code>/api/jobs/{job_id}/llm-call-stream</code> POST Streaming LLM call (SSE) <p> LLM Calls API Details</p>"},{"location":"api-reference/overview/#teams-api","title":"Teams API","text":"<p>Manage teams and access:</p> Endpoint Method Description <code>/api/teams/create</code> POST Create a new team <code>/api/teams/{team_id}</code> GET Get team details <code>/api/teams/{team_id}</code> PUT Update team <code>/api/teams/{team_id}/suspend</code> POST Suspend team <code>/api/teams/{team_id}/resume</code> POST Resume team <code>/api/teams/{team_id}/usage</code> GET Get team usage stats <p> Teams API Details</p>"},{"location":"api-reference/overview/#organizations-api","title":"Organizations API","text":"<p>Manage organizations:</p> Endpoint Method Description <code>/api/organizations/create</code> POST Create organization <code>/api/organizations/{org_id}</code> GET Get organization details <code>/api/organizations/{org_id}/teams</code> GET List organization teams <p> Organizations API Details</p>"},{"location":"api-reference/overview/#credits-api","title":"Credits API","text":"<p>Manage team credits:</p> Endpoint Method Description <code>/api/credits/balance</code> GET Get credit balance <code>/api/credits/add</code> POST Add credits to team <code>/api/credits/transactions</code> GET Get credit transaction history"},{"location":"api-reference/overview/#model-access-groups-api","title":"Model Access Groups API","text":"<p>Control model access per team:</p> Endpoint Method Description <code>/api/model-access-groups/create</code> POST Create access group <code>/api/model-access-groups/{group_name}</code> GET Get access group <code>/api/model-access-groups/{group_name}</code> PUT Update access group"},{"location":"api-reference/overview/#model-aliases-api","title":"Model Aliases API","text":"<p>Configure model aliases:</p> Endpoint Method Description <code>/api/model-aliases/create</code> POST Create model alias <code>/api/model-aliases/{alias_name}</code> GET Get model alias"},{"location":"api-reference/overview/#health-api","title":"Health API","text":"<p>Check system health:</p> Endpoint Method Description <code>/health</code> GET Health check"},{"location":"api-reference/overview/#authentication","title":"Authentication","text":"<p>All endpoints (except <code>/health</code>) require authentication with a virtual key:</p> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"team_id\": \"acme-corp\", \"job_type\": \"test\"}'\n</code></pre> <p> Authentication Guide</p>"},{"location":"api-reference/overview/#common-request-patterns","title":"Common Request Patterns","text":""},{"location":"api-reference/overview/#create-job-make-call-complete","title":"Create Job \u2192 Make Call \u2192 Complete","text":"<pre><code># 1. Create job\njob = POST /api/jobs/create\n  {\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"analysis\"\n  }\n\n# 2. Make LLM call\nresponse = POST /api/jobs/{job_id}/llm-call\n  {\n    \"messages\": [{\"role\": \"user\", \"content\": \"...\"}]\n  }\n\n# 3. Complete job\nresult = POST /api/jobs/{job_id}/complete\n  {\n    \"status\": \"completed\"\n  }\n</code></pre>"},{"location":"api-reference/overview/#check-credits-before-call","title":"Check Credits Before Call","text":"<pre><code># Check balance\nbalance = GET /api/credits/balance?team_id=acme-corp\n\nif balance[\"credits_remaining\"] &gt; 0:\n    # Make call\n    pass\nelse:\n    # Add credits first\n    POST /api/credits/add\n      {\n        \"team_id\": \"acme-corp\",\n        \"amount\": 100\n      }\n</code></pre>"},{"location":"api-reference/overview/#response-formats","title":"Response Formats","text":""},{"location":"api-reference/overview/#success-response-200-ok","title":"Success Response (200 OK)","text":"<pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2024-10-14T12:00:00Z\"\n}\n</code></pre>"},{"location":"api-reference/overview/#error-response-4xx5xx","title":"Error Response (4xx/5xx)","text":"<pre><code>{\n  \"detail\": \"Insufficient credits\"\n}\n</code></pre>"},{"location":"api-reference/overview/#validation-error-422","title":"Validation Error (422)","text":"<pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"messages\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api-reference/overview/#http-status-codes","title":"HTTP Status Codes","text":"Code Meaning Description 200 OK Request successful 201 Created Resource created 400 Bad Request Invalid request format 401 Unauthorized Invalid or missing virtual key 403 Forbidden Insufficient credits or access denied 404 Not Found Resource not found 422 Unprocessable Entity Validation error 429 Too Many Requests Rate limit exceeded 500 Internal Server Error Server error 503 Service Unavailable Service temporarily unavailable <p> Error Handling Guide</p>"},{"location":"api-reference/overview/#rate-limits","title":"Rate Limits","text":"<p>Rate limits are enforced per team:</p> <ul> <li>Requests per minute (RPM): Configurable per team</li> <li>Tokens per minute (TPM): Configurable per team</li> </ul> <p>When rate limited, you'll receive a <code>429 Too Many Requests</code> response. Implement exponential backoff for retries.</p>"},{"location":"api-reference/overview/#pagination","title":"Pagination","text":"<p>Endpoints that return lists support pagination:</p> <p>Query Parameters: - <code>limit</code> (int, default: 100) - Number of items per page - <code>offset</code> (int, default: 0) - Number of items to skip</p> <p>Example: <pre><code>GET /api/teams/acme-corp/usage?limit=50&amp;offset=100\n</code></pre></p>"},{"location":"api-reference/overview/#filtering-and-sorting","title":"Filtering and Sorting","text":""},{"location":"api-reference/overview/#time-based-filtering","title":"Time-based Filtering","text":"<pre><code>GET /api/jobs?start_date=2024-10-01&amp;end_date=2024-10-31\n</code></pre>"},{"location":"api-reference/overview/#sorting","title":"Sorting","text":"<pre><code>GET /api/jobs?sort_by=created_at&amp;order=desc\n</code></pre>"},{"location":"api-reference/overview/#versioning","title":"Versioning","text":"<p>The API uses URL-based versioning:</p> <ul> <li>Current version: v1 (default, no prefix required)</li> <li>Future versions: <code>/api/v2/...</code></li> </ul>"},{"location":"api-reference/overview/#openapi-specification","title":"OpenAPI Specification","text":"<p>Download the OpenAPI 3.0 specification:</p> <pre><code>curl http://localhost:8003/openapi.json &gt; saas-api-spec.json\n</code></pre> <p>Use the spec to: - Generate client libraries - Import into API testing tools (Postman, Insomnia) - Build custom tooling</p>"},{"location":"api-reference/overview/#sdks-and-clients","title":"SDKs and Clients","text":""},{"location":"api-reference/overview/#python-client","title":"Python Client","text":"<p>Type-safe async Python client:</p> <pre><code>from examples.typed_client import SaaSLLMClient\n\nasync with SaaSLLMClient(\n    base_url=\"http://localhost:8003\",\n    team_id=\"acme-corp\",\n    virtual_key=\"sk-your-key\"\n) as client:\n    job_id = await client.create_job(\"test\")\n    # ...\n</code></pre> <p> Typed Client Guide</p>"},{"location":"api-reference/overview/#other-languages","title":"Other Languages","text":"<p>Currently, we provide an official Python client. For other languages:</p> <ol> <li>Use the OpenAPI spec to generate clients</li> <li>Use standard HTTP libraries</li> <li>See example code in the integration guides</li> </ol>"},{"location":"api-reference/overview/#webhooks","title":"Webhooks","text":"<p>Register webhooks to receive notifications:</p> <pre><code>POST /api/webhooks/register\n{\n  \"team_id\": \"acme-corp\",\n  \"webhook_url\": \"https://your-app.com/webhooks/job-complete\",\n  \"events\": [\"job.completed\", \"job.failed\"]\n}\n</code></pre> <p>Webhook Payload: <pre><code>{\n  \"event\": \"job.completed\",\n  \"job_id\": \"job_789abc\",\n  \"team_id\": \"acme-corp\",\n  \"timestamp\": \"2024-10-14T12:00:00Z\",\n  \"data\": {\n    \"total_calls\": 5,\n    \"duration_seconds\": 45\n  }\n}\n</code></pre></p>"},{"location":"api-reference/overview/#idempotency","title":"Idempotency","text":"<p>POST requests support idempotency keys to prevent duplicate operations:</p> <pre><code>POST /api/jobs/create\n  -H \"Idempotency-Key: unique-key-123\"\n  -d '{\"team_id\": \"acme-corp\", \"job_type\": \"test\"}'\n</code></pre> <p>If you retry with the same idempotency key within 24 hours, you'll receive the same response.</p>"},{"location":"api-reference/overview/#cors","title":"CORS","text":"<p>CORS is enabled for web applications. Allowed origins can be configured in the server settings.</p>"},{"location":"api-reference/overview/#testing","title":"Testing","text":""},{"location":"api-reference/overview/#test-endpoints-in-browser","title":"Test Endpoints in Browser","text":"<p>Use Swagger UI for interactive testing:</p> <p> Open Swagger UI</p>"},{"location":"api-reference/overview/#test-with-curl","title":"Test with cURL","text":"<pre><code># Create job\ncurl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"team_id\": \"acme-corp\", \"job_type\": \"test\"}'\n\n# Make LLM call\ncurl -X POST http://localhost:8003/api/jobs/{job_id}/llm-call \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'\n</code></pre>"},{"location":"api-reference/overview/#test-with-python","title":"Test with Python","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n)\n\nprint(response.json())\n</code></pre>"},{"location":"api-reference/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Always use HTTPS in production</li> <li>Implement exponential backoff for retries</li> <li>Set reasonable timeouts (30-60 seconds)</li> <li>Handle all error codes appropriately</li> <li>Monitor rate limits and credit usage</li> <li>Use idempotency keys for critical operations</li> <li>Validate request data before sending</li> <li>Log requests for debugging</li> </ol> <p> Complete Best Practices Guide</p>"},{"location":"api-reference/overview/#detailed-api-documentation","title":"Detailed API Documentation","text":"<p>For detailed documentation on specific API categories:</p> <ul> <li> <p>Jobs API</p> <p>Create and manage jobs for cost tracking</p> </li> <li> <p>LLM Calls API</p> <p>Make streaming and non-streaming LLM calls</p> </li> <li> <p>Teams API</p> <p>Manage teams and access controls</p> </li> <li> <p>Organizations API</p> <p>Manage organizations and hierarchies</p> </li> </ul>"},{"location":"api-reference/overview/#getting-help","title":"Getting Help","text":"<p>If you encounter issues with the API:</p> <ol> <li>Check the interactive docs - http://localhost:8003/docs</li> <li>Review error handling guide - Error Handling</li> <li>See examples - Basic Usage</li> <li>Check troubleshooting - Troubleshooting Guide</li> </ol>"},{"location":"api-reference/overview/#next-steps","title":"Next Steps","text":"<ol> <li>Try the Interactive Docs - Test endpoints in your browser</li> <li>Read Integration Guide - Learn integration patterns</li> <li>See Examples - Working code examples</li> <li>Review Jobs API - Detailed job endpoint documentation</li> </ol>"},{"location":"api-reference/redoc-link/","title":"Full API Documentation","text":"<p>For complete, interactive API documentation, use the auto-generated API docs provided by the SaaS API.</p>"},{"location":"api-reference/redoc-link/#interactive-api-documentation","title":"Interactive API Documentation","text":""},{"location":"api-reference/redoc-link/#redoc-recommended","title":"ReDoc (Recommended)","text":"<p>Beautiful, responsive API documentation with examples</p> <p> Open ReDoc Documentation (Production)</p> <p> Open ReDoc Documentation (Local)</p> <p>ReDoc provides: - \u2705 Clean, professional interface - \u2705 Comprehensive endpoint documentation - \u2705 Request/response schemas - \u2705 Example payloads - \u2705 Model definitions - \u2705 Authentication details - \u2705 Mobile-friendly</p> <p>URL: <code>http://localhost:8003/redoc</code></p>"},{"location":"api-reference/redoc-link/#swagger-ui","title":"Swagger UI","text":"<p>Interactive API explorer with \"Try it out\" functionality</p> <p> Open Swagger UI (Production)</p> <p> Open Swagger UI (Local)</p> <p>Swagger UI provides: - \u2705 Try API calls directly in the browser - \u2705 Interactive request builder - \u2705 Real-time response preview - \u2705 Schema validation - \u2705 Authentication testing - \u2705 Download OpenAPI spec</p> <p>URL: <code>http://localhost:8003/docs</code></p>"},{"location":"api-reference/redoc-link/#whats-available","title":"What's Available","text":"<p>Both interfaces provide complete documentation for:</p>"},{"location":"api-reference/redoc-link/#organizations-api","title":"Organizations API","text":"<ul> <li>Create, read, update, delete organizations</li> <li>List organization teams</li> <li>Get organization usage statistics</li> </ul>"},{"location":"api-reference/redoc-link/#teams-api","title":"Teams API","text":"<ul> <li>Create, read, update, delete teams</li> <li>Manage model access groups</li> <li>Suspend/resume teams</li> <li>View team credits</li> </ul>"},{"location":"api-reference/redoc-link/#model-access-groups-api","title":"Model Access Groups API","text":"<ul> <li>Create and manage access groups</li> <li>Assign model aliases</li> <li>Control team access</li> </ul>"},{"location":"api-reference/redoc-link/#model-aliases-api","title":"Model Aliases API","text":"<ul> <li>Create model aliases</li> <li>Configure pricing (input/output tokens)</li> <li>Provider configuration</li> </ul>"},{"location":"api-reference/redoc-link/#jobs-api","title":"Jobs API","text":"<ul> <li>Create jobs</li> <li>Single-call job endpoints (streaming and non-streaming)</li> <li><code>POST /api/jobs/create-and-call</code> - Non-streaming single-call</li> <li><code>POST /api/jobs/create-and-call-stream</code> - Streaming single-call (SSE)</li> <li>Make LLM calls within jobs</li> <li>Complete jobs</li> <li>Get job details and costs</li> </ul>"},{"location":"api-reference/redoc-link/#credits-api","title":"Credits API","text":"<ul> <li>Check credit balance</li> <li>Add credits</li> <li>View credit transactions</li> <li>Credit availability check</li> </ul>"},{"location":"api-reference/redoc-link/#health-status","title":"Health &amp; Status","text":"<ul> <li>Health check endpoint</li> <li>System status</li> </ul>"},{"location":"api-reference/redoc-link/#quick-access","title":"Quick Access","text":"<ul> <li> <p> ReDoc</p> <p>Beautiful, responsive documentation</p> <p> Production |  Local</p> </li> <li> <p> Swagger UI</p> <p>Interactive API testing interface</p> <p> Production |  Local</p> </li> <li> <p> OpenAPI Spec</p> <p>Download OpenAPI 3.0 specification</p> <p> Production |  Local</p> </li> </ul>"},{"location":"api-reference/redoc-link/#using-the-interactive-docs","title":"Using the Interactive Docs","text":""},{"location":"api-reference/redoc-link/#redoc-interface","title":"ReDoc Interface","text":"<ol> <li>Browse endpoints - Navigate through the API structure</li> <li>View schemas - See request/response models</li> <li>Copy examples - Use example code in your app</li> <li>Search - Find specific endpoints quickly</li> </ol>"},{"location":"api-reference/redoc-link/#swagger-ui-interface","title":"Swagger UI Interface","text":"<ol> <li>Select an endpoint - Click on any API endpoint</li> <li>Click \"Try it out\" - Enable interactive mode</li> <li>Fill parameters - Add your virtual key and request data</li> <li>Execute - Send the request and see live results</li> </ol> <p>Authentication in Swagger UI</p> <p>To test authenticated endpoints in Swagger UI:</p> <ol> <li>Click the \"Authorize\" button at the top</li> <li>Enter your virtual key: <code>Bearer sk-your-virtual-key</code></li> <li>Click \"Authorize\"</li> <li>Now all requests will include authentication</li> </ol>"},{"location":"api-reference/redoc-link/#openapi-specification","title":"OpenAPI Specification","text":"<p>Download the OpenAPI 3.0 specification to: - Generate client libraries - Import into API testing tools (Postman, Insomnia) - Build custom tooling - Integrate with CI/CD</p> <pre><code># Download OpenAPI spec\ncurl http://localhost:8003/openapi.json &gt; saas-api-spec.json\n</code></pre>"},{"location":"api-reference/redoc-link/#production-urls","title":"Production URLs","text":"<p>When deploying to production, the API documentation will be available at:</p> Environment ReDoc Swagger UI OpenAPI Spec Production https://llm-saas.usegittie.com/redoc https://llm-saas.usegittie.com/docs https://llm-saas.usegittie.com/openapi.json Local http://localhost:8003/redoc http://localhost:8003/docs http://localhost:8003/openapi.json"},{"location":"api-reference/redoc-link/#additional-api-resources","title":"Additional API Resources","text":"<p>Beyond the interactive documentation, explore these resources:</p>"},{"location":"api-reference/redoc-link/#detailed-guides","title":"Detailed Guides","text":"<ul> <li>API Overview - Introduction to the API structure</li> <li>Jobs API - Job management endpoints</li> <li>LLM Calls API - Making LLM calls</li> <li>Teams API - Team management</li> <li>Organizations API - Organization management</li> </ul>"},{"location":"api-reference/redoc-link/#integration-guides","title":"Integration Guides","text":"<ul> <li>Integration Overview - How to integrate</li> <li>Authentication - Virtual key auth</li> <li>Job Workflow - Job-based pattern</li> <li>Streaming - SSE streaming guide</li> </ul>"},{"location":"api-reference/redoc-link/#examples","title":"Examples","text":"<ul> <li>Basic Usage - Simple examples</li> <li>Streaming Examples - Real-time streaming</li> <li>Full Chain - Complete workflow</li> </ul>"},{"location":"api-reference/redoc-link/#getting-help","title":"Getting Help","text":"<p>If you have questions about the API:</p> <ol> <li>Check the interactive docs - Most questions are answered there</li> <li>Review the integration guides - Detailed explanations and examples</li> <li>Try the examples - Working code you can run locally</li> </ol>"},{"location":"api-reference/redoc-link/#screenshots","title":"Screenshots","text":""},{"location":"api-reference/redoc-link/#redoc-interface_1","title":"ReDoc Interface","text":"<p>Clean, professional API documentation with ReDoc</p>"},{"location":"api-reference/redoc-link/#swagger-ui-interface_1","title":"Swagger UI Interface","text":"<p>Interactive API testing with Swagger UI</p> <p>Ready to Explore</p> <p>Open the interactive documentation and start exploring the API:</p> <ul> <li>ReDoc - Beautiful documentation</li> <li>Swagger UI - Interactive testing</li> </ul>"},{"location":"api-reference/teams/","title":"Teams API","text":"<p>The Teams API provides endpoints for managing teams, model access, and credit allocations in the SaaS LiteLLM platform. Teams are the primary organizational unit for access control and billing.</p>"},{"location":"api-reference/teams/#overview","title":"Overview","text":"<p>Teams enable you to:</p> <ul> <li>Organize users - Group users by team for access control</li> <li>Control model access - Assign model groups to teams</li> <li>Manage credits - Allocate and track credit usage per team</li> <li>Generate virtual keys - Each team gets a virtual API key</li> <li>Track usage - Monitor team usage and costs</li> </ul> <p>Base URL: <code>/api/teams</code></p> <p>Authentication: All endpoints require a Bearer token (virtual API key) in the <code>Authorization</code> header.</p>"},{"location":"api-reference/teams/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/teams/#create-team","title":"Create Team","text":"<p>Create a new team with model groups, credits, and LiteLLM integration.</p> <p>Endpoint: <code>POST /api/teams/create</code></p> <p>Authentication: Required (admin/platform key)</p> <p>Request Body:</p> <pre><code>{\n  \"organization_id\": \"org_acme\",\n  \"team_id\": \"acme-engineering\",\n  \"team_alias\": \"ACME Engineering Team\",\n  \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\"],\n  \"credits_allocated\": 1000,\n  \"metadata\": {\n    \"department\": \"Engineering\",\n    \"cost_center\": \"CC-1001\"\n  }\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>organization_id</code> string Yes Parent organization ID <code>team_id</code> string Yes Unique team identifier (e.g., \"acme-engineering\") <code>team_alias</code> string No Human-readable team name <code>model_groups</code> array Yes List of model group names the team can access <code>credits_allocated</code> integer No Initial credits to allocate (default: 0) <code>metadata</code> object No Custom metadata for the team <p>Response (200 OK):</p> <pre><code>{\n  \"team_id\": \"acme-engineering\",\n  \"organization_id\": \"org_acme\",\n  \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\"],\n  \"allowed_models\": [\n    \"gpt-4-turbo\",\n    \"gpt-4\",\n    \"claude-3-opus-20240229\",\n    \"claude-3-sonnet-20240229\"\n  ],\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 1000,\n  \"virtual_key\": \"sk-1234567890abcdef\",\n  \"message\": \"Team created successfully with LiteLLM integration\"\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>team_id</code> string Team identifier <code>organization_id</code> string Parent organization ID <code>model_groups</code> array Model groups assigned to team <code>allowed_models</code> array Actual model names (resolved from model groups) <code>credits_allocated</code> integer Total credits allocated <code>credits_remaining</code> integer Credits remaining (initially same as allocated) <code>virtual_key</code> string Virtual API key for this team <code>message</code> string Success message <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Authorization: Bearer sk-platform-admin-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-engineering\",\n    \"team_alias\": \"ACME Engineering Team\",\n    \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\"],\n    \"credits_allocated\": 1000,\n    \"metadata\": {\n      \"department\": \"Engineering\",\n      \"cost_center\": \"CC-1001\"\n    }\n  }'\n</code></pre> <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nADMIN_KEY = \"sk-platform-admin-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/teams/create\",\n    headers=headers,\n    json={\n        \"organization_id\": \"org_acme\",\n        \"team_id\": \"acme-engineering\",\n        \"team_alias\": \"ACME Engineering Team\",\n        \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\"],\n        \"credits_allocated\": 1000,\n        \"metadata\": {\n            \"department\": \"Engineering\",\n            \"cost_center\": \"CC-1001\"\n        }\n    }\n)\n\nteam = response.json()\nprint(f\"Team created: {team['team_id']}\")\nprint(f\"Virtual key: {team['virtual_key']}\")\nprint(f\"Allowed models: {', '.join(team['allowed_models'])}\")\nprint(f\"Credits: {team['credits_remaining']}\")\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst ADMIN_KEY = \"sk-platform-admin-key\";\n\nconst response = await fetch(`${API_URL}/teams/create`, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${ADMIN_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    organization_id: 'org_acme',\n    team_id: 'acme-engineering',\n    team_alias: 'ACME Engineering Team',\n    model_groups: ['GPT4Agent', 'Claude3Agent'],\n    credits_allocated: 1000,\n    metadata: {\n      department: 'Engineering',\n      cost_center: 'CC-1001'\n    }\n  })\n});\n\nconst team = await response.json();\nconsole.log(`Team created: ${team.team_id}`);\nconsole.log(`Virtual key: ${team.virtual_key}`);\nconsole.log(`Allowed models: ${team.allowed_models.join(', ')}`);\nconsole.log(`Credits: ${team.credits_remaining}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 400 Bad Request Team already exists or invalid data 404 Not Found Organization or model group not found 422 Validation Error Invalid request data 500 Internal Server Error LiteLLM integration failed or server error <p>Example Error Response:</p> <pre><code>{\n  \"detail\": \"Team 'acme-engineering' already exists\"\n}\n</code></pre>"},{"location":"api-reference/teams/#get-team","title":"Get Team","text":"<p>Retrieve details about a specific team.</p> <p>Endpoint: <code>GET /api/teams/{team_id}</code></p> <p>Authentication: Required (team's virtual key or admin key)</p> <p>Path Parameters:</p> Parameter Type Description <code>team_id</code> string The team identifier <p>Response (200 OK):</p> <pre><code>{\n  \"team_id\": \"acme-engineering\",\n  \"organization_id\": \"org_acme\",\n  \"credits\": {\n    \"team_id\": \"acme-engineering\",\n    \"organization_id\": \"org_acme\",\n    \"credits_allocated\": 1000,\n    \"credits_used\": 150,\n    \"credits_remaining\": 850,\n    \"virtual_key\": \"sk-1234567890abcdef\"\n  },\n  \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\"]\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>team_id</code> string Team identifier <code>organization_id</code> string Parent organization ID <code>credits</code> object Credit balance information <code>credits.credits_allocated</code> integer Total credits allocated <code>credits.credits_used</code> integer Credits consumed <code>credits.credits_remaining</code> integer Credits remaining <code>credits.virtual_key</code> string Team's virtual API key <code>model_groups</code> array Model groups assigned to team <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET http://localhost:8003/api/teams/acme-engineering \\\n  -H \"Authorization: Bearer sk-1234567890abcdef\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/teams/acme-engineering\",\n    headers={\"Authorization\": f\"Bearer {team_virtual_key}\"}\n)\n\nteam = response.json()\nprint(f\"Team: {team['team_id']}\")\nprint(f\"Organization: {team['organization_id']}\")\nprint(f\"Credits remaining: {team['credits']['credits_remaining']}\")\nprint(f\"Model groups: {', '.join(team['model_groups'])}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/teams/acme-engineering`, {\n  headers: {\n    'Authorization': `Bearer ${teamVirtualKey}`\n  }\n});\n\nconst team = await response.json();\nconsole.log(`Team: ${team.team_id}`);\nconsole.log(`Organization: ${team.organization_id}`);\nconsole.log(`Credits remaining: ${team.credits.credits_remaining}`);\nconsole.log(`Model groups: ${team.model_groups.join(', ')}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 404 Not Found Team not found"},{"location":"api-reference/teams/#update-team-model-groups","title":"Update Team Model Groups","text":"<p>Assign or update model groups for a team (replaces existing assignments).</p> <p>Endpoint: <code>PUT /api/teams/{team_id}/model-groups</code></p> <p>Authentication: Required (admin/platform key)</p> <p>Path Parameters:</p> Parameter Type Description <code>team_id</code> string The team identifier <p>Request Body:</p> <pre><code>{\n  \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\", \"GPT35Agent\"]\n}\n</code></pre> <p>Request Fields:</p> Field Type Required Description <code>model_groups</code> array Yes List of model group names to assign (replaces all existing) <p>Response (200 OK):</p> <pre><code>{\n  \"team_id\": \"acme-engineering\",\n  \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\", \"GPT35Agent\"],\n  \"message\": \"Model groups assigned successfully\"\n}\n</code></pre> <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X PUT http://localhost:8003/api/teams/acme-engineering/model-groups \\\n  -H \"Authorization: Bearer sk-platform-admin-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\", \"GPT35Agent\"]\n  }'\n</code></pre> <pre><code>response = requests.put(\n    f\"{API_URL}/teams/acme-engineering/model-groups\",\n    headers={\n        \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model_groups\": [\"GPT4Agent\", \"Claude3Agent\", \"GPT35Agent\"]\n    }\n)\n\nresult = response.json()\nprint(f\"Updated model groups for {result['team_id']}\")\nprint(f\"New groups: {', '.join(result['model_groups'])}\")\n</code></pre> <pre><code>const response = await fetch(`${API_URL}/teams/acme-engineering/model-groups`, {\n  method: 'PUT',\n  headers: {\n    'Authorization': `Bearer ${ADMIN_KEY}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model_groups: ['GPT4Agent', 'Claude3Agent', 'GPT35Agent']\n  })\n});\n\nconst result = await response.json();\nconsole.log(`Updated model groups for ${result.team_id}`);\nconsole.log(`New groups: ${result.model_groups.join(', ')}`);\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing admin key 404 Not Found Team or model group not found 422 Validation Error Invalid model group names"},{"location":"api-reference/teams/#get-team-usage","title":"Get Team Usage","text":"<p>Get usage statistics for a team over a specified period.</p> <p>Endpoint: <code>GET /api/teams/{team_id}/usage</code></p> <p>Authentication: Required (team's virtual key or admin key)</p> <p>Path Parameters:</p> Parameter Type Description <code>team_id</code> string The team identifier <p>Query Parameters:</p> Parameter Type Required Description <code>period</code> string Yes Period in format \"YYYY-MM\" (e.g., \"2025-10\") <p>Response (200 OK):</p> <pre><code>{\n  \"team_id\": \"acme-engineering\",\n  \"period\": \"2025-10\",\n  \"summary\": {\n    \"total_jobs\": 250,\n    \"successful_jobs\": 235,\n    \"failed_jobs\": 15,\n    \"total_cost_usd\": 12.45,\n    \"total_tokens\": 125000,\n    \"avg_cost_per_job\": 0.0498\n  },\n  \"job_types\": {\n    \"resume_analysis\": {\n      \"count\": 150,\n      \"cost_usd\": 7.5\n    },\n    \"document_parsing\": {\n      \"count\": 80,\n      \"cost_usd\": 4.0\n    },\n    \"chat_session\": {\n      \"count\": 20,\n      \"cost_usd\": 0.95\n    }\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>team_id</code> string Team identifier <code>period</code> string Requested period <code>summary.total_jobs</code> integer Total number of jobs <code>summary.successful_jobs</code> integer Successfully completed jobs <code>summary.failed_jobs</code> integer Failed jobs <code>summary.total_cost_usd</code> number Total cost in USD (internal tracking) <code>summary.total_tokens</code> integer Total tokens used <code>summary.avg_cost_per_job</code> number Average cost per job <code>job_types</code> object Breakdown by job type <p>Example Request:</p> cURLPythonJavaScript <pre><code>curl -X GET \"http://localhost:8003/api/teams/acme-engineering/usage?period=2025-10\" \\\n  -H \"Authorization: Bearer sk-1234567890abcdef\"\n</code></pre> <pre><code>response = requests.get(\n    f\"{API_URL}/teams/acme-engineering/usage\",\n    headers={\"Authorization\": f\"Bearer {team_virtual_key}\"},\n    params={\"period\": \"2025-10\"}\n)\n\nusage = response.json()\nprint(f\"Team: {usage['team_id']}\")\nprint(f\"Period: {usage['period']}\")\nprint(f\"Total jobs: {usage['summary']['total_jobs']}\")\nprint(f\"Success rate: {usage['summary']['successful_jobs'] / usage['summary']['total_jobs'] * 100:.1f}%\")\nprint(f\"Total cost: ${usage['summary']['total_cost_usd']:.2f}\")\nprint(f\"Total tokens: {usage['summary']['total_tokens']:,}\")\nprint(\"\\nJob type breakdown:\")\nfor job_type, stats in usage['job_types'].items():\n    print(f\"  {job_type}: {stats['count']} jobs (${stats['cost_usd']:.2f})\")\n</code></pre> <pre><code>const response = await fetch(\n  `${API_URL}/teams/acme-engineering/usage?period=2025-10`,\n  {\n    headers: {\n      'Authorization': `Bearer ${teamVirtualKey}`\n    }\n  }\n);\n\nconst usage = await response.json();\nconsole.log(`Team: ${usage.team_id}`);\nconsole.log(`Period: ${usage.period}`);\nconsole.log(`Total jobs: ${usage.summary.total_jobs}`);\nconsole.log(`Success rate: ${(usage.summary.successful_jobs / usage.summary.total_jobs * 100).toFixed(1)}%`);\nconsole.log(`Total cost: $${usage.summary.total_cost_usd.toFixed(2)}`);\nconsole.log(`Total tokens: ${usage.summary.total_tokens.toLocaleString()}`);\nconsole.log('\\nJob type breakdown:');\nfor (const [jobType, stats] of Object.entries(usage.job_types)) {\n  console.log(`  ${jobType}: ${stats.count} jobs ($${stats.cost_usd.toFixed(2)})`);\n}\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Cannot access usage data for a different team 404 Not Found Team not found 422 Validation Error Invalid period format"},{"location":"api-reference/teams/#list-team-jobs","title":"List Team Jobs","text":"<p>List recent jobs for a team with optional filtering.</p> <p>Endpoint: <code>GET /api/teams/{team_id}/jobs</code></p> <p>Authentication: Required (team's virtual key)</p> <p>Path Parameters:</p> Parameter Type Description <code>team_id</code> string The team identifier <p>Query Parameters:</p> Parameter Type Required Default Description <code>limit</code> integer No 100 Maximum number of jobs to return <code>offset</code> integer No 0 Number of jobs to skip <code>status</code> string No None Filter by status (pending, in_progress, completed, failed) <p>Response (200 OK):</p> <pre><code>{\n  \"team_id\": \"acme-engineering\",\n  \"total\": 25,\n  \"jobs\": [\n    {\n      \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"job_type\": \"resume_analysis\",\n      \"status\": \"completed\",\n      \"created_at\": \"2025-10-14T12:00:00.000Z\",\n      \"completed_at\": \"2025-10-14T12:05:23.000Z\",\n      \"credit_applied\": true\n    },\n    {\n      \"job_id\": \"660e9511-f30c-52e5-b827-557766551111\",\n      \"job_type\": \"document_parsing\",\n      \"status\": \"in_progress\",\n      \"created_at\": \"2025-10-14T13:00:00.000Z\",\n      \"completed_at\": null,\n      \"credit_applied\": false\n    }\n  ]\n}\n</code></pre> <p>Example Request:</p> cURLPython <pre><code># Get all jobs\ncurl -X GET \"http://localhost:8003/api/teams/acme-engineering/jobs\" \\\n  -H \"Authorization: Bearer sk-1234567890abcdef\"\n\n# Get only completed jobs\ncurl -X GET \"http://localhost:8003/api/teams/acme-engineering/jobs?status=completed\" \\\n  -H \"Authorization: Bearer sk-1234567890abcdef\"\n\n# Pagination\ncurl -X GET \"http://localhost:8003/api/teams/acme-engineering/jobs?limit=50&amp;offset=100\" \\\n  -H \"Authorization: Bearer sk-1234567890abcdef\"\n</code></pre> <pre><code># Get all jobs\nresponse = requests.get(\n    f\"{API_URL}/teams/acme-engineering/jobs\",\n    headers={\"Authorization\": f\"Bearer {team_virtual_key}\"}\n)\n\njobs = response.json()\nprint(f\"Team: {jobs['team_id']}\")\nprint(f\"Total jobs: {jobs['total']}\")\nfor job in jobs['jobs']:\n    print(f\"  {job['job_id']}: {job['job_type']} - {job['status']}\")\n\n# Get only completed jobs\nresponse = requests.get(\n    f\"{API_URL}/teams/acme-engineering/jobs\",\n    headers={\"Authorization\": f\"Bearer {team_virtual_key}\"},\n    params={\"status\": \"completed\", \"limit\": 50}\n)\n</code></pre> <p>Error Responses:</p> Status Code Error Description 401 Unauthorized Invalid or missing virtual key 403 Forbidden Cannot access jobs for a different team 404 Not Found Team not found"},{"location":"api-reference/teams/#team-lifecycle","title":"Team Lifecycle","text":""},{"location":"api-reference/teams/#creating-a-team","title":"Creating a Team","text":"<p>When you create a team:</p> <ol> <li>Team record is created in the database</li> <li>Team is registered with LiteLLM proxy</li> <li>Virtual API key is generated</li> <li>Model groups are assigned</li> <li>Credits are allocated (if specified)</li> <li>LiteLLM budget is configured based on credits</li> </ol>"},{"location":"api-reference/teams/#managing-team-access","title":"Managing Team Access","text":"<p>Teams control access through:</p> <ul> <li>Model Groups - Which models the team can use</li> <li>Credits - How many jobs the team can run</li> <li>Virtual Key - API authentication for the team</li> </ul>"},{"location":"api-reference/teams/#team-credits","title":"Team Credits","text":"<p>Credits are deducted when: - A job completes successfully (status: \"completed\") - All LLM calls in the job succeeded - Credit hasn't already been applied</p> <p>1 Job = 1 Credit regardless of: - Number of LLM calls (1 or 100) - Models used - Actual USD cost - Time duration</p> <p>See Credits API for credit management.</p>"},{"location":"api-reference/teams/#complete-example","title":"Complete Example","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nADMIN_KEY = \"sk-platform-admin-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create organization (if not exists)\norg_response = requests.post(\n    f\"{API_URL}/organizations/create\",\n    headers=headers,\n    json={\n        \"organization_id\": \"org_acme\",\n        \"name\": \"ACME Corporation\",\n        \"metadata\": {\"industry\": \"Technology\"}\n    }\n)\nprint(f\"Organization: {org_response.json()['name']}\")\n\n# 2. Create team\nteam_response = requests.post(\n    f\"{API_URL}/teams/create\",\n    headers=headers,\n    json={\n        \"organization_id\": \"org_acme\",\n        \"team_id\": \"acme-engineering\",\n        \"team_alias\": \"ACME Engineering Team\",\n        \"model_groups\": [\"GPT4Agent\"],\n        \"credits_allocated\": 1000,\n        \"metadata\": {\n            \"department\": \"Engineering\"\n        }\n    }\n)\n\nteam = team_response.json()\nprint(f\"\\nTeam created: {team['team_id']}\")\nprint(f\"Virtual key: {team['virtual_key']}\")\nprint(f\"Allowed models: {', '.join(team['allowed_models'])}\")\nprint(f\"Credits: {team['credits_remaining']}\")\n\n# 3. Use team virtual key to create jobs\nteam_key = team['virtual_key']\nteam_headers = {\n    \"Authorization\": f\"Bearer {team_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Create a job\njob_response = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=team_headers,\n    json={\n        \"team_id\": \"acme-engineering\",\n        \"job_type\": \"test\"\n    }\n)\n\nprint(f\"\\nJob created: {job_response.json()['job_id']}\")\n\n# 4. Get team details\nteam_details = requests.get(\n    f\"{API_URL}/teams/acme-engineering\",\n    headers=team_headers\n).json()\n\nprint(f\"\\nTeam details:\")\nprint(f\"  Organization: {team_details['organization_id']}\")\nprint(f\"  Credits remaining: {team_details['credits']['credits_remaining']}\")\nprint(f\"  Model groups: {', '.join(team_details['model_groups'])}\")\n</code></pre>"},{"location":"api-reference/teams/#rate-limiting","title":"Rate Limiting","text":"<p>Teams API endpoints have rate limits:</p> <ul> <li>GET endpoints: 100 requests per minute</li> <li>POST/PUT endpoints: 30 requests per minute</li> </ul>"},{"location":"api-reference/teams/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive team IDs - Use meaningful identifiers like \"acme-engineering\"</li> <li>Store virtual keys securely - Treat virtual keys like passwords</li> <li>Monitor credit usage - Check team usage regularly</li> <li>Assign appropriate model groups - Give teams only the models they need</li> <li>Use team metadata - Add context for billing and analytics</li> <li>Implement credit alerts - Notify teams when credits are low</li> </ol>"},{"location":"api-reference/teams/#see-also","title":"See Also","text":"<ul> <li>Organizations API - Manage parent organizations</li> <li>Credits API - Manage team credits</li> <li>Model Groups - Configure model groups</li> <li>Authentication Guide - Using virtual keys</li> <li>Jobs API - Create and manage jobs</li> </ul>"},{"location":"deployment/cors-and-authentication/","title":"CORS and Authentication","text":"<p>Understanding the critical difference between CORS and authentication in SaaS LiteLLM.</p> <p>Deploying on Railway?</p> <p>For Railway-specific CORS setup using dynamic service references (no hardcoded URLs), see the Railway CORS Setup Guide.</p> <p>This guide covers the general concepts. The Railway guide shows you how to use <code>${{service.RAILWAY_PUBLIC_DOMAIN}}</code> for automatic configuration.</p>"},{"location":"deployment/cors-and-authentication/#tldr-the-key-insight","title":"TL;DR - The Key Insight","text":"<p>CORS is a browser-only security feature. Server-side API clients completely ignore CORS.</p> <ul> <li>\u2705 Team API clients (Python, Node.js, curl, Go) - NOT affected by CORS</li> <li>\u26a0\ufe0f Browser-based admin panel (Next.js) - Subject to CORS restrictions</li> </ul> <p>Authentication (Bearer tokens) secures the API regardless of CORS.</p>"},{"location":"deployment/cors-and-authentication/#what-is-cors","title":"What is CORS?","text":"<p>CORS (Cross-Origin Resource Sharing) is a browser security mechanism that restricts which domains can make requests from JavaScript running in a web browser.</p>"},{"location":"deployment/cors-and-authentication/#cors-only-affects-browsers","title":"CORS Only Affects Browsers","text":"<p>When JavaScript in a web browser tries to make a request to a different domain, the browser enforces CORS:</p> <pre><code>// This code running in a browser at https://example.com\n// trying to call https://api.yourcompany.com\n\nfetch('https://api.yourcompany.com/api/jobs/create', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-...',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({...})\n})\n// \u26a0\ufe0f Browser checks CORS before sending this request\n</code></pre> <p>The browser will: 1. Send a \"preflight\" OPTIONS request to check if the domain is allowed 2. Block the request if the server doesn't allow the origin 3. Only send the actual request if CORS headers permit it</p>"},{"location":"deployment/cors-and-authentication/#cors-does-not-affect-server-side-clients","title":"CORS Does NOT Affect Server-Side Clients","text":"<p>Server-side HTTP clients completely ignore CORS:</p> <pre><code># Python requests - CORS does NOT apply\nimport requests\n\nresponse = requests.post(\n    'https://api.yourcompany.com/api/jobs/create',\n    headers={'Authorization': 'Bearer sk-...'},\n    json={...}\n)\n# \u2705 This works regardless of CORS configuration\n</code></pre> <pre><code># curl - CORS does NOT apply\ncurl -X POST https://api.yourcompany.com/api/jobs/create \\\n  -H \"Authorization: Bearer sk-...\" \\\n  -d '{...}'\n# \u2705 This works regardless of CORS configuration\n</code></pre> <pre><code>// Node.js (server-side) - CORS does NOT apply\nconst axios = require('axios');\n\nconst response = await axios.post(\n  'https://api.yourcompany.com/api/jobs/create',\n  {...},\n  {headers: {Authorization: 'Bearer sk-...'}}\n);\n// \u2705 This works regardless of CORS configuration\n</code></pre>"},{"location":"deployment/cors-and-authentication/#saas-litellm-architecture","title":"SaaS LiteLLM Architecture","text":""},{"location":"deployment/cors-and-authentication/#two-types-of-clients","title":"Two Types of Clients","text":"Client Type Uses CORS? Authentication Example Team API Clients \u274c No (server-side) Bearer token (virtual key) Python app, Node.js service, cron job Admin Panel \u2705 Yes (browser-based) MASTER_KEY (X-Admin-Key header) Next.js admin dashboard"},{"location":"deployment/cors-and-authentication/#authentication-flow","title":"Authentication Flow","text":"<pre><code>graph TD\n    A[Team Client: Python/Node.js/curl] --&gt;|Bearer token| B[SaaS API]\n    C[Admin Panel: Browser] --&gt;|MASTER_KEY| D{CORS Check}\n    D --&gt;|Origin allowed| B\n    D --&gt;|Origin blocked| E[CORS Error]\n\n    B --&gt;|Validates Bearer token| F[Team Authenticated]\n    B --&gt;|Validates MASTER_KEY| G[Admin Authenticated]\n\n    style A fill:#4CAF50\n    style C fill:#9C27B0\n    style D fill:#FF9800\n    style E fill:#F44336</code></pre>"},{"location":"deployment/cors-and-authentication/#current-cors-configuration","title":"Current CORS Configuration","text":""},{"location":"deployment/cors-and-authentication/#in-srcsaas_apipy","title":"In <code>src/saas_api.py</code>","text":"<pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        # Local development\n        \"http://localhost:3000\",  # Admin panel dev\n        \"http://localhost:3002\",  # Admin panel alternate\n        \"http://localhost:3001\",\n        # Production - Add your admin panel URL\n        # \"https://admin-panel-production.up.railway.app\",\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"deployment/cors-and-authentication/#what-this-means","title":"What This Means","text":"<p>Browser-based admin panel: - \u2705 Can make requests from localhost:3000, 3001, 3002 - \u274c Blocked from other origins (until you add them) - Must add production admin panel URL for production</p> <p>Server-side team clients: - \u2705 Work from ANY location - \u2705 Completely ignore CORS configuration - \u2705 Only need valid Bearer token (virtual key)</p>"},{"location":"deployment/cors-and-authentication/#common-scenarios","title":"Common Scenarios","text":""},{"location":"deployment/cors-and-authentication/#scenario-1-team-client-python","title":"Scenario 1: Team Client (Python)","text":"<pre><code># Team's Python application making API calls\n\nimport requests\n\nAPI_URL = \"https://saas-api-production.up.railway.app\"\nVIRTUAL_KEY = \"sk-team-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Create job\nresponse = requests.post(\n    f\"{API_URL}/api/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"analysis\"}\n)\n\n# \u2705 Works perfectly - CORS doesn't apply to server-side Python\n# \u2705 Only requires valid virtual key\n# \u2705 Can be called from anywhere (AWS, Google Cloud, on-premise, etc.)\n</code></pre> <p>Result: \u2705 Works - CORS is ignored by Python <code>requests</code></p>"},{"location":"deployment/cors-and-authentication/#scenario-2-admin-panel-browser","title":"Scenario 2: Admin Panel (Browser)","text":"<pre><code>// Admin panel (Next.js) running at https://admin.yourcompany.com\n// Making request to https://api.yourcompany.com\n\nconst response = await fetch('https://api.yourcompany.com/api/model-groups', {\n  headers: {\n    'X-Admin-Key': masterKey\n  }\n})\n\n// \u26a0\ufe0f Browser checks CORS before sending this request\n</code></pre> <p>If CORS allows <code>https://admin.yourcompany.com</code>: - \u2705 Request succeeds</p> <p>If CORS doesn't allow <code>https://admin.yourcompany.com</code>: - \u274c Browser blocks request with CORS error - \u274c See error in browser console: \"CORS policy: No 'Access-Control-Allow-Origin' header\"</p>"},{"location":"deployment/cors-and-authentication/#scenario-3-browser-based-team-client-not-recommended","title":"Scenario 3: Browser-Based Team Client (Not Recommended)","text":"<pre><code>&lt;!-- Team trying to call API directly from browser JavaScript --&gt;\n&lt;script&gt;\n  fetch('https://api.yourcompany.com/api/jobs/create', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-team-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({...})\n  })\n  // \u274c CORS error - API doesn't allow this origin\n  // \u26a0\ufe0f SECURITY RISK - Virtual key exposed in browser\n&lt;/script&gt;\n</code></pre> <p>Result: \u274c Blocked by CORS (and insecure anyway)</p> <p>Solution: Use a backend API to proxy the requests:</p> <pre><code>Browser \u2192 Your Backend API \u2192 SaaS LiteLLM API\n        (No CORS issue)    (Server-side, CORS ignored)\n</code></pre>"},{"location":"deployment/cors-and-authentication/#setting-up-production-cors","title":"Setting Up Production CORS","text":""},{"location":"deployment/cors-and-authentication/#step-1-deploy-admin-panel","title":"Step 1: Deploy Admin Panel","text":"<p>Deploy your admin panel to production (Railway, Vercel, etc.) and note the URL:</p> <pre><code>https://admin-panel-production-abc123.up.railway.app\n</code></pre>"},{"location":"deployment/cors-and-authentication/#step-2-update-cors-configuration","title":"Step 2: Update CORS Configuration","text":"<p>Edit <code>src/saas_api.py</code>:</p> <pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        # Local development\n        \"http://localhost:3000\",\n        \"http://localhost:3002\",\n        \"http://localhost:3001\",\n        # Production\n        \"https://admin-panel-production-abc123.up.railway.app\",\n        # Or with custom domain\n        # \"https://admin.yourcompany.com\",\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"deployment/cors-and-authentication/#step-3-redeploy-saas-api","title":"Step 3: Redeploy SaaS API","text":"<pre><code># Commit and push changes\ngit add src/saas_api.py\ngit commit -m \"Add production admin panel to CORS\"\ngit push\n\n# Railway will auto-deploy\n</code></pre>"},{"location":"deployment/cors-and-authentication/#step-4-verify","title":"Step 4: Verify","text":"<p>Open your production admin panel and test: - Login should work - API requests should succeed - No CORS errors in browser console</p>"},{"location":"deployment/cors-and-authentication/#using-environment-variables-for-cors","title":"Using Environment Variables for CORS","text":"<p>For more flexible configuration, use environment variables:</p>"},{"location":"deployment/cors-and-authentication/#1-update-srcconfigsettingspy","title":"1. Update <code>src/config/settings.py</code>","text":"<pre><code>class Settings(BaseSettings):\n    # ... existing settings ...\n\n    # Admin panel URL for CORS\n    admin_panel_url: str = \"http://localhost:3002\"\n\n    # Additional CORS origins (comma-separated)\n    additional_cors_origins: str = \"\"\n</code></pre>"},{"location":"deployment/cors-and-authentication/#2-update-srcsaas_apipy","title":"2. Update <code>src/saas_api.py</code>","text":"<pre><code># Build CORS origins from settings\ncors_origins = [\n    \"http://localhost:3000\",\n    \"http://localhost:3001\",\n    \"http://localhost:3002\",\n    settings.admin_panel_url,\n]\n\n# Add additional origins if configured\nif settings.additional_cors_origins:\n    additional = [\n        origin.strip()\n        for origin in settings.additional_cors_origins.split(',')\n        if origin.strip()\n    ]\n    cors_origins.extend(additional)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=cors_origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"deployment/cors-and-authentication/#3-configure-in-railway","title":"3. Configure in Railway","text":"<p>Add environment variable:</p> <pre><code># Railway Variables\nADMIN_PANEL_URL=https://admin-panel-production.up.railway.app\n\n# Or multiple origins\nADDITIONAL_CORS_ORIGINS=https://admin.yourcompany.com,https://admin-staging.yourcompany.com\n</code></pre>"},{"location":"deployment/cors-and-authentication/#troubleshooting-cors","title":"Troubleshooting CORS","text":""},{"location":"deployment/cors-and-authentication/#error-cors-policy-no-access-control-allow-origin-header","title":"Error: \"CORS policy: No 'Access-Control-Allow-Origin' header\"","text":"<p>Problem: Browser is blocking the request because the origin is not allowed.</p> <p>Check: 1. Are you making the request from a browser?    - Yes \u2192 CORS applies, add origin to <code>allow_origins</code>    - No (Python/Node.js/curl) \u2192 CORS doesn't apply, this is a different error</p> <ol> <li> <p>Is the origin in the <code>allow_origins</code> list?    <pre><code># Check current CORS config in src/saas_api.py\nallow_origins=[...]\n</code></pre></p> </li> <li> <p>Did you redeploy after updating CORS?</p> </li> <li>Changes to <code>src/saas_api.py</code> require redeployment</li> </ol> <p>Solution: <pre><code># Add your origin to allow_origins\nallow_origins=[\n    \"http://localhost:3000\",\n    \"https://your-admin-panel-url.com\",  # \u2190 Add this\n]\n</code></pre></p>"},{"location":"deployment/cors-and-authentication/#error-team-api-client-getting-401-unauthorized","title":"Error: \"Team API client getting 401 Unauthorized\"","text":"<p>Problem: This is NOT a CORS issue (server-side clients ignore CORS).</p> <p>Check: 1. Is the virtual key correct? 2. Is the <code>Authorization</code> header formatted correctly?    <pre><code>Authorization: Bearer sk-your-virtual-key\n</code></pre> 3. Is the team active and not suspended?</p> <p>This is an authentication issue, not CORS.</p>"},{"location":"deployment/cors-and-authentication/#error-preflight-request-doesnt-pass-access-control-check","title":"Error: \"Preflight request doesn't pass access control check\"","text":"<p>Problem: Browser preflight (OPTIONS) request is being blocked.</p> <p>Check: 1. Is <code>allow_methods=[\"*\"]</code> set? 2. Is <code>allow_headers=[\"*\"]</code> set? 3. Is the origin in <code>allow_origins</code>?</p> <p>Solution: <pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://your-origin.com\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # \u2190 Must allow all methods\n    allow_headers=[\"*\"],  # \u2190 Must allow all headers\n)\n</code></pre></p>"},{"location":"deployment/cors-and-authentication/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/cors-and-authentication/#cors-is-not-authentication","title":"CORS is NOT Authentication","text":"<p>CORS does NOT secure your API. It only controls which browser-based clients can access it.</p> <ul> <li>\u274c CORS does NOT validate API keys</li> <li>\u274c CORS does NOT prevent malicious server-side clients</li> <li>\u274c CORS does NOT encrypt data</li> </ul> <p>Authentication (Bearer tokens) secures your API:</p> <ul> <li>\u2705 Bearer tokens validate team identity</li> <li>\u2705 MASTER_KEY validates admin identity</li> <li>\u2705 Works for all clients (browser and server-side)</li> </ul>"},{"location":"deployment/cors-and-authentication/#cors-authentication-together","title":"CORS + Authentication Together","text":"<pre><code>Layer 1: CORS (Browser-only)\n  \u2193 Checks if origin is allowed\n  \u2193 (Server-side clients skip this layer)\n\nLayer 2: Authentication (All clients)\n  \u2193 Validates Bearer token or MASTER_KEY\n  \u2193 Verifies team/admin identity\n  \u2193 Checks team status (active/suspended)\n\nLayer 3: Authorization\n  \u2193 Verifies team has access to requested model\n  \u2193 Checks credit balance\n  \u2193 Validates rate limits\n</code></pre>"},{"location":"deployment/cors-and-authentication/#dont-use-wildcard-cors","title":"Don't Use Wildcard CORS","text":"<p>Avoid this in production: <pre><code># \u274c Don't do this\nallow_origins=[\"*\"]\n</code></pre></p> <p>Why: - Allows any browser-based client to call your API - Exposes admin endpoints to malicious websites - Increases attack surface</p> <p>Instead: <pre><code># \u2705 Explicitly list allowed origins\nallow_origins=[\n    \"https://admin.yourcompany.com\",\n    \"https://admin-staging.yourcompany.com\",\n]\n</code></pre></p>"},{"location":"deployment/cors-and-authentication/#summary","title":"Summary","text":"Aspect CORS Authentication Applies to Browser-based clients only All clients Purpose Control which domains can make browser requests Verify client identity Layer Browser security API security Team clients Not affected Required (Bearer token) Admin panel Required Required (MASTER_KEY) Configuration <code>allow_origins</code> in CORS middleware Virtual keys, MASTER_KEY"},{"location":"deployment/cors-and-authentication/#next-steps","title":"Next Steps","text":"<ul> <li>Railway Deployment - Deploy with proper CORS</li> <li>Environment Variables - Configure CORS via env vars</li> <li>Admin Authentication - Set up MASTER_KEY</li> <li>Team Authentication - Use virtual keys</li> </ul>"},{"location":"deployment/cors-and-authentication/#additional-resources","title":"Additional Resources","text":"<ul> <li>MDN CORS Guide - Official CORS documentation</li> <li>FastAPI CORS - FastAPI CORS middleware docs</li> </ul>"},{"location":"deployment/docker/","title":"Docker Deployment Guide","text":"<p>This guide covers Docker setup, configuration, and troubleshooting for the SaaS LiteLLM platform.</p>"},{"location":"deployment/docker/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Start</li> <li>Docker Compose Setup</li> <li>Service Configuration</li> <li>Building Custom Images</li> <li>Production Deployment</li> <li>Networking</li> <li>Volumes and Persistence</li> <li>Health Checks</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/docker/#overview","title":"Overview","text":"<p>The SaaS LiteLLM platform uses Docker for:</p> <ol> <li>Local Development - Docker Compose with PostgreSQL and Redis</li> <li>Production Deployment - Container images for Railway/cloud platforms</li> <li>Service Isolation - Separate containers for each component</li> </ol>"},{"location":"deployment/docker/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   SaaS API (8003)   \u2502  FastAPI wrapper with job tracking\n\u2502  (FastAPI + Python) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LiteLLM Proxy (8002)\u2502  Proxy with model routing &amp; caching\n\u2502   (LiteLLM Server)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n      \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502PostgreSQL\u2502 \u2502  Redis   \u2502\n\u2502  (5432)  \u2502 \u2502  (6379)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/docker/#quick-start","title":"Quick Start","text":""},{"location":"deployment/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10+ installed</li> <li>Docker Compose 2.0+ installed</li> <li>4GB+ RAM available</li> <li>10GB+ disk space</li> </ul>"},{"location":"deployment/docker/#one-command-setup","title":"One-Command Setup","text":"<pre><code># Clone and setup\ngit clone https://github.com/yourusername/SaasLiteLLM.git\ncd SaasLiteLLM\n\n# Setup environment\ncp .env.example .env\nnano .env  # Add your API keys\n\n# Start everything\n./scripts/docker_setup.sh\n</code></pre> <p>The script will: 1. Start PostgreSQL container 2. Start Redis container 3. Initialize database schema 4. Wait for services to be healthy</p>"},{"location":"deployment/docker/#verify-installation","title":"Verify Installation","text":"<pre><code># Check all services are running\ndocker compose ps\n\n# Should show:\n# litellm-postgres   running   5432/tcp\n# litellm-redis      running   6379/tcp\n\n# Test database connection\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm -c \"SELECT 1;\"\n\n# Test Redis connection\ndocker exec -it litellm-redis redis-cli ping\n</code></pre>"},{"location":"deployment/docker/#docker-compose-setup","title":"Docker Compose Setup","text":""},{"location":"deployment/docker/#services-overview","title":"Services Overview","text":"<p>The <code>docker-compose.yml</code> defines three core services and one optional service:</p> Service Image Port Purpose postgres <code>postgres:15-alpine</code> 5432 Main database redis <code>redis:7-alpine</code> 6380\u21926379 Caching &amp; rate limiting litellm <code>ghcr.io/berriai/litellm:main-latest</code> 8002\u21924000 LiteLLM proxy pgadmin (optional) <code>dpage/pgadmin4:latest</code> 5050 Database management UI"},{"location":"deployment/docker/#complete-docker-composeyml","title":"Complete docker-compose.yml","text":"<pre><code>services:\n  postgres:\n    image: postgres:15-alpine\n    container_name: litellm-postgres\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: litellm_user\n      POSTGRES_PASSWORD: litellm_password\n      POSTGRES_HOST_AUTH_METHOD: trust\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U litellm_user -d litellm\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - litellm-network\n\n  redis:\n    image: redis:7-alpine\n    container_name: litellm-redis\n    ports:\n      - \"6380:6379\"  # External 6380 to avoid conflicts\n    volumes:\n      - redis_data:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - litellm-network\n\n  litellm:\n    image: ghcr.io/berriai/litellm:main-latest\n    container_name: litellm-proxy\n    ports:\n      - \"8002:4000\"\n    volumes:\n      - ./src/config/litellm_config.yaml:/app/config.yaml\n    environment:\n      DATABASE_URL: postgresql://litellm_user:litellm_password@postgres:5432/litellm\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-local-dev-master-key-change-me}\n      OPENAI_API_KEY: ${OPENAI_API_KEY:-}\n      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n    command: [\"--config\", \"/app/config.yaml\", \"--port\", \"4000\", \"--detailed_debug\"]\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - litellm-network\n    restart: unless-stopped\n\n  # Optional: pgAdmin for database management\n  pgadmin:\n    image: dpage/pgadmin4:latest\n    container_name: litellm-pgadmin\n    environment:\n      PGADMIN_DEFAULT_EMAIL: admin@litellm.local\n      PGADMIN_DEFAULT_PASSWORD: admin\n      PGADMIN_CONFIG_SERVER_MODE: 'False'\n    ports:\n      - \"5050:80\"\n    depends_on:\n      - postgres\n    networks:\n      - litellm-network\n    profiles:\n      - pgadmin  # Only starts with --profile pgadmin\n\nvolumes:\n  postgres_data:\n  redis_data:\n\nnetworks:\n  litellm-network:\n    driver: bridge\n</code></pre>"},{"location":"deployment/docker/#common-commands","title":"Common Commands","text":"<pre><code># Start all services\ndocker compose up -d\n\n# Start with pgAdmin\ndocker compose --profile pgadmin up -d\n\n# Stop all services\ndocker compose down\n\n# Stop and remove volumes (CAUTION: deletes data)\ndocker compose down -v\n\n# View logs\ndocker compose logs -f\n\n# View logs for specific service\ndocker compose logs -f postgres\ndocker compose logs -f redis\ndocker compose logs -f litellm\n\n# Restart a service\ndocker compose restart postgres\n\n# Execute command in container\ndocker compose exec postgres psql -U litellm_user -d litellm\ndocker compose exec redis redis-cli\n\n# Check service status\ndocker compose ps\n</code></pre>"},{"location":"deployment/docker/#service-configuration","title":"Service Configuration","text":""},{"location":"deployment/docker/#postgresql-configuration","title":"PostgreSQL Configuration","text":""},{"location":"deployment/docker/#environment-variables","title":"Environment Variables","text":"<pre><code>environment:\n  POSTGRES_DB: litellm          # Database name\n  POSTGRES_USER: litellm_user   # Username\n  POSTGRES_PASSWORD: litellm_password  # Password\n  POSTGRES_HOST_AUTH_METHOD: trust     # Auth method\n</code></pre>"},{"location":"deployment/docker/#initialization-script","title":"Initialization Script","text":"<p>The <code>docker/postgres/init.sql</code> script runs on first startup:</p> <pre><code>-- Create user if not exists\nDO $$\nBEGIN\n   IF NOT EXISTS (SELECT FROM pg_catalog.pg_roles WHERE rolname = 'litellm_user') THEN\n      CREATE USER litellm_user WITH PASSWORD 'litellm_password';\n   END IF;\nEND\n$$;\n\n-- Grant privileges on database\nGRANT ALL PRIVILEGES ON DATABASE litellm TO litellm_user;\n\n-- Connect to litellm database\n\\c litellm;\n\n-- Grant comprehensive schema privileges\nGRANT ALL ON SCHEMA public TO litellm_user;\nGRANT CREATE ON SCHEMA public TO litellm_user;\nGRANT USAGE ON SCHEMA public TO litellm_user;\n\n-- Grant privileges on existing tables and sequences\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO litellm_user;\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO litellm_user;\nGRANT ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA public TO litellm_user;\n\n-- Grant default privileges for future objects\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO litellm_user;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO litellm_user;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON FUNCTIONS TO litellm_user;\n\n-- Make litellm_user the owner of the public schema\nALTER SCHEMA public OWNER TO litellm_user;\n\n-- Create extension for UUID generation if needed\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\n-- Basic setup complete\nSELECT 'LiteLLM PostgreSQL database initialized successfully!' as message;\n</code></pre>"},{"location":"deployment/docker/#custom-configuration","title":"Custom Configuration","text":"<p>To customize PostgreSQL, mount a custom config:</p> <pre><code>postgres:\n  volumes:\n    - postgres_data:/var/lib/postgresql/data\n    - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\n    - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf\n  command: postgres -c config_file=/etc/postgresql/postgresql.conf\n</code></pre>"},{"location":"deployment/docker/#redis-configuration","title":"Redis Configuration","text":""},{"location":"deployment/docker/#basic-setup","title":"Basic Setup","text":"<p>Redis runs with default configuration, suitable for development.</p>"},{"location":"deployment/docker/#production-configuration","title":"Production Configuration","text":"<p>For production, create <code>docker/redis/redis.conf</code>:</p> <pre><code># Network\nbind 0.0.0.0\nport 6379\nprotected-mode yes\n\n# Security\nrequirepass your-secure-password\n\n# Memory\nmaxmemory 512mb\nmaxmemory-policy allkeys-lru\n\n# Persistence\nsave 900 1\nsave 300 10\nsave 60 10000\nappendonly yes\nappendfilename \"appendonly.aof\"\n\n# Performance\ntcp-backlog 511\ntimeout 0\ntcp-keepalive 300\n</code></pre> <p>Mount configuration:</p> <pre><code>redis:\n  volumes:\n    - redis_data:/data\n    - ./docker/redis/redis.conf:/usr/local/etc/redis/redis.conf\n  command: redis-server /usr/local/etc/redis/redis.conf\n</code></pre>"},{"location":"deployment/docker/#litellm-configuration","title":"LiteLLM Configuration","text":"<p>The LiteLLM container uses <code>src/config/litellm_config.yaml</code>:</p> <pre><code># Models are managed via database - add them through the dashboard UI\n# Visit http://localhost:8002/ui to add models\nmodel_list: []\n\n# General settings\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n  database_url: os.environ/DATABASE_URL\n  store_model_in_db: true\n\n  # Redis caching\n  cache: true\n  cache_params:\n    type: \"redis\"\n    host: os.environ/REDIS_HOST\n    port: os.environ/REDIS_PORT\n    password: os.environ/REDIS_PASSWORD\n    ttl: 600  # 10 minutes\n\n  # Cost tracking per team\n  track_cost_per_deployment: true\n\n# Router settings\nrouter_settings:\n  redis_host: os.environ/REDIS_HOST\n  redis_port: os.environ/REDIS_PORT\n  redis_password: os.environ/REDIS_PASSWORD\n  enable_pre_call_checks: true\n  routing_strategy: \"usage-based-routing\"\n  num_retries: 3\n  timeout: 600\n  redis_enabled: true\n</code></pre>"},{"location":"deployment/docker/#building-custom-images","title":"Building Custom Images","text":""},{"location":"deployment/docker/#saas-api-image","title":"SaaS API Image","text":"<p>The main <code>Dockerfile</code> builds the FastAPI SaaS API wrapper:</p> <pre><code># Dockerfile for FastAPI SaaS API Service\n# This wraps LiteLLM with job-based cost tracking\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    postgresql-client \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy dependency files from project root\nCOPY pyproject.toml ./\nCOPY README.md ./\n\n# Copy application code from project root\nCOPY src/ ./src/\n\n# Install build tools and Python dependencies\nRUN pip install --no-cache-dir --upgrade pip &amp;&amp; \\\n    pip install --no-cache-dir hatchling &amp;&amp; \\\n    pip install --no-cache-dir .\n\n# Create non-root user for security\nRUN useradd -m -u 1000 saasapi &amp;&amp; \\\n    chown -R saasapi:saasapi /app\n\nUSER saasapi\n\n# Expose port (Railway will assign PORT env var)\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Start FastAPI application\n# Use Railway's PORT env var if provided, otherwise default to 8000\nCMD uvicorn src.saas_api:app --host 0.0.0.0 --port ${PORT:-8000}\n</code></pre>"},{"location":"deployment/docker/#build-and-run","title":"Build and Run","text":"<pre><code># Build image\ndocker build -t saas-api:latest .\n\n# Run locally\ndocker run -p 8000:8000 \\\n  -e DATABASE_URL=postgresql://... \\\n  -e LITELLM_MASTER_KEY=sk-... \\\n  -e LITELLM_PROXY_URL=http://litellm:4000 \\\n  saas-api:latest\n\n# Build for multiple platforms (M1/M2 Macs)\ndocker buildx build --platform linux/amd64,linux/arm64 -t saas-api:latest .\n</code></pre>"},{"location":"deployment/docker/#litellm-proxy-image","title":"LiteLLM Proxy Image","text":"<p>The <code>services/litellm/Dockerfile</code> builds a custom LiteLLM image:</p> <pre><code># Dockerfile for LiteLLM Proxy Service\n# Use the official LiteLLM Docker image\nFROM ghcr.io/berriai/litellm:main-latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy configuration file\nCOPY litellm_config.yaml /app/config.yaml\n\n# Expose port (Railway will use PORT env var)\nEXPOSE 4000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n    CMD curl -f http://localhost:4000/health || exit 1\n\n# Start LiteLLM proxy\n# Railway provides PORT env var, but LiteLLM uses 4000 by default\nCMD [\"--config\", \"/app/config.yaml\", \"--port\", \"4000\", \"--detailed_debug\"]\n</code></pre>"},{"location":"deployment/docker/#build-and-run_1","title":"Build and Run","text":"<pre><code># Build image\ncd services/litellm\ndocker build -t litellm-proxy:latest .\n\n# Run locally\ndocker run -p 4000:4000 \\\n  -e DATABASE_URL=postgresql://... \\\n  -e LITELLM_MASTER_KEY=sk-... \\\n  -e OPENAI_API_KEY=sk-... \\\n  -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n  litellm-proxy:latest\n</code></pre>"},{"location":"deployment/docker/#multi-stage-builds-optimization","title":"Multi-Stage Builds (Optimization)","text":"<p>For smaller images, use multi-stage builds:</p> <pre><code># Stage 1: Builder\nFROM python:3.11-slim as builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; apt-get install -y gcc\n\n# Install Python dependencies\nCOPY pyproject.toml README.md ./\nRUN pip install --no-cache-dir --upgrade pip &amp;&amp; \\\n    pip install --no-cache-dir hatchling &amp;&amp; \\\n    pip wheel --no-cache-dir --wheel-dir /app/wheels .\n\n# Stage 2: Runtime\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    postgresql-client \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy wheels from builder\nCOPY --from=builder /app/wheels /wheels\nRUN pip install --no-cache-dir /wheels/*\n\n# Copy application code\nCOPY src/ ./src/\n\n# Create non-root user\nRUN useradd -m -u 1000 saasapi &amp;&amp; \\\n    chown -R saasapi:saasapi /app\n\nUSER saasapi\n\nEXPOSE 8000\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\nCMD uvicorn src.saas_api:app --host 0.0.0.0 --port ${PORT:-8000}\n</code></pre>"},{"location":"deployment/docker/#production-deployment","title":"Production Deployment","text":""},{"location":"deployment/docker/#railway-deployment","title":"Railway Deployment","text":""},{"location":"deployment/docker/#using-pre-built-images-recommended","title":"Using Pre-built Images (Recommended)","text":"<p>Deploy from GitHub Container Registry:</p> <pre><code># Railway service configuration\n# Image: ghcr.io/yourusername/saaslitellm/saas-api:latest\n# Port: 8000\n</code></pre> <p>Environment variables in Railway:</p> <pre><code>DATABASE_URL=${{Postgres.DATABASE_URL}}\nLITELLM_MASTER_KEY=sk-prod-your-key\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\nOPENAI_API_KEY=${{OPENAI_API_KEY}}\nREDIS_HOST=${{Redis.REDIS_HOST}}\n</code></pre>"},{"location":"deployment/docker/#build-from-source","title":"Build from Source","text":"<p>Railway can build directly from your repository:</p> <ol> <li>Connect GitHub repository</li> <li>Set build context:</li> <li>Root directory: <code>/</code></li> <li>Dockerfile: <code>/Dockerfile</code></li> <li>Configure environment variables</li> <li>Deploy</li> </ol>"},{"location":"deployment/docker/#docker-compose-for-production","title":"Docker Compose for Production","text":"<p>For VPS or self-hosted deployment:</p> <pre><code># docker-compose.prod.yml\nversion: '3.8'\n\nservices:\n  postgres:\n    image: postgres:15-alpine\n    container_name: prod-postgres\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    networks:\n      - prod-network\n    restart: always\n\n  redis:\n    image: redis:7-alpine\n    container_name: prod-redis\n    command: redis-server --requirepass ${REDIS_PASSWORD}\n    volumes:\n      - redis_data:/data\n    networks:\n      - prod-network\n    restart: always\n\n  litellm:\n    image: ghcr.io/berriai/litellm:main-latest\n    container_name: prod-litellm\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\n      OPENAI_API_KEY: ${OPENAI_API_KEY}\n      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n    volumes:\n      - ./src/config/litellm_config.yaml:/app/config.yaml\n    depends_on:\n      - postgres\n      - redis\n    networks:\n      - prod-network\n    restart: always\n\n  saas-api:\n    image: ghcr.io/yourusername/saaslitellm/saas-api:latest\n    container_name: prod-saas-api\n    ports:\n      - \"8000:8000\"\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\n      LITELLM_PROXY_URL: http://litellm:4000\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      ENVIRONMENT: production\n      DEBUG: false\n      WORKERS: 4\n    depends_on:\n      - postgres\n      - redis\n      - litellm\n    networks:\n      - prod-network\n    restart: always\n\nvolumes:\n  postgres_data:\n  redis_data:\n\nnetworks:\n  prod-network:\n    driver: bridge\n</code></pre> <p>Deploy:</p> <pre><code># Create .env.prod\ncp .env.example .env.prod\nnano .env.prod  # Set production values\n\n# Deploy\ndocker compose -f docker-compose.prod.yml --env-file .env.prod up -d\n\n# Monitor\ndocker compose -f docker-compose.prod.yml logs -f\n</code></pre>"},{"location":"deployment/docker/#networking","title":"Networking","text":""},{"location":"deployment/docker/#docker-networks","title":"Docker Networks","text":"<p>Services communicate via the <code>litellm-network</code> bridge network:</p> <pre><code>networks:\n  litellm-network:\n    driver: bridge\n</code></pre> <p>Internal DNS: - Services can reach each other by service name - <code>postgres</code> resolves to PostgreSQL container - <code>redis</code> resolves to Redis container - <code>litellm</code> resolves to LiteLLM container</p> <p>Example connection strings: <pre><code># From SaaS API to LiteLLM\nLITELLM_PROXY_URL=http://litellm:4000\n\n# From LiteLLM to PostgreSQL\nDATABASE_URL=postgresql://litellm_user:litellm_password@postgres:5432/litellm\n\n# From LiteLLM to Redis\nREDIS_HOST=redis\nREDIS_PORT=6379\n</code></pre></p>"},{"location":"deployment/docker/#port-mapping","title":"Port Mapping","text":"<p>External ports are mapped to avoid conflicts:</p> Service Internal Port External Port Purpose PostgreSQL 5432 5432 Database access Redis 6379 6380 Cache access (avoids local Redis) LiteLLM 4000 8002 Proxy API SaaS API 8000 8003 Main API pgAdmin 80 5050 Database UI <p>Access from host: <pre><code># PostgreSQL\npsql postgresql://litellm_user:litellm_password@localhost:5432/litellm\n\n# Redis\nredis-cli -h localhost -p 6380\n\n# LiteLLM\ncurl http://localhost:8002/health\n\n# SaaS API\ncurl http://localhost:8003/health\n</code></pre></p>"},{"location":"deployment/docker/#custom-networks","title":"Custom Networks","text":"<p>Create isolated networks for different environments:</p> <pre><code>networks:\n  backend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n\n  frontend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.21.0.0/16\n\nservices:\n  postgres:\n    networks:\n      - backend\n\n  saas-api:\n    networks:\n      - backend\n      - frontend\n</code></pre>"},{"location":"deployment/docker/#volumes-and-persistence","title":"Volumes and Persistence","text":""},{"location":"deployment/docker/#named-volumes","title":"Named Volumes","text":"<p>Data is persisted using Docker volumes:</p> <pre><code>volumes:\n  postgres_data:    # PostgreSQL database files\n  redis_data:       # Redis persistence files\n</code></pre>"},{"location":"deployment/docker/#volume-management","title":"Volume Management","text":"<pre><code># List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect saaslitellm_postgres_data\n\n# Backup PostgreSQL\ndocker exec litellm-postgres pg_dump -U litellm_user litellm &gt; backup.sql\n\n# Restore PostgreSQL\ndocker exec -i litellm-postgres psql -U litellm_user litellm &lt; backup.sql\n\n# Backup Redis\ndocker exec litellm-redis redis-cli SAVE\ndocker cp litellm-redis:/data/dump.rdb ./redis-backup.rdb\n\n# Remove volumes (CAUTION: deletes all data)\ndocker compose down -v\n</code></pre>"},{"location":"deployment/docker/#bind-mounts","title":"Bind Mounts","text":"<p>For development, mount local files:</p> <pre><code>litellm:\n  volumes:\n    - ./src/config/litellm_config.yaml:/app/config.yaml  # Config file\n    - ./logs:/app/logs  # Log files\n</code></pre> <p>Hot reload with bind mounts:</p> <pre><code>saas-api:\n  build: .\n  volumes:\n    - ./src:/app/src  # Mount source code\n  command: uvicorn src.saas_api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre>"},{"location":"deployment/docker/#health-checks","title":"Health Checks","text":""},{"location":"deployment/docker/#postgresql-health-check","title":"PostgreSQL Health Check","text":"<pre><code>healthcheck:\n  test: [\"CMD-SHELL\", \"pg_isready -U litellm_user -d litellm\"]\n  interval: 10s\n  timeout: 5s\n  retries: 5\n  start_period: 10s\n</code></pre> <p>Manual check: <pre><code>docker exec litellm-postgres pg_isready -U litellm_user -d litellm\n</code></pre></p>"},{"location":"deployment/docker/#redis-health-check","title":"Redis Health Check","text":"<pre><code>healthcheck:\n  test: [\"CMD\", \"redis-cli\", \"ping\"]\n  interval: 10s\n  timeout: 5s\n  retries: 5\n</code></pre> <p>Manual check: <pre><code>docker exec litellm-redis redis-cli ping\n</code></pre></p>"},{"location":"deployment/docker/#application-health-check","title":"Application Health Check","text":"<pre><code>healthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n  interval: 30s\n  timeout: 10s\n  start_period: 40s\n  retries: 3\n</code></pre> <p>Manual check: <pre><code>curl http://localhost:8000/health\n</code></pre></p>"},{"location":"deployment/docker/#monitoring-health-status","title":"Monitoring Health Status","text":"<pre><code># Check all health statuses\ndocker compose ps\n\n# Watch health status\nwatch -n 2 'docker compose ps'\n\n# Get detailed health info\ndocker inspect --format='{{json .State.Health}}' litellm-postgres | jq\n</code></pre>"},{"location":"deployment/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/docker/#container-wont-start","title":"Container Won't Start","text":"<p>Check logs: <pre><code>docker compose logs postgres\ndocker compose logs redis\ndocker compose logs litellm\n</code></pre></p> <p>Common issues:</p> <ol> <li> <p>Port already in use <pre><code>Error: bind: address already in use\n</code></pre>    Solution: Stop conflicting service or change port    <pre><code># Find process using port\nlsof -i :5432\n\n# Kill process or change port in docker-compose.yml\nports:\n  - \"5433:5432\"  # Use different external port\n</code></pre></p> </li> <li> <p>Volume permission issues <pre><code>Error: permission denied\n</code></pre>    Solution: Fix volume permissions    <pre><code>sudo chown -R $(id -u):$(id -g) ./volumes\n</code></pre></p> </li> <li> <p>Out of memory <pre><code>Error: Container killed (OOMKilled)\n</code></pre>    Solution: Increase Docker memory    <pre><code># Docker Desktop: Settings \u2192 Resources \u2192 Memory\n# Or add to docker-compose.yml:\ndeploy:\n  resources:\n    limits:\n      memory: 2G\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#database-connection-issues","title":"Database Connection Issues","text":"<p>Error: <code>FATAL: database \"litellm\" does not exist</code></p> <pre><code># Check if database exists\ndocker exec litellm-postgres psql -U litellm_user -l\n\n# Create database if missing\ndocker exec litellm-postgres psql -U postgres -c \"CREATE DATABASE litellm;\"\n\n# Grant privileges\ndocker exec litellm-postgres psql -U postgres -c \"GRANT ALL PRIVILEGES ON DATABASE litellm TO litellm_user;\"\n</code></pre> <p>Error: <code>FATAL: role \"litellm_user\" does not exist</code></p> <pre><code># Create user\ndocker exec litellm-postgres psql -U postgres -c \"CREATE USER litellm_user WITH PASSWORD 'litellm_password';\"\n</code></pre> <p>Error: <code>could not connect to server</code></p> <pre><code># Check if PostgreSQL is running\ndocker compose ps postgres\n\n# Check PostgreSQL logs\ndocker compose logs postgres\n\n# Restart PostgreSQL\ndocker compose restart postgres\n\n# Check health status\ndocker inspect --format='{{json .State.Health}}' litellm-postgres\n</code></pre>"},{"location":"deployment/docker/#redis-connection-issues","title":"Redis Connection Issues","text":"<p>Error: <code>Error connecting to Redis</code></p> <pre><code># Check if Redis is running\ndocker compose ps redis\n\n# Test Redis connection\ndocker exec litellm-redis redis-cli ping\n\n# Check Redis logs\ndocker compose logs redis\n\n# Restart Redis\ndocker compose restart redis\n</code></pre> <p>Error: <code>NOAUTH Authentication required</code></p> <pre><code># If Redis has password enabled, update config\ndocker exec litellm-redis redis-cli -a your-password ping\n</code></pre>"},{"location":"deployment/docker/#litellm-proxy-issues","title":"LiteLLM Proxy Issues","text":"<p>Error: <code>Connection refused to LiteLLM proxy</code></p> <pre><code># Check if LiteLLM is running\ndocker compose ps litellm\n\n# Check logs for errors\ndocker compose logs litellm\n\n# Test LiteLLM health endpoint\ncurl http://localhost:8002/health\n\n# Verify environment variables\ndocker compose exec litellm env | grep LITELLM\n</code></pre> <p>Error: <code>Invalid master key</code></p> <pre><code># Check master key configuration\ndocker compose exec litellm env | grep LITELLM_MASTER_KEY\n\n# Update .env file with correct key\nnano .env\n\n# Restart LiteLLM\ndocker compose restart litellm\n</code></pre>"},{"location":"deployment/docker/#performance-issues","title":"Performance Issues","text":"<p>High CPU usage:</p> <pre><code># Check resource usage\ndocker stats\n\n# Limit CPU usage\ndeploy:\n  resources:\n    limits:\n      cpus: '2.0'\n</code></pre> <p>High memory usage:</p> <pre><code># Check memory usage\ndocker stats\n\n# Limit memory\ndeploy:\n  resources:\n    limits:\n      memory: 2G\n    reservations:\n      memory: 1G\n</code></pre> <p>Slow startup:</p> <pre><code># Increase health check start period\nhealthcheck:\n  start_period: 60s  # Give more time to start\n</code></pre>"},{"location":"deployment/docker/#cleanup-and-reset","title":"Cleanup and Reset","text":"<p>Remove everything and start fresh:</p> <pre><code># Stop all containers\ndocker compose down\n\n# Remove volumes (CAUTION: deletes all data)\ndocker compose down -v\n\n# Remove images\ndocker compose down --rmi all\n\n# Clean up Docker system\ndocker system prune -a --volumes\n\n# Restart fresh\n./scripts/docker_setup.sh\n</code></pre> <p>Rebuild images:</p> <pre><code># Rebuild all images\ndocker compose build --no-cache\n\n# Rebuild specific service\ndocker compose build --no-cache saas-api\n\n# Force pull latest base images\ndocker compose pull\n</code></pre>"},{"location":"deployment/docker/#debugging-tips","title":"Debugging Tips","text":"<p>Enter container shell:</p> <pre><code># PostgreSQL\ndocker exec -it litellm-postgres bash\npsql -U litellm_user -d litellm\n\n# Redis\ndocker exec -it litellm-redis sh\nredis-cli\n\n# LiteLLM (if custom image)\ndocker exec -it litellm-proxy bash\n</code></pre> <p>Check environment variables:</p> <pre><code># List all environment variables\ndocker compose exec postgres env\ndocker compose exec redis env\ndocker compose exec litellm env\n</code></pre> <p>Monitor logs in real-time:</p> <pre><code># All services\ndocker compose logs -f\n\n# Specific service with timestamps\ndocker compose logs -f --timestamps postgres\n\n# Last 100 lines\ndocker compose logs --tail=100 litellm\n</code></pre> <p>Network debugging:</p> <pre><code># Inspect network\ndocker network inspect saaslitellm_litellm-network\n\n# Test connectivity between containers\ndocker compose exec saas-api ping postgres\ndocker compose exec saas-api ping redis\ndocker compose exec saas-api nc -zv litellm 4000\n</code></pre>"},{"location":"deployment/docker/#best-practices","title":"Best Practices","text":""},{"location":"deployment/docker/#development","title":"Development","text":"<ol> <li>Use Docker Compose for local development</li> <li>Mount source code for hot reload</li> <li>Use named volumes for data persistence</li> <li>Enable health checks to ensure services are ready</li> <li>Check logs frequently during development</li> </ol>"},{"location":"deployment/docker/#production","title":"Production","text":"<ol> <li>Use specific image tags (not <code>:latest</code>)</li> <li>Enable restart policies (<code>restart: always</code>)</li> <li>Set resource limits to prevent resource exhaustion</li> <li>Use secrets management for sensitive data</li> <li>Enable Redis persistence with AOF</li> <li>Regular backups of PostgreSQL data</li> <li>Monitor health checks and set up alerts</li> <li>Use multi-stage builds for smaller images</li> <li>Run as non-root user for security</li> <li>Keep images updated for security patches</li> </ol>"},{"location":"deployment/docker/#security","title":"Security","text":"<ol> <li>Change default passwords in production</li> <li>Use strong master keys (32+ chars)</li> <li>Enable Redis authentication</li> <li>Restrict network access with firewalls</li> <li>Use TLS/SSL for connections</li> <li>Scan images for vulnerabilities</li> <li>Limit container capabilities</li> <li>Use Docker secrets for sensitive data</li> </ol>"},{"location":"deployment/docker/#additional-resources","title":"Additional Resources","text":"<ul> <li>Docker Documentation</li> <li>Docker Compose Documentation</li> <li>PostgreSQL Docker Image</li> <li>Redis Docker Image</li> <li>LiteLLM Docker Image</li> <li>Railway Docker Deployment</li> </ul>"},{"location":"deployment/docker/#support","title":"Support","text":"<p>For Docker-related issues:</p> <ol> <li>Check container logs: <code>docker compose logs</code></li> <li>Verify health checks: <code>docker compose ps</code></li> <li>Review this troubleshooting guide</li> <li>Check Railway Deployment Guide</li> <li>Create an issue with logs and configuration</li> </ol>"},{"location":"deployment/environment-variables/","title":"Environment Variables Reference","text":"<p>Complete guide to all environment variables needed for SaasLiteLLM deployment.</p>"},{"location":"deployment/environment-variables/#quick-reference","title":"Quick Reference","text":"Service Env File Port Key Variables SaaS API <code>.env</code> (root) 8003 <code>DATABASE_URL</code>, <code>MASTER_KEY</code>, <code>LITELLM_MASTER_KEY</code>, <code>LITELLM_PROXY_URL</code> LiteLLM Proxy <code>.env</code> (root) 8002 <code>DATABASE_URL</code>, <code>LITELLM_MASTER_KEY</code>, <code>REDIS_*</code>, <code>STORE_MODEL_IN_DB</code> Admin Panel <code>admin-panel/.env.local</code> 3002 <code>NEXT_PUBLIC_API_URL</code>"},{"location":"deployment/environment-variables/#1-saas-api-environment-variables","title":"1. SaaS API Environment Variables","text":"<p>File: <code>.env</code> in project root</p>"},{"location":"deployment/environment-variables/#required","title":"Required","text":"<pre><code># Database Connection\nDATABASE_URL=postgresql://username:password@host:port/database\n\n# Admin Authentication (for SaaS API management endpoints)\n# Use this key with X-Admin-Key header to access admin endpoints\nMASTER_KEY=sk-admin-your-super-secure-admin-key-here\n\n# LiteLLM Connection\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-litellm-key-here\nLITELLM_PROXY_URL=http://localhost:8002\n</code></pre>"},{"location":"deployment/environment-variables/#optional-provider-api-keys","title":"Optional (Provider API Keys)","text":"<pre><code># LLM Provider API Keys (optional if configured via LiteLLM UI)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n# Add other providers as needed\n</code></pre>"},{"location":"deployment/environment-variables/#optional-redis-caching","title":"Optional (Redis Caching)","text":"<pre><code># Redis Configuration (optional for SaaS API, but recommended)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"deployment/environment-variables/#optional-server-configuration","title":"Optional (Server Configuration)","text":"<pre><code># Server Settings\nHOST=0.0.0.0\nPORT=8000\nWORKERS=1\nENVIRONMENT=development\nDEBUG=false\n</code></pre>"},{"location":"deployment/environment-variables/#2-litellm-proxy-environment-variables","title":"2. LiteLLM Proxy Environment Variables","text":"<p>File: <code>.env</code> in project root (SAME FILE as SaaS API)</p>"},{"location":"deployment/environment-variables/#required_1","title":"Required","text":"<pre><code># Database Connection (for storing models, teams, keys)\nDATABASE_URL=postgresql://username:password@host:port/database\n\n# LiteLLM Authentication\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-litellm-key-here\n\n# Enable database storage for models/teams/keys\nSTORE_MODEL_IN_DB=True\n\n# Redis Configuration (REQUIRED for LiteLLM caching and rate limiting)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"deployment/environment-variables/#optional-provider-api-keys_1","title":"Optional (Provider API Keys)","text":"<pre><code># LLM Provider API Keys\n# Can be configured here OR via LiteLLM UI at http://localhost:8002/ui\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nAZURE_API_KEY=...\nAZURE_API_BASE=https://...\nAZURE_API_VERSION=2024-02-15-preview\n# Add other providers as needed\n</code></pre>"},{"location":"deployment/environment-variables/#3-admin-panel-environment-variables","title":"3. Admin Panel Environment Variables","text":"<p>File: <code>admin-panel/.env.local</code></p>"},{"location":"deployment/environment-variables/#required_2","title":"Required","text":"<pre><code># SaaS API URL\n# Points to where your SaaS API is running\nNEXT_PUBLIC_API_URL=http://localhost:8003\n</code></pre> <p>Note: The MASTER_KEY is NOT stored in the admin panel's <code>.env.local</code> file. Users enter it in the login page, and it's validated against the SaaS API.</p>"},{"location":"deployment/environment-variables/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"deployment/environment-variables/#local-development","title":"Local Development","text":"<p>SaaS API &amp; LiteLLM (<code>.env</code>): <pre><code>DATABASE_URL=postgresql://litellm_user:litellm_password@localhost:5432/litellm\nMASTER_KEY=sk-admin-local-dev-change-in-production\nLITELLM_MASTER_KEY=sk-local-dev-master-key-change-me\nLITELLM_PROXY_URL=http://localhost:8002\nSTORE_MODEL_IN_DB=True\n\n# Redis (from Docker Compose)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_URL=redis://localhost:6379\n\n# Provider keys (optional)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Note: If you have multiple Redis instances running locally, adjust <code>REDIS_PORT</code> to match your instance (e.g., 6380). For Docker Compose users, Redis is exposed on port 6380 on the host but uses 6379 internally.</p> <p>Admin Panel (<code>admin-panel/.env.local</code>): <pre><code>NEXT_PUBLIC_API_URL=http://localhost:8003\n</code></pre></p>"},{"location":"deployment/environment-variables/#railway-deployment","title":"Railway Deployment","text":"<p>SaaS API Service: <pre><code>DATABASE_URL=${{Postgres.DATABASE_URL}}\nMASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n\n# Redis (if using Railway Redis)\nREDIS_HOST=${{Redis.REDISHOST}}\nREDIS_PORT=${{Redis.REDISPORT}}\nREDIS_PASSWORD=${{Redis.REDIS_PASSWORD}}\nREDIS_URL=${{Redis.REDIS_URL}}\n\n# Provider keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>LiteLLM Proxy Service: <pre><code>DATABASE_URL=${{Postgres.DATABASE_URL}}\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\nSTORE_MODEL_IN_DB=True\n\n# Redis (REQUIRED for LiteLLM)\nREDIS_HOST=${{Redis.REDISHOST}}\nREDIS_PORT=${{Redis.REDISPORT}}\nREDIS_PASSWORD=${{Redis.REDIS_PASSWORD}}\nREDIS_URL=${{Redis.REDIS_URL}}\n\n# Provider keys (can also be added via UI)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Admin Panel Service: <pre><code>NEXT_PUBLIC_API_URL=https://your-saas-api.railway.app\n</code></pre></p>"},{"location":"deployment/environment-variables/#key-concepts","title":"Key Concepts","text":""},{"location":"deployment/environment-variables/#1-two-master-keys","title":"1. Two Master Keys","text":"<p>There are TWO different master keys:</p> Key Purpose Used By <code>MASTER_KEY</code> SaaS API admin authentication Admin Panel, API management tools <code>LITELLM_MASTER_KEY</code> LiteLLM proxy authentication SaaS API \u2194 LiteLLM communication, LiteLLM UI access <p>IMPORTANT: These must be DIFFERENT keys for security.</p>"},{"location":"deployment/environment-variables/#2-shared-database","title":"2. Shared Database","text":"<p>Both SaaS API and LiteLLM Proxy can share the SAME PostgreSQL database: - SaaS API uses it for: organizations, teams, credits, jobs - LiteLLM uses it for: models, virtual keys, budgets (when <code>STORE_MODEL_IN_DB=True</code>)</p> <p>They use different tables, so there's no conflict.</p>"},{"location":"deployment/environment-variables/#3-redis-configuration","title":"3. Redis Configuration","text":"<p>Redis serves different purposes:</p> <p>For SaaS API (optional): - Response caching - Session management - Performance optimization</p> <p>For LiteLLM (REQUIRED): - Request caching - Rate limiting - Token counting - Budget tracking</p>"},{"location":"deployment/environment-variables/#4-store_model_in_db-flag","title":"4. STORE_MODEL_IN_DB Flag","text":"<p>When <code>STORE_MODEL_IN_DB=True</code>: - Models are stored in PostgreSQL (not YAML config) - Teams/keys are persisted in database - Changes via LiteLLM UI are permanent - Enables dynamic model management</p> <p>This is REQUIRED for production deployments.</p>"},{"location":"deployment/environment-variables/#generating-secure-keys","title":"Generating Secure Keys","text":"<p>For production, generate strong random keys:</p> <pre><code># Generate MASTER_KEY\nopenssl rand -hex 32\n\n# Generate LITELLM_MASTER_KEY\nopenssl rand -hex 32\n</code></pre> <p>Then format them with proper prefixes: <pre><code>MASTER_KEY=sk-admin-&lt;generated-hex&gt;\nLITELLM_MASTER_KEY=sk-litellm-&lt;generated-hex&gt;\n</code></pre></p>"},{"location":"deployment/environment-variables/#validation-checklist","title":"Validation Checklist","text":"<p>Before deploying, verify:</p>"},{"location":"deployment/environment-variables/#saas-api","title":"SaaS API","text":"<ul> <li> <code>DATABASE_URL</code> connects successfully</li> <li> <code>MASTER_KEY</code> is secure and unique</li> <li> <code>LITELLM_MASTER_KEY</code> matches LiteLLM proxy</li> <li> <code>LITELLM_PROXY_URL</code> is correct for environment</li> <li> Provider API keys are valid (if provided)</li> </ul>"},{"location":"deployment/environment-variables/#litellm-proxy","title":"LiteLLM Proxy","text":"<ul> <li> <code>DATABASE_URL</code> connects successfully</li> <li> <code>LITELLM_MASTER_KEY</code> is secure and unique</li> <li> <code>STORE_MODEL_IN_DB=True</code> is set</li> <li> Redis connection works (<code>REDIS_HOST</code>, <code>REDIS_PORT</code>)</li> <li> Can access UI at <code>http://localhost:8002/ui</code></li> </ul>"},{"location":"deployment/environment-variables/#admin-panel","title":"Admin Panel","text":"<ul> <li> <code>NEXT_PUBLIC_API_URL</code> points to correct SaaS API</li> <li> Can log in with <code>MASTER_KEY</code></li> <li> All API requests include <code>X-Admin-Key</code> header</li> </ul>"},{"location":"deployment/environment-variables/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/environment-variables/#failed-to-connect-to-litellm","title":"\"Failed to connect to LiteLLM\"","text":"<ul> <li>Check <code>LITELLM_PROXY_URL</code> is correct</li> <li>Verify <code>LITELLM_MASTER_KEY</code> matches in both services</li> <li>Ensure LiteLLM proxy is running</li> </ul>"},{"location":"deployment/environment-variables/#401-unauthorized-from-saas-api","title":"\"401 Unauthorized\" from SaaS API","text":"<ul> <li>Verify <code>MASTER_KEY</code> is correct</li> <li>Check <code>X-Admin-Key</code> header is being sent</li> <li>Ensure admin panel has correct API URL</li> </ul>"},{"location":"deployment/environment-variables/#models-not-persisting-in-litellm","title":"\"Models not persisting\" in LiteLLM","text":"<ul> <li>Verify <code>STORE_MODEL_IN_DB=True</code> is set</li> <li>Check <code>DATABASE_URL</code> is correct</li> <li>Restart LiteLLM proxy after changing this setting</li> </ul>"},{"location":"deployment/environment-variables/#redis-connection-errors","title":"Redis connection errors","text":"<ul> <li>Verify Redis is running</li> <li>Check <code>REDIS_HOST</code> and <code>REDIS_PORT</code></li> <li>For Railway, ensure Redis service is linked</li> </ul>"},{"location":"deployment/environment-variables/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit keys to git</li> <li>Use <code>.env</code> files (in <code>.gitignore</code>)</li> <li> <p>Use Railway's built-in secret management</p> </li> <li> <p>Use different keys for different environments</p> </li> <li>Development keys should differ from production</li> <li> <p>Each Railway environment should have unique keys</p> </li> <li> <p>Rotate keys regularly</p> </li> <li>Change <code>MASTER_KEY</code> quarterly</li> <li>Change <code>LITELLM_MASTER_KEY</code> quarterly</li> <li> <p>Update provider API keys as needed</p> </li> <li> <p>Limit key access</p> </li> <li>Only share <code>MASTER_KEY</code> with admins</li> <li>End users should NEVER see <code>MASTER_KEY</code> or <code>LITELLM_MASTER_KEY</code></li> <li> <p>Use virtual keys for team access</p> </li> <li> <p>Monitor for unauthorized access</p> </li> <li>Check logs for 401 errors</li> <li>Monitor unusual API usage patterns</li> <li>Set up alerts for multiple failed auth attempts</li> </ol>"},{"location":"deployment/integration-flow/","title":"Complete Integration Flow","text":"<p>This guide explains how all components of SaasLiteLLM work together, from admin setup to end-user requests.</p>"},{"location":"deployment/integration-flow/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        ADMIN SETUP PHASE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  1. Admin \u2192 LiteLLM UI (Port 8002)                              \u2502\n\u2502     - Add provider credentials (OpenAI, Anthropic, etc.)        \u2502\n\u2502     - Configure models (gpt-4, claude-3-sonnet, etc.)           \u2502\n\u2502     - Test models in playground                                  \u2502\n\u2502                                                                  \u2502\n\u2502  2. Admin \u2192 SaaS Admin Panel (Port 3000)                        \u2502\n\u2502     - Create organizations                                       \u2502\n\u2502     - Create model groups (referencing LiteLLM models)          \u2502\n\u2502     - Create teams with budgets                                  \u2502\n\u2502     - Assign model groups to teams                               \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     END USER REQUEST PHASE                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  Client Application                                              \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (1) Uses team's virtual_key                           \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  SaaS API (Port 8003)                                           \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (2) Validates key, checks credits, tracks job         \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  LiteLLM Proxy (Port 8002)                                      \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (3) Routes to provider, tracks costs                  \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  LLM Provider (OpenAI, Anthropic, etc.)                         \u2502\n\u2502         \u2502                                                         \u2502\n\u2502         \u2502 (4) Returns completion                                 \u2502\n\u2502         \u25bc                                                         \u2502\n\u2502  Response flows back to client                                   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/integration-flow/#authentication-keys","title":"Authentication Keys","text":"<p>Understanding the different keys is crucial:</p>"},{"location":"deployment/integration-flow/#1-master_key-saas-api-admin-key","title":"1. MASTER_KEY (SaaS API Admin Key)","text":"<p>Purpose: Admin access to SaaS API management endpoints</p> <p>Used for: - Creating organizations - Creating teams - Creating model groups - Adding credits - Viewing usage reports</p> <p>Used by: System administrators via admin panel or API calls</p> <p>Configured in: <code>saas-api/.env</code> \u2192 <code>MASTER_KEY</code></p> <p>Example: <pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"X-Admin-Key: sk-admin-your-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Acme Corp\", \"litellm_organization_id\": \"acme\"}'\n</code></pre></p> <p>Security: - \u26a0\ufe0f Never expose to end users - Store securely (environment variables, secrets manager) - Rotate quarterly - Different from LITELLM_MASTER_KEY</p>"},{"location":"deployment/integration-flow/#2-litellm_master_key-litellm-admin-key","title":"2. LITELLM_MASTER_KEY (LiteLLM Admin Key)","text":"<p>Purpose: Admin access to LiteLLM proxy management</p> <p>Used for: - Accessing LiteLLM UI - SaaS API creating virtual keys for teams - Managing models and credentials in LiteLLM - Direct LiteLLM API access (admin only)</p> <p>Used by: - Admins accessing LiteLLM UI - SaaS API (programmatically)</p> <p>Configured in: Both <code>saas-api/.env</code> and <code>litellm/.env</code> \u2192 <code>LITELLM_MASTER_KEY</code></p> <p>Example: <pre><code># Access LiteLLM UI\nhttp://localhost:8002/ui\n# Login with: LITELLM_MASTER_KEY value\n\n# Direct API call\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer sk-litellm-your-master-key\" \\\n  -d '{\"model\": \"gpt-3.5-turbo\", \"messages\": [...]}'\n</code></pre></p> <p>Security: - \u26a0\ufe0f Never expose to end users - Must be the same in both SaaS API and LiteLLM .env files - Used by SaaS API to create team virtual keys - Rotate quarterly</p>"},{"location":"deployment/integration-flow/#3-virtual-keys-team-keys","title":"3. Virtual Keys (Team Keys)","text":"<p>Purpose: Team-specific access to LiteLLM with budget/model controls</p> <p>Used for: - End users making LLM requests - Enforcing budget limits per team - Restricting model access per team - Tracking usage per team</p> <p>Created by: SaaS API automatically when creating a team</p> <p>Used by: End-user applications (your SaaS customers)</p> <p>Example: <pre><code># Your customer's application uses their team's virtual key\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer sk-litellm-virtual-key-team-abc-123\" \\\n  -d '{\"model\": \"gpt-3.5-turbo\", \"messages\": [...]}'\n</code></pre></p> <p>Security: - \u2705 Safe to give to end users - Budget-limited (can't overspend) - Model-limited (only assigned models) - Rate-limited (RPM/TPM limits) - Tracked per team</p>"},{"location":"deployment/integration-flow/#setup-flow-admin-perspective","title":"Setup Flow (Admin Perspective)","text":""},{"location":"deployment/integration-flow/#phase-1-infrastructure-setup","title":"Phase 1: Infrastructure Setup","text":"<ol> <li>Deploy services (Docker Compose, Railway, etc.)</li> <li>PostgreSQL database</li> <li>Redis cache</li> <li>LiteLLM proxy</li> <li>SaaS API</li> <li> <p>Admin panel</p> </li> <li> <p>Configure environment variables:    <pre><code># Generate strong keys\nMASTER_KEY=$(openssl rand -hex 32)\nLITELLM_MASTER_KEY=$(openssl rand -hex 32)\n\n# Set in .env files\n# saas-api/.env\nMASTER_KEY=sk-admin-...\nLITELLM_MASTER_KEY=sk-litellm-...\n\n# litellm/.env\nLITELLM_MASTER_KEY=sk-litellm-...  # Same as SaaS API\n</code></pre></p> </li> <li> <p>Verify services are running:    <pre><code>docker ps\n# Should see: postgres, redis, litellm-proxy, saas-api, admin-panel\n</code></pre></p> </li> </ol>"},{"location":"deployment/integration-flow/#phase-2-litellm-configuration-critical","title":"Phase 2: LiteLLM Configuration (CRITICAL)","text":"<p>This must be done BEFORE creating teams in SaaS API</p> <ol> <li>Access LiteLLM UI: http://localhost:8002/ui</li> <li> <p>Login with <code>LITELLM_MASTER_KEY</code></p> </li> <li> <p>Add Provider Credentials:</p> </li> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li> <p>For each provider you want to use:      <pre><code>OpenAI:\n  - Key Alias: openai-prod\n  - Provider: openai\n  - API Key: sk-... (from OpenAI dashboard)\n\nAnthropic:\n  - Key Alias: anthropic-prod\n  - Provider: anthropic\n  - API Key: sk-ant-... (from Anthropic console)\n</code></pre></p> </li> <li> <p>Add Models:</p> </li> <li>Navigate to Models tab</li> <li>Click + Add Model</li> <li> <p>For each model you want to offer:      <pre><code>Example 1:\n  - Model Name: gpt-3.5-turbo\n  - LiteLLM Model Name: openai/gpt-3.5-turbo\n  - Credential: openai-prod\n  - Cost per 1K input tokens: 0.0005\n  - Cost per 1K output tokens: 0.0015\n\nExample 2:\n  - Model Name: claude-3-sonnet\n  - LiteLLM Model Name: anthropic/claude-3-sonnet-20240229\n  - Credential: anthropic-prod\n  - Cost per 1K input tokens: 0.003\n  - Cost per 1K output tokens: 0.015\n</code></pre></p> </li> <li> <p>Test Models:</p> </li> <li>Navigate to Playground tab</li> <li>Select each model</li> <li>Send test message: \"Hello, are you working?\"</li> <li>Verify responses</li> </ol> <p>\u2705 Checkpoint: All models should respond successfully in playground</p>"},{"location":"deployment/integration-flow/#phase-3-saas-api-configuration","title":"Phase 3: SaaS API Configuration","text":"<ol> <li>Access Admin Panel: http://localhost:3000</li> <li> <p>Login with <code>MASTER_KEY</code> (from SaaS API .env)</p> </li> <li> <p>Create Organization:    <pre><code>- Name: Acme Corporation\n- LiteLLM Org ID: acme-corp\n- Description: Main organization\n</code></pre></p> </li> <li> <p>Create Model Groups:    <pre><code>Group 1 - Basic Tier:\n  - Name: basic-models\n  - Models: gpt-3.5-turbo\n  - Description: Fast, cost-effective models\n\nGroup 2 - Premium Tier:\n  - Name: premium-models\n  - Models: gpt-4-turbo, claude-3-sonnet, claude-3-opus\n  - Description: Most capable models\n</code></pre></p> </li> </ol> <p>\u26a0\ufe0f Important: Model names must match EXACTLY what you configured in LiteLLM</p> <ol> <li> <p>Create Team:    <pre><code>- Name: Acme Dev Team\n- Organization: Acme Corporation\n- Model Group: basic-models\n- Max Budget: 100.0 (USD/credits)\n- Budget Duration: 30d\n- Rate Limits:\n  - RPM: 100\n  - TPM: 50000\n</code></pre></p> </li> <li> <p>Get Team Virtual Key:</p> </li> <li>After creating team, view team details</li> <li>Copy the <code>virtual_key</code> value</li> <li>This is what your customer will use</li> </ol> <p>\u2705 Checkpoint: Team created successfully with virtual_key</p>"},{"location":"deployment/integration-flow/#request-flow-end-user-perspective","title":"Request Flow (End User Perspective)","text":""},{"location":"deployment/integration-flow/#step-1-client-gets-team-credentials","title":"Step 1: Client Gets Team Credentials","text":"<p>Your SaaS application provides the team's virtual key to their application:</p> <pre><code>// Your application provides these to the customer\nconst teamVirtualKey = \"sk-litellm-virtual-key-abc123\";\nconst apiEndpoint = \"http://localhost:8002\"; // LiteLLM proxy URL\n</code></pre>"},{"location":"deployment/integration-flow/#step-2-client-makes-llm-request","title":"Step 2: Client Makes LLM Request","text":"<p>The customer's application makes requests directly to LiteLLM:</p> <pre><code>const response = await fetch('http://localhost:8002/chat/completions', {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${teamVirtualKey}`,\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'gpt-3.5-turbo',\n    messages: [\n      { role: 'user', content: 'Hello, world!' }\n    ]\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.choices[0].message.content);\n</code></pre>"},{"location":"deployment/integration-flow/#step-3-litellm-processes-request","title":"Step 3: LiteLLM Processes Request","text":"<p>Behind the scenes, LiteLLM:</p> <ol> <li>Validates virtual key: Checks if key exists and is valid</li> <li>Checks team budget: Ensures team has enough credits</li> <li>Checks model access: Verifies team can use requested model</li> <li>Checks rate limits: Ensures RPM/TPM not exceeded</li> <li>Routes to provider: Forwards request to OpenAI/Anthropic/etc.</li> <li>Tracks costs: Records token usage and costs</li> <li>Deducts budget: Updates team's remaining budget</li> <li>Returns response: Sends completion back to client</li> </ol>"},{"location":"deployment/integration-flow/#step-4-saas-api-tracks-usage","title":"Step 4: SaaS API Tracks Usage","text":"<p>The SaaS API can query usage data:</p> <pre><code># Admin checks team usage\ncurl http://localhost:8003/api/teams/{team_id}/usage \\\n  -H \"X-Admin-Key: $MASTER_KEY\"\n\n# Response includes:\n{\n  \"team_id\": \"team_abc123\",\n  \"total_spend\": 15.50,\n  \"remaining_budget\": 84.50,\n  \"request_count\": 1234,\n  \"models_used\": {\n    \"gpt-3.5-turbo\": { \"requests\": 1000, \"cost\": 10.00 },\n    \"gpt-4\": { \"requests\": 234, \"cost\": 5.50 }\n  }\n}\n</code></pre>"},{"location":"deployment/integration-flow/#budget-management","title":"Budget Management","text":""},{"location":"deployment/integration-flow/#how-credits-work","title":"How Credits Work","text":"<ol> <li> <p>Admin adds credits to team:    <pre><code>curl -X POST http://localhost:8003/api/credits/teams/{team_id}/add \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -d '{\"amount\": 100.0, \"reason\": \"Monthly allocation\"}'\n</code></pre></p> </li> <li> <p>LiteLLM tracks spending:</p> </li> <li>Every request deducts from team budget</li> <li>Cost = (input_tokens * input_cost) + (output_tokens * output_cost)</li> <li> <p>Stored in LiteLLM's database</p> </li> <li> <p>When budget exhausted:</p> </li> <li>Team's virtual key stops working</li> <li>Requests return 429 error (quota exceeded)</li> <li>Admin must add more credits</li> </ol>"},{"location":"deployment/integration-flow/#budget-modes","title":"Budget Modes","text":"<p>Teams can have different budget enforcement modes:</p> <p>Hard Limit (default): - Requests blocked when budget reached - Team cannot exceed budget under any circumstance</p> <p>Soft Limit: - Warning sent when 80% budget used - Requests still allowed after budget reached - Useful for enterprise customers with invoicing</p>"},{"location":"deployment/integration-flow/#model-access-control","title":"Model Access Control","text":""},{"location":"deployment/integration-flow/#how-model-groups-work","title":"How Model Groups Work","text":"<ol> <li> <p>Admin creates model group in SaaS API:    <pre><code>Model Group: \"premium-models\"\nModels: [\"gpt-4-turbo\", \"claude-3-opus\"]\n</code></pre></p> </li> <li> <p>Admin assigns to team:</p> </li> <li>Team's virtual key is configured in LiteLLM</li> <li> <p>Only specified models are accessible</p> </li> <li> <p>User requests model:    <pre><code>// \u2705 Allowed (model in group)\n{ model: \"gpt-4-turbo\", messages: [...] }\n\n// \u274c Denied (model not in group)\n{ model: \"gpt-3.5-turbo\", messages: [...] }\n// Returns 403 Forbidden\n</code></pre></p> </li> </ol>"},{"location":"deployment/integration-flow/#changing-model-access","title":"Changing Model Access","text":"<p>To change a team's model access:</p> <ol> <li>Update the model group assignment in SaaS API admin panel</li> <li>Or create a new model group and reassign the team</li> <li>Changes take effect immediately</li> <li>No need to regenerate virtual keys</li> </ol>"},{"location":"deployment/integration-flow/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/integration-flow/#litellm-ui-dashboard","title":"LiteLLM UI Dashboard","text":"<p>Access: http://localhost:8002/ui</p> <p>Available views: - Usage by team - Cost breakdown by model - Request counts and latencies - Error rates - Budget remaining per team</p>"},{"location":"deployment/integration-flow/#saas-api-endpoints","title":"SaaS API Endpoints","text":"<p>Organization usage: <pre><code>GET /api/organizations/{org_id}/usage\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p> <p>Team usage: <pre><code>GET /api/teams/{team_id}/usage\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p> <p>All teams usage: <pre><code>GET /api/teams\nHeaders: X-Admin-Key: $MASTER_KEY\n</code></pre></p>"},{"location":"deployment/integration-flow/#logs","title":"Logs","text":"<p>LiteLLM logs: <pre><code>docker logs litellm-proxy\n# Shows: requests, costs, errors, rate limits\n</code></pre></p> <p>SaaS API logs: <pre><code>docker logs saas-api\n# Shows: admin actions, team creation, credit additions\n</code></pre></p>"},{"location":"deployment/integration-flow/#production-deployment","title":"Production Deployment","text":""},{"location":"deployment/integration-flow/#environment-configuration","title":"Environment Configuration","text":"<p>Railway (recommended):</p> <ol> <li>Deploy LiteLLM:</li> <li>Expose public URL for admin access</li> <li>Use private networking for SaaS API connection</li> <li> <p>Set <code>LITELLM_MASTER_KEY</code> in Railway secrets</p> </li> <li> <p>Deploy SaaS API:</p> </li> <li>Private networking to LiteLLM</li> <li>Set <code>MASTER_KEY</code> and <code>LITELLM_MASTER_KEY</code></li> <li> <p>Set <code>LITELLM_PROXY_URL</code> to LiteLLM's internal URL</p> </li> <li> <p>Deploy Admin Panel:</p> </li> <li>Public URL for admin access</li> <li>Set <code>NEXT_PUBLIC_API_URL</code> to SaaS API URL</li> </ol>"},{"location":"deployment/integration-flow/#security-checklist","title":"Security Checklist","text":"<ul> <li> Changed default <code>MASTER_KEY</code> to strong random value</li> <li> Changed default <code>LITELLM_MASTER_KEY</code> to strong random value</li> <li> LiteLLM UI accessible only via HTTPS</li> <li> Admin panel accessible only via HTTPS</li> <li> Rate limiting enabled in LiteLLM</li> <li> Budget limits set on all teams</li> <li> Monitoring/alerting configured</li> <li> Database backups enabled</li> <li> Provider API keys stored securely</li> <li> No keys committed to git</li> </ul>"},{"location":"deployment/integration-flow/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"deployment/integration-flow/#issue-model-not-found","title":"Issue: \"Model not found\"","text":"<p>Cause: Model name mismatch between SaaS API and LiteLLM</p> <p>Solution: 1. Check model name in SaaS API model group 2. Check model name in LiteLLM UI Models tab 3. Ensure they match exactly (case-sensitive)</p>"},{"location":"deployment/integration-flow/#issue-invalid-api-key","title":"Issue: \"Invalid API key\"","text":"<p>Cause: Team virtual key not working</p> <p>Solution: 1. Verify team exists and has budget 2. Check LiteLLM UI Keys tab for the virtual key 3. Ensure model group is assigned to team 4. Try recreating the team</p>"},{"location":"deployment/integration-flow/#issue-quota-exceeded","title":"Issue: \"Quota exceeded\"","text":"<p>Cause: Team has exhausted budget</p> <p>Solution: 1. Check team's remaining budget in admin panel 2. Add more credits via admin panel 3. Or adjust budget limits for the team</p>"},{"location":"deployment/integration-flow/#issue-saas-api-cant-connect-to-litellm","title":"Issue: SaaS API can't connect to LiteLLM","text":"<p>Cause: Network configuration problem</p> <p>Solution: 1. Verify <code>LITELLM_PROXY_URL</code> in SaaS API .env 2. Check both services on same Docker network 3. Test connectivity: <code>curl http://litellm-proxy:8002/health</code> 4. Verify <code>LITELLM_MASTER_KEY</code> matches in both services</p>"},{"location":"deployment/integration-flow/#next-steps","title":"Next Steps","text":"<ul> <li>LiteLLM Setup Guide - Detailed LiteLLM configuration</li> <li>API Reference - Complete API documentation</li> <li>Deployment Guide - Production deployment</li> <li>Security Documentation - Security best practices</li> </ul>"},{"location":"deployment/litellm-setup/","title":"LiteLLM Proxy Setup Guide","text":"<p>This guide explains how to configure and integrate the LiteLLM proxy with the SaaS API.</p>"},{"location":"deployment/litellm-setup/#critical-setup-requirement","title":"\u26a0\ufe0f Critical Setup Requirement","text":"<p>You MUST configure LiteLLM with provider credentials BEFORE your SaaS API will work.</p> <p>The LiteLLM proxy is where you: 1. Store your LLM provider API keys (OpenAI, Anthropic, etc.) 2. Configure which models are available 3. Set up model parameters and routing</p> <p>The LiteLLM UI must be exposed so you can configure these settings. All end-user requests will go through your SaaS API, but the admin must configure models in LiteLLM first.</p>"},{"location":"deployment/litellm-setup/#overview","title":"Overview","text":"<p>SaasLiteLLM uses LiteLLM as a unified proxy layer for multiple LLM providers. LiteLLM provides:</p> <ul> <li>Unified API: Call 100+ LLM providers using the OpenAI format</li> <li>Cost Tracking: Automatic usage and cost tracking per team</li> <li>Rate Limiting: Built-in rate limiting and budget management</li> <li>Caching: Redis-based response caching for cost savings</li> <li>Load Balancing: Smart routing across multiple models</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  SaaS API   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   LiteLLM    \u2502\n\u2502 Application \u2502     \u2502 (Port 8003) \u2502     \u2502  (Port 8002) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                          \u2502               \u2502\n                    \u25bc                          \u25bc               \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  OpenAI  \u2502              \u2502 Anthropic\u2502    \u2502  Others  \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/litellm-setup/#quick-start-exposing-litellm-ui","title":"Quick Start: Exposing LiteLLM UI","text":""},{"location":"deployment/litellm-setup/#1-start-litellm-with-ui-exposed","title":"1. Start LiteLLM with UI Exposed","text":"<p>In your <code>docker-compose.yml</code>, ensure the LiteLLM proxy port is exposed:</p> <pre><code>services:\n  litellm-proxy:\n    image: ghcr.io/berriai/litellm:main-latest\n    ports:\n      - \"8002:8002\"  # This MUST be exposed\n    environment:\n      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}\n      - DATABASE_URL=${DATABASE_URL}\n      # ... other environment variables\n</code></pre>"},{"location":"deployment/litellm-setup/#2-access-the-litellm-admin-ui","title":"2. Access the LiteLLM Admin UI","text":"<p>Once LiteLLM is running, access the admin dashboard:</p> <p>Local Development: <pre><code>http://localhost:8002/ui\n</code></pre></p> <p>Production (Railway/Cloud): <pre><code>https://your-litellm-domain.railway.app/ui\n</code></pre></p> <p>Login with your MASTER_KEY: - When prompted, enter your <code>LITELLM_MASTER_KEY</code> value - This is the same key from your <code>.env</code> file</p>"},{"location":"deployment/litellm-setup/#3-security-for-production","title":"3. Security for Production","text":"<p>When exposing LiteLLM in production:</p> <ol> <li> <p>Use strong authentication: Change <code>LITELLM_MASTER_KEY</code> to a secure random key    <pre><code>openssl rand -hex 32\n</code></pre></p> </li> <li> <p>Restrict access by IP (recommended for Railway):</p> </li> <li>Use Railway's private networking when possible</li> <li> <p>Or configure firewall rules to allow only your admin IPs</p> </li> <li> <p>Use HTTPS: Always access via HTTPS in production</p> </li> <li> <p>Monitor access: Review LiteLLM logs regularly for unauthorized access attempts</p> </li> <li> <p>Separate keys: Never share <code>LITELLM_MASTER_KEY</code> with end users (they use team virtual keys)</p> </li> </ol>"},{"location":"deployment/litellm-setup/#step-by-step-adding-provider-credentials","title":"Step-by-Step: Adding Provider Credentials","text":"<p>This is the most important setup step. Without provider credentials, your SaaS API cannot make LLM requests.</p>"},{"location":"deployment/litellm-setup/#adding-openai-credentials","title":"Adding OpenAI Credentials","text":"<ol> <li>Navigate to Keys tab in LiteLLM UI</li> <li>Click + Add Key</li> <li>Fill in the details:</li> <li>Key Alias: <code>openai-main</code> (or any descriptive name)</li> <li>Provider: Select <code>openai</code></li> <li>API Key: Paste your OpenAI API key (<code>sk-...</code>)</li> <li>Metadata (optional): Add notes like \"Production OpenAI Key\"</li> <li>Click Save</li> </ol>"},{"location":"deployment/litellm-setup/#adding-anthropic-credentials","title":"Adding Anthropic Credentials","text":"<ol> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li>Fill in:</li> <li>Key Alias: <code>anthropic-main</code></li> <li>Provider: <code>anthropic</code></li> <li>API Key: Your Anthropic key (<code>sk-ant-...</code>)</li> <li>Click Save</li> </ol>"},{"location":"deployment/litellm-setup/#adding-azure-openai-credentials","title":"Adding Azure OpenAI Credentials","text":"<ol> <li>Navigate to Keys tab</li> <li>Click + Add Key</li> <li>Fill in:</li> <li>Key Alias: <code>azure-openai</code></li> <li>Provider: <code>azure</code></li> <li>API Key: Your Azure key</li> <li>API Base: <code>https://your-resource.openai.azure.com</code></li> <li>API Version: <code>2024-02-15-preview</code></li> <li>Click Save</li> </ol>"},{"location":"deployment/litellm-setup/#adding-other-provider-credentials","title":"Adding Other Provider Credentials","text":"<p>LiteLLM supports 100+ providers. For each provider:</p> <ol> <li>Keys tab \u2192 + Add Key</li> <li>Select the provider from dropdown</li> <li>Enter the required fields (varies by provider)</li> <li>Save</li> </ol> <p>Supported providers include: - Google (Vertex AI, PaLM) - AWS Bedrock - Cohere - Hugging Face - Replicate - Together AI - Anyscale - And many more...</p> <p>See LiteLLM Providers for the complete list.</p>"},{"location":"deployment/litellm-setup/#step-by-step-adding-models","title":"Step-by-Step: Adding Models","text":"<p>Once you have credentials configured, you can add models.</p>"},{"location":"deployment/litellm-setup/#via-litellm-ui-recommended","title":"Via LiteLLM UI (Recommended)","text":"<ol> <li>Navigate to Models tab</li> <li>Click + Add Model</li> <li>Fill in the model configuration:</li> </ol> <p>Example: Adding GPT-4 Turbo <pre><code>Model Name: gpt-4-turbo\nLiteLLM Model Name: openai/gpt-4-turbo\nProvider Credential: openai-main (select from dropdown)\nModel Info:\n  - Mode: chat\n  - Input Cost (per 1K tokens): 0.01\n  - Output Cost (per 1K tokens): 0.03\n</code></pre></p> <p>Example: Adding Claude 3 Sonnet <pre><code>Model Name: claude-3-sonnet\nLiteLLM Model Name: anthropic/claude-3-sonnet-20240229\nProvider Credential: anthropic-main\nModel Info:\n  - Mode: chat\n  - Input Cost: 0.003\n  - Output Cost: 0.015\n</code></pre></p> <p>Example: Adding GPT-3.5 Turbo <pre><code>Model Name: gpt-3.5-turbo\nLiteLLM Model Name: openai/gpt-3.5-turbo\nProvider Credential: openai-main\nModel Info:\n  - Mode: chat\n  - Input Cost: 0.0005\n  - Output Cost: 0.0015\n</code></pre></p> <ol> <li>Click Save Model</li> <li>The model is now available for your SaaS API teams!</li> </ol>"},{"location":"deployment/litellm-setup/#model-name-format","title":"Model Name Format","text":"<p>The <code>LiteLLM Model Name</code> follows this pattern: <pre><code>&lt;provider&gt;/&lt;model-identifier&gt;\n</code></pre></p> <p>Examples: - OpenAI: <code>openai/gpt-4-turbo</code>, <code>openai/gpt-3.5-turbo</code> - Anthropic: <code>anthropic/claude-3-opus-20240229</code> - Azure: <code>azure/gpt-4</code> (requires API base configured in credentials) - Cohere: <code>cohere/command-r-plus</code> - Bedrock: <code>bedrock/anthropic.claude-v2</code></p>"},{"location":"deployment/litellm-setup/#testing-your-models","title":"Testing Your Models","text":"<p>After adding a model, test it in the LiteLLM UI:</p> <ol> <li>Navigate to Playground tab</li> <li>Select your model from the dropdown</li> <li>Enter a test prompt: \"Hello, are you working?\"</li> <li>Click Send</li> <li>Verify you get a response</li> </ol> <p>If you get an error: - Check that the provider credential is correct - Verify the model name format - Ensure your provider API key has access to that model - Review the error message in the LiteLLM logs</p>"},{"location":"deployment/litellm-setup/#authentication-flow","title":"Authentication Flow","text":""},{"location":"deployment/litellm-setup/#litellm_master_key","title":"LITELLM_MASTER_KEY","text":"<p>The <code>LITELLM_MASTER_KEY</code> is used for:</p> <ol> <li>Admin access to LiteLLM's management UI (<code>http://localhost:8002/ui</code>)</li> <li>SaaS API authentication when creating virtual keys for teams</li> <li>Direct LiteLLM API access (not recommended for end users)</li> </ol> <pre><code># In your .env file\nLITELLM_MASTER_KEY=sk-litellm-your-super-secure-key-here\n</code></pre>"},{"location":"deployment/litellm-setup/#virtual-team-keys","title":"Virtual Team Keys","text":"<p>The SaaS API creates virtual keys for each team automatically:</p> <ul> <li>Teams don't use <code>LITELLM_MASTER_KEY</code> directly</li> <li>Each team gets a unique key with budget/rate limits enforced</li> <li>Keys are managed through the SaaS API, not LiteLLM directly</li> </ul>"},{"location":"deployment/litellm-setup/#environment-configuration","title":"Environment Configuration","text":"<p>Set these required variables in your <code>.env</code> file:</p> <pre><code># LiteLLM Configuration\nLITELLM_MASTER_KEY=sk-litellm-$(openssl rand -hex 32)\nLITELLM_PROXY_URL=http://localhost:8002\nLITELLM_CONFIG_PATH=src/config/litellm_config.yaml\n\n# Database (shared with SaaS API and LiteLLM)\nDATABASE_URL=postgresql://postgres:postgres@localhost:5432/saas_llm_db\n\n# Redis (for caching and rate limiting)\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=your-redis-password\n</code></pre> <p>Note: Provider API keys (OpenAI, Anthropic, etc.) are added through the LiteLLM UI, not as environment variables. This keeps credentials secure and allows dynamic management without restarts.</p>"},{"location":"deployment/litellm-setup/#complete-setup-workflow","title":"Complete Setup Workflow","text":"<p>Here's the complete workflow from zero to working SaaS LLM API:</p>"},{"location":"deployment/litellm-setup/#step-1-start-services","title":"Step 1: Start Services","text":"<pre><code>docker-compose up -d\n</code></pre> <p>Wait for all services to be healthy.</p>"},{"location":"deployment/litellm-setup/#step-2-configure-litellm-required","title":"Step 2: Configure LiteLLM (REQUIRED)","text":"<ol> <li>Access LiteLLM UI: http://localhost:8002/ui</li> <li>Login with your <code>LITELLM_MASTER_KEY</code></li> <li>Add Provider Credentials:</li> <li>Go to Keys tab</li> <li>Add OpenAI key (or your preferred provider)</li> <li>Example: Alias=<code>openai-main</code>, Provider=<code>openai</code>, Key=<code>sk-...</code></li> <li>Add Models:</li> <li>Go to Models tab</li> <li>Add at least one model</li> <li>Example: Name=<code>gpt-3.5-turbo</code>, LiteLLM Name=<code>openai/gpt-3.5-turbo</code>, Credential=<code>openai-main</code></li> <li>Test Model:</li> <li>Go to Playground tab</li> <li>Select your model and send a test message</li> <li>Verify you get a response</li> </ol>"},{"location":"deployment/litellm-setup/#step-3-configure-saas-api","title":"Step 3: Configure SaaS API","text":"<ol> <li>Access Admin Panel: http://localhost:3000</li> <li>Login with your <code>MASTER_KEY</code> (from SaaS API .env)</li> <li>Create Organization:</li> <li>Go to Organizations</li> <li>Create your first organization</li> <li>Create Model Group:</li> <li>Go to Model Groups</li> <li>Create a group with the models you added in LiteLLM</li> <li>Example: Name=<code>standard-models</code>, Models=<code>gpt-3.5-turbo, claude-3-sonnet</code></li> <li>Create Team:</li> <li>Go to Teams</li> <li>Create a team under your organization</li> <li>Assign the model group</li> <li>Set budget (credits)</li> </ol>"},{"location":"deployment/litellm-setup/#step-4-test-end-to-end","title":"Step 4: Test End-to-End","text":"<p>Your team now has a virtual key. Test the complete flow:</p> <pre><code># Get team info (includes virtual_key)\ncurl http://localhost:8003/api/teams/{team_id} \\\n  -H \"X-Admin-Key: $MASTER_KEY\"\n\n# Use team's virtual key to make LLM request\ncurl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $TEAM_VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre> <p>\u2705 If you get a response, your setup is complete!</p>"},{"location":"deployment/litellm-setup/#model-groups-saas-api-feature","title":"Model Groups (SaaS API Feature)","text":"<p>The SaaS API adds Model Groups on top of LiteLLM models:</p> <ul> <li>Group models together: Create logical groupings like \"fast-models\", \"premium-models\"</li> <li>Control team access: Assign specific model groups to teams</li> <li>Flexible pricing: Different teams can have different model access</li> </ul> <p>Important: Model names in your Model Groups must match the model names you configured in LiteLLM.</p> <p>Example workflow:</p> <ol> <li>\u2705 Add models in LiteLLM UI (e.g., <code>gpt-4-turbo</code>, <code>claude-3-sonnet</code>)</li> <li>Create model groups in SaaS API admin panel</li> <li>Add the same model names to your groups</li> <li>Assign groups to teams</li> <li>Teams can now use those models via their virtual keys</li> </ol>"},{"location":"deployment/litellm-setup/#testing-the-setup","title":"Testing the Setup","text":""},{"location":"deployment/litellm-setup/#1-test-litellm-directly","title":"1. Test LiteLLM Directly","text":"<p>Test that LiteLLM can reach your providers:</p> <pre><code>curl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $LITELLM_MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"deployment/litellm-setup/#2-test-saas-api-litellm-connection","title":"2. Test SaaS API \u2192 LiteLLM Connection","text":"<p>Create a test organization and team:</p> <pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Test Org\",\n    \"litellm_organization_id\": \"test-org\"\n  }'\n\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"X-Admin-Key: $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Test Team\",\n    \"organization_id\": \"org_...\",\n    \"litellm_team_id\": \"test-team\",\n    \"max_budget\": 100.0\n  }'\n</code></pre> <p>The SaaS API will create a virtual key in LiteLLM for the team.</p>"},{"location":"deployment/litellm-setup/#3-test-with-team-key","title":"3. Test with Team Key","text":"<p>Use the team's virtual key to make requests:</p> <pre><code>curl -X POST http://localhost:8002/chat/completions \\\n  -H \"Authorization: Bearer $TEAM_VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from team!\"}]\n  }'\n</code></pre>"},{"location":"deployment/litellm-setup/#configuration-reference","title":"Configuration Reference","text":""},{"location":"deployment/litellm-setup/#litellm_configyaml-structure","title":"litellm_config.yaml Structure","text":"<pre><code># Model definitions (if not using database)\nmodel_list: []\n\n# General settings\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n  database_url: os.environ/DATABASE_URL\n  store_model_in_db: true  # Use database for model management\n\n  # Redis caching\n  cache: true\n  cache_params:\n    type: \"redis\"\n    host: os.environ/REDIS_HOST\n    port: os.environ/REDIS_PORT\n    password: os.environ/REDIS_PASSWORD\n    ttl: 600  # Cache TTL in seconds\n\n  # Cost tracking\n  track_cost_per_deployment: true\n\n# Router settings\nrouter_settings:\n  redis_host: os.environ/REDIS_HOST\n  redis_port: os.environ/REDIS_PORT\n  redis_password: os.environ/REDIS_PASSWORD\n  enable_pre_call_checks: true\n  routing_strategy: \"usage-based-routing\"\n  num_retries: 3\n  timeout: 600\n  redis_enabled: true\n</code></pre>"},{"location":"deployment/litellm-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/litellm-setup/#cant-access-litellm-ui","title":"Can't Access LiteLLM UI","text":"<p>Symptoms: Cannot reach http://localhost:8002/ui</p> <p>Solutions:</p> <ol> <li> <p>Verify LiteLLM container is running:    <pre><code>docker ps | grep litellm\n</code></pre></p> </li> <li> <p>Check that port 8002 is exposed in docker-compose.yml:    <pre><code>ports:\n  - \"8002:8002\"\n</code></pre></p> </li> <li> <p>View LiteLLM logs for errors:    <pre><code>docker logs litellm-proxy\n</code></pre></p> </li> <li> <p>Ensure no other service is using port 8002:    <pre><code>lsof -i :8002\n</code></pre></p> </li> </ol>"},{"location":"deployment/litellm-setup/#litellm-ui-login-failing","title":"LiteLLM UI Login Failing","text":"<p>Symptoms: Invalid credentials error when logging in</p> <p>Solutions:</p> <ol> <li>Verify you're using <code>LITELLM_MASTER_KEY</code> (not <code>MASTER_KEY</code>)</li> <li>Check the key value in your <code>.env</code> file</li> <li>Ensure the key doesn't have extra whitespace or quotes</li> <li>Try restarting LiteLLM: <code>docker-compose restart litellm-proxy</code></li> </ol>"},{"location":"deployment/litellm-setup/#provider-credential-not-working","title":"Provider Credential Not Working","text":"<p>Symptoms: \"Invalid API key\" errors when testing models</p> <p>Solutions:</p> <ol> <li>Verify the key is valid:</li> <li>Test directly with the provider (e.g., OpenAI playground)</li> <li>Check if the key has been revoked</li> <li> <p>Ensure you have sufficient credits with the provider</p> </li> <li> <p>Check key format:</p> </li> <li>OpenAI: Should start with <code>sk-...</code></li> <li>Anthropic: Should start with <code>sk-ant-...</code></li> <li> <p>Azure: Varies by deployment</p> </li> <li> <p>Verify provider selection:</p> </li> <li>Make sure you selected the correct provider in the dropdown</li> <li> <p>Provider name must match exactly (case-sensitive)</p> </li> <li> <p>Check API base (for Azure/custom endpoints):</p> </li> <li>Ensure the API base URL is correct</li> <li> <p>Verify the API version is supported</p> </li> <li> <p>Review LiteLLM logs:    <pre><code>docker logs litellm-proxy | grep -i error\n</code></pre></p> </li> </ol>"},{"location":"deployment/litellm-setup/#model-not-working","title":"Model Not Working","text":"<p>Symptoms: \"Model not found\" or model requests failing</p> <p>Solutions:</p> <ol> <li>Verify model exists in LiteLLM:</li> <li>Check the Models tab in LiteLLM UI</li> <li> <p>Ensure the model is saved (not just added)</p> </li> <li> <p>Check model name format:</p> </li> <li>Must use provider prefix: <code>openai/gpt-4-turbo</code> (not just <code>gpt-4-turbo</code>)</li> <li> <p>Check for typos (case-sensitive)</p> </li> <li> <p>Verify credential is linked:</p> </li> <li>Each model must be linked to a provider credential</li> <li> <p>The credential must be valid</p> </li> <li> <p>Test in playground:</p> </li> <li>Use LiteLLM's Playground tab</li> <li>Try a simple prompt</li> <li> <p>Review error messages</p> </li> <li> <p>Check provider access:</p> </li> <li>Ensure your API key has access to that specific model</li> <li>Some models require special access (e.g., GPT-4, Claude Opus)</li> </ol>"},{"location":"deployment/litellm-setup/#saas-api-cant-reach-litellm","title":"SaaS API Can't Reach LiteLLM","text":"<p>Symptoms: Errors when creating teams or model groups</p> <p>Solutions:</p> <ol> <li> <p>Verify <code>LITELLM_PROXY_URL</code> in SaaS API <code>.env</code>:    <pre><code># Should be\nLITELLM_PROXY_URL=http://localhost:8002\n# Or for Docker network\nLITELLM_PROXY_URL=http://litellm-proxy:8002\n</code></pre></p> </li> <li> <p>Test connectivity from SaaS API container:    <pre><code>docker exec saas-api curl http://litellm-proxy:8002/health\n</code></pre></p> </li> <li> <p>Check both services are on the same Docker network:    <pre><code>docker network inspect saas-litellm_default\n</code></pre></p> </li> <li> <p>Verify <code>LITELLM_MASTER_KEY</code> matches in both services' .env files</p> </li> </ol>"},{"location":"deployment/litellm-setup/#virtual-keys-not-working","title":"Virtual Keys Not Working","text":"<p>Symptoms: Teams can't use their assigned virtual keys</p> <p>Solutions:</p> <ol> <li>Verify key was created:</li> <li>Check LiteLLM UI Keys tab</li> <li> <p>Look for keys with team_id in metadata</p> </li> <li> <p>Check team budget:</p> </li> <li>View team details in SaaS API admin panel</li> <li>Verify credits haven't been exhausted</li> <li> <p>Check budget limits in LiteLLM</p> </li> <li> <p>Verify model access:</p> </li> <li>Ensure team's model group includes the requested model</li> <li> <p>Check model name matches exactly what's in LiteLLM</p> </li> <li> <p>Review rate limits:</p> </li> <li>Check if team has hit rate limits (RPM/TPM)</li> <li> <p>View limits in LiteLLM team configuration</p> </li> <li> <p>Check LiteLLM logs:    <pre><code>docker logs litellm-proxy | grep team_id\n</code></pre></p> </li> </ol>"},{"location":"deployment/litellm-setup/#model-group-mismatch","title":"Model Group Mismatch","text":"<p>Symptoms: Team can't access models despite having a model group assigned</p> <p>Solutions:</p> <ol> <li>Verify model names match:</li> <li>Model names in SaaS API Model Group MUST match LiteLLM model names</li> <li> <p>Check for typos, extra spaces, or case differences</p> </li> <li> <p>Confirm models exist in LiteLLM:</p> </li> <li>Open LiteLLM UI \u2192 Models tab</li> <li> <p>Verify each model in your group exists</p> </li> <li> <p>Re-create the team (if needed):</p> </li> <li>This refreshes the virtual key configuration in LiteLLM</li> <li>Ensures latest model group is applied</li> </ol>"},{"location":"deployment/litellm-setup/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never expose <code>LITELLM_MASTER_KEY</code> to end users</li> <li>Use strong, random keys for production:    <pre><code>openssl rand -hex 32\n</code></pre></li> <li>Rotate keys regularly (quarterly recommended)</li> <li>Set appropriate budgets on all teams to prevent runaway costs</li> <li>Monitor usage via LiteLLM dashboard</li> <li>Use rate limits to prevent abuse</li> <li>Enable Redis caching to reduce costs</li> </ol>"},{"location":"deployment/litellm-setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>LiteLLM Documentation</li> <li>LiteLLM Supported Providers</li> <li>LiteLLM GitHub Repository</li> <li>OpenAI API Reference</li> <li>Anthropic Claude API</li> </ul>"},{"location":"deployment/litellm-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Integration Guide - Complete authentication and connection flow</li> <li>API Reference - SaaS API endpoints</li> <li>Deployment Guide - Deploy to production</li> </ul>"},{"location":"deployment/local-development/","title":"Local Development","text":"<p>Run SaaS LiteLLM locally for development and testing.</p>"},{"location":"deployment/local-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>PostgreSQL 14+</li> <li>Git</li> </ul>"},{"location":"deployment/local-development/#quick-start","title":"Quick Start","text":""},{"location":"deployment/local-development/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/GittieLabs/SaasLiteLLM.git\ncd SaasLiteLLM\n</code></pre>"},{"location":"deployment/local-development/#2-set-up-database","title":"2. Set Up Database","text":"<pre><code># Start PostgreSQL (if using Docker)\ndocker run --name saas-postgres \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_DB=saas_llm_db \\\n  -p 5432:5432 \\\n  -d postgres:14\n\n# Run migrations\nPGPASSWORD=postgres psql -h localhost -U postgres -d saas_llm_db \\\n  -f scripts/migrations/001_initial_schema.sql\n# Run all subsequent migrations...\n</code></pre>"},{"location":"deployment/local-development/#3-configure-environment","title":"3. Configure Environment","text":"<p>Create <code>.env</code> file:</p> <pre><code># Database\nDATABASE_URL=postgresql://postgres:postgres@localhost:5432/saas_llm_db\n\n# LiteLLM Configuration\nLITELLM_PROXY_BASE_URL=http://localhost:4000\n\n# Master Key (for admin operations)\nMASTER_KEY=sk-your-master-key-here\n\n# OpenAI API Key (or other provider keys)\nOPENAI_API_KEY=sk-your-openai-key\n</code></pre>"},{"location":"deployment/local-development/#4-install-dependencies","title":"4. Install Dependencies","text":"<pre><code># Using pip\npip install -e .\n\n# Or using uv (faster)\nuv pip install -e .\n</code></pre>"},{"location":"deployment/local-development/#5-start-services","title":"5. Start Services","text":"<p>Terminal 1: LiteLLM Proxy <pre><code>litellm --config litellm_config.yaml --port 4000\n</code></pre></p> <p>Terminal 2: SaaS API <pre><code>python -m uvicorn src.saas_api:app --host 0.0.0.0 --port 8003 --reload\n</code></pre></p> <p>Terminal 3: Admin Panel (optional) <pre><code>cd admin-panel\nnpm install\nnpm run dev\n</code></pre></p> <p>Terminal 4: Documentation (optional) <pre><code>mkdocs serve --dev-addr 0.0.0.0:8004\n</code></pre></p>"},{"location":"deployment/local-development/#verify-installation","title":"Verify Installation","text":""},{"location":"deployment/local-development/#test-saas-api","title":"Test SaaS API","text":"<pre><code>curl http://localhost:8003/health\n</code></pre> <p>Expected response: <pre><code>{\n  \"status\": \"healthy\",\n  \"database\": \"connected\",\n  \"litellm_proxy\": \"reachable\"\n}\n</code></pre></p>"},{"location":"deployment/local-development/#create-test-organization","title":"Create Test Organization","text":"<pre><code>curl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Authorization: Bearer sk-your-master-key-here\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"org_id\": \"test-org\",\n    \"org_name\": \"Test Organization\"\n  }'\n</code></pre>"},{"location":"deployment/local-development/#create-test-team","title":"Create Test Team","text":"<pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Authorization: Bearer sk-your-master-key-here\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"test-team\",\n    \"team_name\": \"Test Team\",\n    \"org_id\": \"test-org\",\n    \"credits\": 1000\n  }'\n</code></pre> <p>Response includes virtual key: <pre><code>{\n  \"team_id\": \"test-team\",\n  \"virtual_key\": \"sk-xxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"credits\": 1000\n}\n</code></pre></p>"},{"location":"deployment/local-development/#make-test-llm-call","title":"Make Test LLM Call","text":"<pre><code># Save the virtual key from previous step\nVIRTUAL_KEY=\"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\n\n# Create job\nJOB_RESPONSE=$(curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer $VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"test-team\",\n    \"job_type\": \"test\"\n  }')\n\nJOB_ID=$(echo $JOB_RESPONSE | jq -r '.job_id')\n\n# Make LLM call\ncurl -X POST \"http://localhost:8003/api/jobs/$JOB_ID/llm-call\" \\\n  -H \"Authorization: Bearer $VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# Complete job\ncurl -X POST \"http://localhost:8003/api/jobs/$JOB_ID/complete\" \\\n  -H \"Authorization: Bearer $VIRTUAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\": \"completed\"}'\n</code></pre>"},{"location":"deployment/local-development/#development-workflow","title":"Development Workflow","text":""},{"location":"deployment/local-development/#run-tests","title":"Run Tests","text":"<pre><code># Unit tests\npytest tests/\n\n# Integration tests\npython scripts/test_full_integration.py\n</code></pre>"},{"location":"deployment/local-development/#check-code-quality","title":"Check Code Quality","text":"<pre><code># Type checking\nmypy src/\n\n# Linting\nruff check src/\n\n# Format code\nruff format src/\n</code></pre>"},{"location":"deployment/local-development/#database-migrations","title":"Database Migrations","text":"<p>Create new migration:</p> <pre><code># Create file: scripts/migrations/XXX_description.sql\ntouch scripts/migrations/010_add_new_feature.sql\n\n# Edit with SQL commands\n# Run migration\n./scripts/run_migrations.sh\n</code></pre>"},{"location":"deployment/local-development/#hot-reload","title":"Hot Reload","text":"<p>The <code>--reload</code> flag enables automatic restart when code changes: - Edit Python files in <code>src/</code> - Save changes - API automatically restarts</p>"},{"location":"deployment/local-development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/local-development/#database-connection-failed","title":"Database Connection Failed","text":"<p>Problem: <code>could not connect to database</code></p> <p>Solutions: 1. Check PostgreSQL is running: <code>docker ps</code> or <code>brew services list</code> 2. Verify credentials in <code>.env</code> 3. Check port 5432 is not in use: <code>lsof -i :5432</code></p>"},{"location":"deployment/local-development/#litellm-proxy-not-reachable","title":"LiteLLM Proxy Not Reachable","text":"<p>Problem: <code>Connection refused to localhost:4000</code></p> <p>Solutions: 1. Start LiteLLM proxy: <code>litellm --config litellm_config.yaml --port 4000</code> 2. Check <code>LITELLM_PROXY_BASE_URL</code> in <code>.env</code> 3. Verify config file exists: <code>ls litellm_config.yaml</code></p>"},{"location":"deployment/local-development/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'src'</code></p> <p>Solutions: 1. Install in editable mode: <code>pip install -e .</code> 2. Set PYTHONPATH: <code>export PYTHONPATH=/path/to/SaasLiteLLM</code></p>"},{"location":"deployment/local-development/#port-already-in-use","title":"Port Already in Use","text":"<p>Problem: <code>Address already in use: port 8003</code></p> <p>Solutions: <pre><code># Find process using port\nlsof -ti:8003\n\n# Kill process\nlsof -ti:8003 | xargs kill -9\n\n# Or use different port\nuvicorn src.saas_api:app --port 8005\n</code></pre></p>"},{"location":"deployment/local-development/#next-steps","title":"Next Steps","text":"<ul> <li>Railway Deployment - Deploy to production</li> <li>Environment Variables - Configure all settings</li> <li>Docker Setup - Use Docker for development</li> </ul>"},{"location":"deployment/railway-cors-setup/","title":"Railway CORS Setup","text":"<p>How to configure CORS in Railway using dynamic service references - no hardcoded URLs needed!</p>"},{"location":"deployment/railway-cors-setup/#the-problem-with-hardcoded-urls","title":"The Problem with Hardcoded URLs","text":"<p>You shouldn't need to know your production URLs before deploying. Railway generates URLs dynamically, and hardcoding them creates issues:</p> <ul> <li>\u274c Can't commit the code until you know the URL</li> <li>\u274c URLs change if you redeploy</li> <li>\u274c Manual updates required for each environment</li> </ul>"},{"location":"deployment/railway-cors-setup/#the-railway-solution-service-references","title":"The Railway Solution: Service References","text":"<p>Railway provides service references that automatically resolve to the correct URLs:</p> <pre><code># Instead of hardcoding:\nADMIN_PANEL_URL=https://admin-panel-production-abc123.up.railway.app\n\n# Use Railway service reference:\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>Benefits: - \u2705 No hardcoded URLs - \u2705 Automatically updates if service is redeployed - \u2705 Works immediately after deployment - \u2705 Same config across environments</p>"},{"location":"deployment/railway-cors-setup/#how-cors-works-in-saas-litellm","title":"How CORS Works in SaaS LiteLLM","text":""},{"location":"deployment/railway-cors-setup/#important-cors-is-browser-only","title":"Important: CORS is Browser-Only","text":"<p>Server-side team clients (Python, Node.js, curl) completely ignore CORS.</p> <p>CORS only affects: - Browser-based admin panel (Next.js client-side requests) - Any JavaScript running in web browsers</p> <p>Team API clients work from anywhere regardless of CORS configuration.</p> <p> Learn more about CORS vs Authentication</p>"},{"location":"deployment/railway-cors-setup/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Admin Panel (Browser-Based)                        \u2502\n\u2502  https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 Client-side fetch() calls\n                   \u2502 (Subject to CORS)\n                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SaaS API                                            \u2502\n\u2502  https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}        \u2502\n\u2502                                                      \u2502\n\u2502  CORS allows: admin-panel.RAILWAY_PUBLIC_DOMAIN     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Team Client (Python/Node.js/curl)                  \u2502\n\u2502  From anywhere (AWS, Google Cloud, etc.)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 Server-side HTTP requests\n                   \u2502 (CORS does NOT apply)\n                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SaaS API                                            \u2502\n\u2502  https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}        \u2502\n\u2502                                                      \u2502\n\u2502  Bearer token authentication (no CORS needed)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/railway-cors-setup/#railway-configuration","title":"Railway Configuration","text":""},{"location":"deployment/railway-cors-setup/#step-1-saas-api-service","title":"Step 1: SaaS API Service","text":"<p>In your saas-api service on Railway, add this environment variable:</p> <pre><code># Railway Environment Variables for saas-api service\n\n# Use Railway service reference to dynamically get admin panel URL\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>How it works: 1. Railway automatically resolves <code>${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}</code> to the actual domain 2. Example: <code>https://admin-panel-production-abc123.up.railway.app</code> 3. SaaS API reads this and adds it to CORS <code>allow_origins</code> 4. Updates automatically if admin panel is redeployed</p>"},{"location":"deployment/railway-cors-setup/#step-2-admin-panel-service","title":"Step 2: Admin Panel Service","text":"<p>In your admin-panel service on Railway, set the API URL:</p> <pre><code># Railway Environment Variables for admin-panel service\n\n# Point to SaaS API public URL\nNEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>Note: Admin panel must use the public URL because: - Browsers can't access <code>.railway.internal</code> domains - CORS requires public URLs for origin checking - Client-side Next.js code runs in the browser</p>"},{"location":"deployment/railway-cors-setup/#complete-railway-setup","title":"Complete Railway Setup","text":"<p>Here's the full environment variable configuration for Railway:</p>"},{"location":"deployment/railway-cors-setup/#saas-api-service","title":"saas-api Service","text":"<pre><code># Database\nDATABASE_URL=${{Postgres.DATABASE_URL}}\n\n# Admin Authentication\nMASTER_KEY=sk-admin-GENERATE-SECURE-KEY-HERE\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\n\n# LiteLLM Connection (use INTERNAL for lower latency)\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n\n# CORS - Admin Panel (use service reference)\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n\n# Provider Keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"deployment/railway-cors-setup/#admin-panel-service","title":"admin-panel Service","text":"<pre><code># SaaS API URL (use service reference)\nNEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre>"},{"location":"deployment/railway-cors-setup/#litellm-proxy-service","title":"litellm-proxy Service","text":"<pre><code># Database\nDATABASE_URL=${{Postgres.DATABASE_URL}}\n\n# Authentication\nLITELLM_MASTER_KEY=sk-litellm-GENERATE-SECURE-KEY-HERE\n\n# Storage\nSTORE_MODEL_IN_DB=True\n\n# Provider Keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"deployment/railway-cors-setup/#advanced-multiple-admin-panel-environments","title":"Advanced: Multiple Admin Panel Environments","text":"<p>If you have multiple environments (staging, production), use <code>ADDITIONAL_CORS_ORIGINS</code>:</p>"},{"location":"deployment/railway-cors-setup/#railway-setup","title":"Railway Setup","text":"<p>saas-api Service: <pre><code># Primary admin panel\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n\n# Additional environments (comma-separated)\nADDITIONAL_CORS_ORIGINS=https://admin-staging.yourcompany.com,https://admin-dev.yourcompany.com\n</code></pre></p> <p>How it works: - <code>ADMIN_PANEL_URL</code> adds one origin - <code>ADDITIONAL_CORS_ORIGINS</code> adds multiple (comma-separated) - All are combined into the CORS <code>allow_origins</code> list</p>"},{"location":"deployment/railway-cors-setup/#verification","title":"Verification","text":""},{"location":"deployment/railway-cors-setup/#check-cors-configuration","title":"Check CORS Configuration","text":"<p>After deploying, verify CORS is working:</p> <p>1. Check SaaS API Logs:</p> <p>Railway will show the resolved URLs in logs:</p> <pre><code># Railway Logs \u2192 saas-api\nINFO:     Started server process\nINFO:     CORS origins: ['http://localhost:3000', 'http://localhost:3001', 'http://localhost:3002', 'https://admin-panel-production-abc123.up.railway.app']\n</code></pre> <p>2. Test Admin Panel:</p> <p>Open your admin panel in the browser: <pre><code>https://admin-panel-production-abc123.up.railway.app\n</code></pre></p> <p>Try to log in: - \u2705 If login works \u2192 CORS is configured correctly - \u274c If you see CORS errors in browser console \u2192 Check configuration</p> <p>3. Test Team Client:</p> <p>Test from your development machine (server-side):</p> <pre><code>curl -X POST https://saas-api-production-abc123.up.railway.app/api/jobs/create \\\n  -H \"Authorization: Bearer sk-team-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"team_id\": \"test\", \"job_type\": \"test\"}'\n\n# \u2705 Should work - CORS doesn't apply to curl\n</code></pre>"},{"location":"deployment/railway-cors-setup/#railway-service-reference-syntax","title":"Railway Service Reference Syntax","text":"<p>Railway supports these reference patterns:</p>"},{"location":"deployment/railway-cors-setup/#public-domain","title":"Public Domain","text":"<pre><code># Service's public domain (e.g., service-name-xxx.up.railway.app)\n${{service-name.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>Use for: - Admin panel connecting to SaaS API (browser-based) - SaaS API allowing admin panel origin (CORS) - Any external access</p>"},{"location":"deployment/railway-cors-setup/#private-domain","title":"Private Domain","text":"<pre><code># Service's internal domain (e.g., service-name.railway.internal)\n${{service-name.RAILWAY_PRIVATE_DOMAIN}}\n</code></pre> <p>Use for: - Service-to-service communication within Railway - SaaS API connecting to LiteLLM proxy - Lower latency, more secure</p>"},{"location":"deployment/railway-cors-setup/#other-variables","title":"Other Variables","text":"<pre><code># Reference another service's environment variable\n${{service-name.ENV_VAR_NAME}}\n\n# Database connection string\n${{Postgres.DATABASE_URL}}\n\n# Redis connection\n${{Redis.REDIS_URL}}\n</code></pre>"},{"location":"deployment/railway-cors-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/railway-cors-setup/#issue-cors-policy-no-access-control-allow-origin-header","title":"Issue: \"CORS policy: No 'Access-Control-Allow-Origin' header\"","text":"<p>Possible causes:</p> <ol> <li>Service reference not resolving:</li> <li>Check Railway logs for the actual resolved URL</li> <li> <p>Verify <code>admin-panel</code> service name is correct</p> </li> <li> <p>Environment variable not set: <pre><code># Check in Railway dashboard:\nsaas-api \u2192 Variables \u2192 ADMIN_PANEL_URL should show the URL\n</code></pre></p> </li> <li> <p>Service not deployed:</p> </li> <li>Ensure admin-panel service deployed successfully</li> <li><code>RAILWAY_PUBLIC_DOMAIN</code> only exists after deployment</li> </ol> <p>Solution:</p> <ol> <li>Verify service names in Railway:</li> <li>Go to Railway project</li> <li>Check exact service names (case-sensitive)</li> <li> <p>Use exact name in reference: <code>${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}</code></p> </li> <li> <p>Check environment variable: <pre><code># In saas-api service variables\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n\n# Should resolve to something like:\n# https://admin-panel-production-abc123.up.railway.app\n</code></pre></p> </li> <li> <p>Redeploy saas-api:</p> </li> <li>Changes to environment variables require redeployment</li> <li>Railway \u2192 saas-api \u2192 Deploy \u2192 Redeploy</li> </ol>"},{"location":"deployment/railway-cors-setup/#issue-cannot-read-properties-of-undefined","title":"Issue: \"Cannot read properties of undefined\"","text":"<p>Cause: Service reference uses wrong service name.</p> <p>Solution: 1. Get exact service name from Railway dashboard 2. Update reference: <code>${{exact-service-name.RAILWAY_PUBLIC_DOMAIN}}</code></p>"},{"location":"deployment/railway-cors-setup/#issue-admin-panel-cant-connect-to-api","title":"Issue: Admin panel can't connect to API","text":"<p>Check:</p> <ol> <li> <p>Admin panel environment variable: <pre><code>NEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre></p> </li> <li> <p>SaaS API is deployed and running:</p> </li> <li>Check Railway \u2192 saas-api \u2192 Logs</li> <li> <p>Verify service is healthy</p> </li> <li> <p>Public domain is generated:</p> </li> <li>Railway \u2192 saas-api \u2192 Settings \u2192 Networking</li> <li>\"Generate Domain\" should show a URL</li> </ol>"},{"location":"deployment/railway-cors-setup/#best-practices","title":"Best Practices","text":""},{"location":"deployment/railway-cors-setup/#1-use-service-references-everywhere","title":"1. Use Service References Everywhere","text":"<p>\u2705 Good: <pre><code>LITELLM_PROXY_URL=http://${{litellm-proxy.RAILWAY_PRIVATE_DOMAIN}}:4000\nADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre></p> <p>\u274c Bad: <pre><code>LITELLM_PROXY_URL=http://litellm-proxy-production.up.railway.app:4000\nADMIN_PANEL_URL=https://admin-panel-production-abc123.up.railway.app\n</code></pre></p>"},{"location":"deployment/railway-cors-setup/#2-use-internal-urls-for-service-to-service","title":"2. Use Internal URLs for Service-to-Service","text":"<p>\u2705 Good: <pre><code># SaaS API \u2192 LiteLLM (internal)\nLITELLM_PROXY_URL=http://${{litellm-proxy.RAILWAY_PRIVATE_DOMAIN}}:4000\n</code></pre></p> <p>\u274c Bad: <pre><code># Using public URL (slower, unnecessary)\nLITELLM_PROXY_URL=https://${{litellm-proxy.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre></p>"},{"location":"deployment/railway-cors-setup/#3-use-public-urls-for-browser-clients","title":"3. Use Public URLs for Browser Clients","text":"<p>\u2705 Good: <pre><code># Admin panel (browser) \u2192 SaaS API\nNEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre></p> <p>\u274c Bad: <pre><code># Browsers can't access internal URLs\nNEXT_PUBLIC_API_URL=http://${{saas-api.RAILWAY_PRIVATE_DOMAIN}}:8080\n</code></pre></p>"},{"location":"deployment/railway-cors-setup/#4-document-your-service-names","title":"4. Document Your Service Names","text":"<p>Keep a reference of service names in your project:</p> <pre><code># .railway/service-names.yml\nservices:\n  saas_api: saas-api\n  admin_panel: admin-panel\n  litellm_proxy: litellm-proxy\n  postgres: Postgres\n  redis: Redis\n</code></pre>"},{"location":"deployment/railway-cors-setup/#summary","title":"Summary","text":"Configuration Railway Variable Resolved Example SaaS API CORS <code>ADMIN_PANEL_URL=https://${{admin-panel.RAILWAY_PUBLIC_DOMAIN}}</code> <code>https://admin-panel-production-abc123.up.railway.app</code> Admin Panel API <code>NEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}</code> <code>https://saas-api-production-xyz789.up.railway.app</code> SaaS \u2192 LiteLLM <code>LITELLM_PROXY_URL=http://${{litellm-proxy.RAILWAY_PRIVATE_DOMAIN}}:4000</code> <code>http://litellm-proxy.railway.internal:4000</code>"},{"location":"deployment/railway-cors-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Railway Deployment Guide - Complete deployment walkthrough</li> <li>Environment Variables Reference - All configuration options</li> <li>CORS &amp; Authentication Guide - Understand CORS in depth</li> <li>Railway Networking - Internal vs public URLs</li> </ul>"},{"location":"deployment/railway-networking/","title":"Railway Networking Guide","text":"<p>Understanding Railway's internal and public URLs for proper SaasLiteLLM deployment.</p>"},{"location":"deployment/railway-networking/#overview","title":"Overview","text":"<p>Railway provides two types of URLs for each service:</p> URL Type Format Accessibility Used For Public URL <code>https://service-name-xxx.up.railway.app</code> Internet-accessible External clients, admin panel, LiteLLM UI Internal URL <code>http://service-name.railway.internal:port</code> Railway-only Service-to-service communication <p>Critical</p> <p>Using the wrong URL type will cause connection failures!</p> <ul> <li>External clients cannot access <code>.railway.internal</code> URLs</li> <li>Internal services should use <code>.railway.internal</code> for better performance</li> </ul>"},{"location":"deployment/railway-networking/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Internet / Clients                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 Public URLs \u2502             \u2502\n            \u25bc             \u25bc             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Admin Panel \u2502  \u2502   Teams  \u2502  \u2502   Admins     \u2502\n    \u2502  (Next.js)   \u2502  \u2502  (API    \u2502  \u2502  (LiteLLM   \u2502\n    \u2502              \u2502  \u2502  Clients)\u2502  \u2502   UI)        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                \u2502               \u2502\n           \u2502 Public URL     \u2502 Public URL    \u2502 Public URL\n           \u2502                \u2502               \u2502\n           \u25bc                \u25bc               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n    \u2502   SaaS API Service               \u2502    \u2502\n    \u2502   https://saas-api-xxx.up.       \u2502    \u2502\n    \u2502   railway.app (PUBLIC)           \u2502    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                   \u2502                        \u2502\n                   \u2502 Internal URL           \u2502\n                   \u2502 (Faster, Private)      \u2502\n                   \u25bc                        \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u2500\u2518\n    \u2502   LiteLLM Proxy Service         \u2502\n    \u2502   Internal: http://litellm-     \u2502\n    \u2502   proxy.railway.internal:4000   \u2502\n    \u2502   Public: https://litellm-      \u2502\n    \u2502   proxy-xxx.up.railway.app      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u25bc\n          OpenAI/Anthropic/etc.\n</code></pre>"},{"location":"deployment/railway-networking/#service-configuration","title":"Service Configuration","text":""},{"location":"deployment/railway-networking/#1-saas-api-service","title":"1. SaaS API Service","text":"<p>Environment Variables:</p> <pre><code># Database (provided by Railway)\nDATABASE_URL=${{Postgres.DATABASE_URL}}\n\n# Admin Authentication\nMASTER_KEY=sk-admin-SECURE-KEY-HERE\nLITELLM_MASTER_KEY=sk-litellm-SECURE-KEY-HERE\n\n# LiteLLM Connection (INTERNAL URL)\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n\n# Provider Keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>Public URL: - Railway auto-generates: <code>https://saas-api-production-abc123.up.railway.app</code> - Used by: Admin panel, team clients, external apps - Enable in: Settings \u2192 Networking \u2192 Generate Domain</p> <p>Port: 8080 (or Railway's <code>PORT</code> variable)</p>"},{"location":"deployment/railway-networking/#2-litellm-proxy-service","title":"2. LiteLLM Proxy Service","text":"<p>Environment Variables:</p> <pre><code># Database (same as SaaS API)\nDATABASE_URL=${{Postgres.DATABASE_URL}}\n\n# Authentication\nLITELLM_MASTER_KEY=sk-litellm-SECURE-KEY-HERE\n\n# Storage\nSTORE_MODEL_IN_DB=True\n\n# Redis (if using)\nREDIS_HOST=${{Redis.REDISHOST}}\nREDIS_PORT=${{Redis.REDISPORT}}\nREDIS_PASSWORD=${{Redis.REDIS_PASSWORD}}\nREDIS_URL=${{Redis.REDIS_URL}}\n\n# Provider Keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>Public URL: - Railway auto-generates: <code>https://litellm-proxy-xyz789.up.railway.app</code> - Used by: Admins accessing <code>/ui</code> for model configuration - Enable in: Settings \u2192 Networking \u2192 Generate Domain</p> <p>Internal URL: - Automatically available: <code>http://litellm-proxy.railway.internal:4000</code> - Used by: SaaS API to proxy LLM requests - No configuration needed - Railway provides this automatically</p> <p>Port: 4000 (LiteLLM default)</p>"},{"location":"deployment/railway-networking/#3-admin-panel-service","title":"3. Admin Panel Service","text":"<p>Environment Variables:</p> <pre><code># SaaS API URL (PUBLIC URL - NOT internal!)\nNEXT_PUBLIC_API_URL=https://saas-api-production-abc123.up.railway.app\n\n# Or use Railway reference (recommended):\nNEXT_PUBLIC_API_URL=${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>Important: Must use the SaaS API's PUBLIC URL, not internal!</p> <p>Public URL: - Railway auto-generates: <code>https://admin-panel-def456.up.railway.app</code> - Used by: Admins to manage organizations/teams - Enable in: Settings \u2192 Networking \u2192 Generate Domain</p> <p>Port: 3000 (Next.js default)</p>"},{"location":"deployment/railway-networking/#finding-your-railway-urls","title":"Finding Your Railway URLs","text":""},{"location":"deployment/railway-networking/#method-1-railway-dashboard","title":"Method 1: Railway Dashboard","text":"<ol> <li>Go to your Railway project</li> <li>Click on a service (e.g., \"saas-api\")</li> <li>Look at the top - you'll see:    <pre><code>https://saas-api-production-abc123.up.railway.app\n</code></pre></li> <li>Copy this URL for external use</li> </ol>"},{"location":"deployment/railway-networking/#method-2-railway-variables","title":"Method 2: Railway Variables","text":"<p>Use Railway's built-in variable references:</p> <pre><code># In any service's environment variables:\n${{service-name.RAILWAY_PUBLIC_DOMAIN}}\n\n# Example:\nNEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n</code></pre> <p>Available Railway Variables: - <code>RAILWAY_PUBLIC_DOMAIN</code> - Public domain (e.g., <code>saas-api-xxx.up.railway.app</code>) - <code>RAILWAY_PRIVATE_DOMAIN</code> - Internal domain (e.g., <code>saas-api.railway.internal</code>) - <code>RAILWAY_TCP_PROXY_PORT</code> - Port for TCP proxy - <code>PORT</code> - Port your service should listen on</p>"},{"location":"deployment/railway-networking/#method-3-railway-cli","title":"Method 3: Railway CLI","text":"<pre><code># Install Railway CLI\nnpm install -g @railway/cli\n\n# Login\nrailway login\n\n# Link to your project\nrailway link\n\n# Get service info\nrailway status\n\n# Get service URL\nrailway domain\n</code></pre>"},{"location":"deployment/railway-networking/#common-url-configurations","title":"Common URL Configurations","text":""},{"location":"deployment/railway-networking/#team-api-clients","title":"Team API Clients","text":"<p>Teams making LLM requests use the SaaS API public URL:</p> <pre><code># Team client configuration\nAPI_URL = \"https://saas-api-production-abc123.up.railway.app\"\nVIRTUAL_KEY = \"sk-team-virtual-key\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/api/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"analysis\"}\n)\n</code></pre>"},{"location":"deployment/railway-networking/#admin-panel","title":"Admin Panel","text":"<p>Admin panel uses the SaaS API public URL:</p> <pre><code># admin-panel/.env.local (DO NOT use .railway.internal!)\nNEXT_PUBLIC_API_URL=https://saas-api-production-abc123.up.railway.app\n</code></pre>"},{"location":"deployment/railway-networking/#saas-api-to-litellm","title":"SaaS API to LiteLLM","text":"<p>SaaS API uses LiteLLM internal URL for better performance:</p> <pre><code># SaaS API environment\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n</code></pre>"},{"location":"deployment/railway-networking/#admins-configuring-models","title":"Admins Configuring Models","text":"<p>Admins use LiteLLM public URL to access UI:</p> <pre><code>https://litellm-proxy-xyz789.up.railway.app/ui\n</code></pre> <p>Login with: <code>LITELLM_MASTER_KEY</code></p>"},{"location":"deployment/railway-networking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/railway-networking/#cannot-connect-to-service","title":"\"Cannot connect to service\"","text":"<p>Problem: External client can't reach service</p> <p>Check: 1. Are you using the public URL?    - \u274c <code>http://saas-api.railway.internal:8080</code>    - \u2705 <code>https://saas-api-production-abc123.up.railway.app</code></p> <ol> <li>Is the service's public domain enabled?</li> <li>Go to service \u2192 Settings \u2192 Networking</li> <li> <p>Click \"Generate Domain\" if none exists</p> </li> <li> <p>Is the service deployed and running?</p> </li> <li>Check service logs for errors</li> <li>Verify deployment succeeded</li> </ol>"},{"location":"deployment/railway-networking/#connection-refused-from-railway-service","title":"\"Connection refused from Railway service\"","text":"<p>Problem: Service-to-service communication failing</p> <p>Check: 1. Are you using the internal URL for service-to-service?    - \u274c <code>https://litellm-proxy-xyz.up.railway.app</code>    - \u2705 <code>http://litellm-proxy.railway.internal:4000</code></p> <ol> <li>Is the target service running?</li> <li>Check if LiteLLM proxy deployed successfully</li> <li> <p>Verify it's listening on the correct port</p> </li> <li> <p>Is the service name correct?</p> </li> <li>Internal URL format: <code>http://&lt;service-name&gt;.railway.internal:&lt;port&gt;</code></li> <li>Service name matches exactly (check Railway dashboard)</li> </ol>"},{"location":"deployment/railway-networking/#cors-errors-in-browser","title":"\"CORS errors in browser\"","text":"<p>Problem: Admin panel can't make requests to SaaS API</p> <p>Check: 1. Is <code>NEXT_PUBLIC_API_URL</code> using public URL?    <pre><code># Correct\nNEXT_PUBLIC_API_URL=https://saas-api-production.up.railway.app\n\n# Wrong\nNEXT_PUBLIC_API_URL=http://saas-api.railway.internal:8080\n</code></pre></p> <ol> <li>Is CORS configured in SaaS API?    <pre><code># src/saas_api.py\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://admin-panel-xxx.up.railway.app\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre></li> </ol>"},{"location":"deployment/railway-networking/#admin-panel-shows-api-connection-failed","title":"\"Admin panel shows 'API connection failed'\"","text":"<p>Checklist: 1. \u2705 <code>NEXT_PUBLIC_API_URL</code> is set to SaaS API public URL 2. \u2705 SaaS API service is deployed and running 3. \u2705 SaaS API public domain is generated and accessible 4. \u2705 No typos in the URL 5. \u2705 CORS is properly configured</p> <p>Test manually: <pre><code># Should return 401 (auth required, but reachable)\ncurl https://saas-api-production-abc123.up.railway.app/api/teams\n</code></pre></p>"},{"location":"deployment/railway-networking/#custom-domains-optional","title":"Custom Domains (Optional)","text":"<p>For production, use custom domains instead of Railway-provided URLs:</p>"},{"location":"deployment/railway-networking/#setup-custom-domain","title":"Setup Custom Domain","text":"<ol> <li>Go to Railway service \u2192 Settings \u2192 Networking</li> <li>Click \"Custom Domain\"</li> <li>Enter your domain: <code>api.yourcompany.com</code></li> <li>Update DNS records (Railway provides instructions):    <pre><code>Type: CNAME\nName: api\nValue: saas-api-production-abc123.up.railway.app\n</code></pre></li> <li>Wait for DNS propagation (5-60 minutes)</li> <li>Update environment variables to use custom domain</li> </ol>"},{"location":"deployment/railway-networking/#update-configurations","title":"Update Configurations","text":"<p>After setting custom domains:</p> <p>Admin Panel: <pre><code>NEXT_PUBLIC_API_URL=https://api.yourcompany.com\n</code></pre></p> <p>Team Clients: <pre><code>API_URL = \"https://api.yourcompany.com\"\n</code></pre></p> <p>SaaS API to LiteLLM (still internal): <pre><code>LITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n</code></pre></p>"},{"location":"deployment/railway-networking/#best-practices","title":"Best Practices","text":""},{"location":"deployment/railway-networking/#1-use-internal-urls-for-service-to-service","title":"1. Use Internal URLs for Service-to-Service","text":"<p>Why: Faster, more secure, no external network hops</p> <pre><code># Good: Internal communication\nLITELLM_PROXY_URL=http://litellm-proxy.railway.internal:4000\n\n# Bad: Using public URL for internal communication\nLITELLM_PROXY_URL=https://litellm-proxy-xyz.up.railway.app\n</code></pre>"},{"location":"deployment/railway-networking/#2-use-public-urls-for-external-access","title":"2. Use Public URLs for External Access","text":"<p>Why: External clients cannot access <code>.railway.internal</code></p> <pre><code># Good: External access\nNEXT_PUBLIC_API_URL=https://saas-api-production.up.railway.app\n\n# Bad: External client using internal URL\nNEXT_PUBLIC_API_URL=http://saas-api.railway.internal:8080\n</code></pre>"},{"location":"deployment/railway-networking/#3-use-railway-variables-for-dynamic-urls","title":"3. Use Railway Variables for Dynamic URLs","text":"<p>Why: Automatically updates if service is redeployed</p> <pre><code># Good: Dynamic reference\nNEXT_PUBLIC_API_URL=https://${{saas-api.RAILWAY_PUBLIC_DOMAIN}}\n\n# Bad: Hardcoded URL\nNEXT_PUBLIC_API_URL=https://saas-api-production-abc123.up.railway.app\n</code></pre>"},{"location":"deployment/railway-networking/#4-document-your-urls","title":"4. Document Your URLs","text":"<p>Keep a reference of all service URLs:</p> <pre><code># Production URLs\nSaaS API (Public):  https://api.yourcompany.com\nSaaS API (Internal): http://saas-api.railway.internal:8080\nLiteLLM (Public):   https://litellm.yourcompany.com\nLiteLLM (Internal): http://litellm-proxy.railway.internal:4000\nAdmin Panel:        https://admin.yourcompany.com\n</code></pre>"},{"location":"deployment/railway-networking/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/railway-networking/#1-internal-urls-are-private","title":"1. Internal URLs are Private","text":"<ul> <li><code>.railway.internal</code> URLs are NOT accessible from the internet</li> <li>Only accessible within your Railway project</li> <li>Use for sensitive service-to-service communication</li> </ul>"},{"location":"deployment/railway-networking/#2-public-urls-are-exposed","title":"2. Public URLs are Exposed","text":"<ul> <li><code>.up.railway.app</code> URLs are publicly accessible</li> <li>Protect with authentication (MASTER_KEY, virtual keys)</li> <li>Consider custom domains with additional DNS security</li> </ul>"},{"location":"deployment/railway-networking/#3-https-everywhere","title":"3. HTTPS Everywhere","text":"<ul> <li>Railway provides free HTTPS for all public URLs</li> <li>Use HTTPS for all external communication</li> <li>Internal Railway communication can use HTTP (secured by Railway network)</li> </ul>"},{"location":"deployment/railway-networking/#cors-and-authentication","title":"CORS and Authentication","text":"<p>Team Clients Are NOT Affected by CORS</p> <p>Important: Server-side team clients (Python, Node.js, curl) completely ignore CORS restrictions.</p> <ul> <li>\u2705 Team clients - Use Bearer tokens, work from anywhere, CORS doesn't apply</li> <li>\u26a0\ufe0f Admin panel - Browser-based, must configure CORS properly</li> </ul> <p>CORS is a browser-only security feature. Your team API clients work regardless of CORS configuration.</p> <p> Learn more about CORS vs Authentication</p> <p>CORS Configuration:</p> <p>The SaaS API CORS middleware only allows specific origins (for the browser-based admin panel):</p> <pre><code># Only affects browser-based clients (admin panel)\nallow_origins=[\n    \"http://localhost:3000\",  # Local admin panel\n    \"https://admin-panel-xxx.up.railway.app\",  # Production admin panel\n]\n</code></pre> <p>Team API clients (Python, Node.js, curl) completely ignore this configuration.</p>"},{"location":"deployment/railway-networking/#summary","title":"Summary","text":"Component Connection Type URL to Use CORS Applies? Team Client \u2192 SaaS API External Public: <code>https://saas-api-xxx.up.railway.app</code> \u274c No (server-side) Admin Panel \u2192 SaaS API External Public: <code>https://saas-api-xxx.up.railway.app</code> \u2705 Yes (browser) SaaS API \u2192 LiteLLM Internal Internal: <code>http://litellm-proxy.railway.internal:4000</code> \u274c No Admin \u2192 LiteLLM UI External Public: <code>https://litellm-proxy-xxx.up.railway.app/ui</code> \u2705 Yes (browser)"},{"location":"deployment/railway-networking/#next-steps","title":"Next Steps","text":"<ul> <li>Railway Deployment Guide - Complete deployment walkthrough</li> <li>CORS &amp; Authentication Guide - Understand CORS vs authentication</li> <li>Environment Variables - All configuration options</li> <li>Security Guide - Protect your deployment</li> </ul>"},{"location":"deployment/railway/","title":"Railway Deployment","text":"<p>Deploy SaaS LiteLLM to Railway for production use.</p>"},{"location":"deployment/railway/#overview","title":"Overview","text":"<p>Railway deployment consists of 3 services:</p> <ol> <li>PostgreSQL Database - Managed database</li> <li>SaaS API - Main FastAPI application (port 8003)</li> <li>MkDocs Docs - Documentation site (port 8004)</li> <li>Admin Panel (optional) - Next.js dashboard (port 3000)</li> </ol>"},{"location":"deployment/railway/#prerequisites","title":"Prerequisites","text":"<ul> <li>Railway account (signup)</li> <li>GitHub repository with SaasLiteLLM code</li> <li>OpenAI API key (or other LLM provider keys)</li> </ul>"},{"location":"deployment/railway/#step-1-create-railway-project","title":"Step 1: Create Railway Project","text":"<ol> <li>Go to Railway Dashboard</li> <li>Click \"New Project\"</li> <li>Select \"Deploy from GitHub repo\"</li> <li>Choose your SaasLiteLLM repository</li> <li>Railway will detect the project structure</li> </ol>"},{"location":"deployment/railway/#step-2-add-postgresql-database","title":"Step 2: Add PostgreSQL Database","text":"<ol> <li>In your Railway project, click \"+ New\"</li> <li>Select \"Database\" \u2192 \"PostgreSQL\"</li> <li>Railway automatically creates and configures the database</li> <li>Note the connection details (or use <code>${{Postgres.DATABASE_URL}}</code>)</li> </ol>"},{"location":"deployment/railway/#step-3-configure-saas-api-service","title":"Step 3: Configure SaaS API Service","text":""},{"location":"deployment/railway/#service-settings","title":"Service Settings","text":"<ul> <li>Name: <code>saas-api</code></li> <li>Root Directory: (leave empty - uses repository root)</li> <li>Dockerfile Path: <code>Dockerfile</code></li> </ul>"},{"location":"deployment/railway/#environment-variables","title":"Environment Variables","text":"<p>Add these variables to the SaaS API service:</p> <pre><code># Database (automatically provided by Railway)\nDATABASE_URL=${{Postgres.DATABASE_URL}}\n\n# LiteLLM Proxy (use your LiteLLM Cloud or self-hosted URL)\nLITELLM_PROXY_BASE_URL=https://your-litellm-proxy.com\n\n# Master Key (generate a secure key)\nMASTER_KEY=sk-master-xxxxxxxxxxxxxxxxxxxxxxxx\n\n# LLM Provider Keys\nOPENAI_API_KEY=sk-your-openai-key\n# Add other provider keys as needed:\n# ANTHROPIC_API_KEY=sk-ant-xxxxx\n# GOOGLE_API_KEY=xxxxx\n# AZURE_API_KEY=xxxxx\n</code></pre>"},{"location":"deployment/railway/#generate-master-key","title":"Generate Master Key","text":"<pre><code>python -c \"import secrets; print('sk-master-' + secrets.token_urlsafe(32))\"\n</code></pre>"},{"location":"deployment/railway/#networking","title":"Networking","text":"<p>Railway will automatically: - Assign a public URL (e.g., <code>https://saas-api-production-xxxxx.up.railway.app</code>) - Expose port 8003 - Enable HTTPS</p>"},{"location":"deployment/railway/#step-4-run-database-migrations","title":"Step 4: Run Database Migrations","text":"<p>After the SaaS API deploys:</p> <ol> <li>Go to the service Settings \u2192 Variables</li> <li>Add temporary variable for migration:    <pre><code>RUN_MIGRATIONS=true\n</code></pre></li> <li>Redeploy the service</li> <li>Check logs to verify migrations ran successfully</li> <li>Remove the <code>RUN_MIGRATIONS</code> variable</li> </ol> <p>Or run migrations manually:</p> <pre><code># Install Railway CLI\nnpm install -g @railway/cli\n\n# Login\nrailway login\n\n# Link to your project\nrailway link\n\n# Run migrations\nrailway run psql $DATABASE_URL -f scripts/migrations/001_initial_schema.sql\n# Repeat for all migrations...\n</code></pre>"},{"location":"deployment/railway/#step-5-configure-mkdocs-docs-service","title":"Step 5: Configure MkDocs Docs Service","text":""},{"location":"deployment/railway/#service-settings_1","title":"Service Settings","text":"<ul> <li>Name: <code>docs</code></li> <li>Root Directory: <code>docs-service</code></li> <li>Dockerfile Path: <code>Dockerfile</code></li> </ul>"},{"location":"deployment/railway/#environment-variables_1","title":"Environment Variables","text":"<p>None needed - <code>PORT</code> is automatically provided by Railway.</p>"},{"location":"deployment/railway/#networking_1","title":"Networking","text":"<p>Railway will assign a public URL for the docs (e.g., <code>https://docs-xxxxx.up.railway.app</code>)</p>"},{"location":"deployment/railway/#step-6-configure-admin-panel-optional","title":"Step 6: Configure Admin Panel (Optional)","text":""},{"location":"deployment/railway/#service-settings_2","title":"Service Settings","text":"<ul> <li>Name: <code>admin-panel</code></li> <li>Root Directory: <code>admin-panel</code></li> <li>Build Command: <code>npm run build</code></li> <li>Start Command: <code>npm run start</code></li> </ul>"},{"location":"deployment/railway/#environment-variables_2","title":"Environment Variables","text":"<pre><code># API URL (use your SaaS API Railway URL)\nNEXT_PUBLIC_API_URL=${{saas-api.RAILWAY_PUBLIC_URL}}\n\n# Node environment\nNODE_ENV=production\n</code></pre> <p>Railway will automatically detect Next.js and configure the build.</p>"},{"location":"deployment/railway/#step-7-verify-deployment","title":"Step 7: Verify Deployment","text":""},{"location":"deployment/railway/#check-health-endpoint","title":"Check Health Endpoint","text":"<pre><code>curl https://your-saas-api-url.railway.app/health\n</code></pre> <p>Expected response: <pre><code>{\n  \"status\": \"healthy\",\n  \"database\": \"connected\",\n  \"litellm_proxy\": \"reachable\"\n}\n</code></pre></p>"},{"location":"deployment/railway/#create-test-organization","title":"Create Test Organization","text":"<pre><code>curl -X POST https://your-saas-api-url.railway.app/api/organizations/create \\\n  -H \"Authorization: Bearer sk-master-xxxxx\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"org_id\": \"test-org\",\n    \"org_name\": \"Test Organization\"\n  }'\n</code></pre>"},{"location":"deployment/railway/#access-documentation","title":"Access Documentation","text":"<p>Visit <code>https://your-docs-url.railway.app</code> to see the deployed documentation.</p>"},{"location":"deployment/railway/#environment-management","title":"Environment Management","text":""},{"location":"deployment/railway/#production-vs-staging","title":"Production vs Staging","text":"<p>Create separate Railway projects for different environments:</p> <p>Production: - Project name: <code>saas-litellm-production</code> - Branch: <code>main</code> - Domain: <code>api.yourcompany.com</code> (custom domain)</p> <p>Staging: - Project name: <code>saas-litellm-staging</code> - Branch: <code>develop</code> - Domain: Railway-provided URL</p>"},{"location":"deployment/railway/#custom-domains","title":"Custom Domains","text":"<ol> <li>Go to service Settings \u2192 Networking</li> <li>Click \"Generate Domain\" or \"Custom Domain\"</li> <li>Add your custom domain (e.g., <code>api.yourcompany.com</code>)</li> <li>Update DNS records as instructed by Railway</li> </ol>"},{"location":"deployment/railway/#environment-variables_3","title":"Environment Variables","text":"<p>Access via Railway Dashboard: - Shared Variables: Available to all services - Service Variables: Specific to one service - Reference Variables: Use <code>${{service.VARIABLE_NAME}}</code></p>"},{"location":"deployment/railway/#scaling","title":"Scaling","text":"<p>Railway automatically scales based on usage:</p> <ul> <li>Vertical Scaling: Increase memory/CPU in service settings</li> <li>Horizontal Scaling: Available on Pro plan</li> <li>Database: Can upgrade PostgreSQL plan for more resources</li> </ul>"},{"location":"deployment/railway/#monitoring","title":"Monitoring","text":""},{"location":"deployment/railway/#view-logs","title":"View Logs","text":"<ol> <li>Go to service in Railway Dashboard</li> <li>Click \"Logs\" tab</li> <li>Filter by severity, search, or time range</li> </ol>"},{"location":"deployment/railway/#metrics","title":"Metrics","text":"<p>Railway provides built-in metrics: - CPU usage - Memory usage - Network traffic - Request count</p>"},{"location":"deployment/railway/#alerts","title":"Alerts","text":"<p>Set up alerts in Railway Dashboard: - Service crashes - High resource usage - Deployment failures</p>"},{"location":"deployment/railway/#backup-recovery","title":"Backup &amp; Recovery","text":""},{"location":"deployment/railway/#database-backups","title":"Database Backups","text":"<p>Railway provides automatic daily backups for PostgreSQL:</p> <ol> <li>Go to database service</li> <li>Click \"Backups\" tab</li> <li>View available backups</li> <li>Restore from backup if needed</li> </ol>"},{"location":"deployment/railway/#manual-backup","title":"Manual Backup","text":"<pre><code># Export database\nrailway run pg_dump $DATABASE_URL &gt; backup.sql\n\n# Restore database\nrailway run psql $DATABASE_URL &lt; backup.sql\n</code></pre>"},{"location":"deployment/railway/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/railway/#deployment-failed","title":"Deployment Failed","text":"<p>Check logs: 1. Go to service \u2192 Deployments 2. Click failed deployment 3. View build/deploy logs for errors</p> <p>Common issues: - Missing environment variables - Database connection failed - Docker build errors</p>"},{"location":"deployment/railway/#database-connection-issues","title":"Database Connection Issues","text":"<p>Check: 1. <code>DATABASE_URL</code> is correctly set 2. PostgreSQL service is running 3. Migrations completed successfully</p> <p>Test connection: <pre><code>railway run psql $DATABASE_URL -c \"SELECT 1\"\n</code></pre></p>"},{"location":"deployment/railway/#out-of-memory","title":"Out of Memory","text":"<p>Solution: 1. Go to service Settings 2. Increase memory allocation 3. Redeploy service</p>"},{"location":"deployment/railway/#litellm-proxy-unreachable","title":"LiteLLM Proxy Unreachable","text":"<p>Check: 1. <code>LITELLM_PROXY_BASE_URL</code> is correct 2. LiteLLM proxy is running and accessible 3. Network connectivity from Railway</p>"},{"location":"deployment/railway/#cost-optimization","title":"Cost Optimization","text":"<p>Railway pricing is usage-based:</p> <p>Tips to reduce costs: - Use resource limits - Scale down staging environments when not in use - Optimize Docker image size - Use shared PostgreSQL for development - Monitor usage in Railway Dashboard</p> <p>Typical monthly costs: - Hobby plan: $5/month + usage - Pro plan: \\(20/month + usage - Database: ~\\)5-20/month depending on size</p>"},{"location":"deployment/railway/#next-steps","title":"Next Steps","text":"<ul> <li>Environment Variables - Complete configuration reference</li> <li>Docker Setup - Customize Docker builds</li> <li>Monitoring - Track system health</li> </ul>"},{"location":"downloads/typed-client/","title":"Download Typed Client","text":"<p>Get the type-safe Python client to integrate with the SaaS LiteLLM API.</p>"},{"location":"downloads/typed-client/#quick-download","title":"Quick Download","text":"<p>File: <code>typed_client.py</code></p> <p> Download typed_client.py</p>"},{"location":"downloads/typed-client/#installation","title":"Installation","text":""},{"location":"downloads/typed-client/#step-1-download-the-client","title":"Step 1: Download the Client","text":"<p>Choose one of these methods:</p> <p>Method A: Direct Download <pre><code># Download from your SaaS API server\ncurl -O http://YOUR_DOMAIN:8003/examples/typed_client.py\n</code></pre></p> <p>Method B: GitHub (if available) <pre><code>curl -O https://raw.githubusercontent.com/YOUR_ORG/SaasLiteLLM/main/examples/typed_client.py\n</code></pre></p> <p>Method C: Copy from Repository <pre><code># If you have repo access\ncp /path/to/SaasLiteLLM/examples/typed_client.py your_project/saas_litellm_client.py\n</code></pre></p>"},{"location":"downloads/typed-client/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<pre><code>pip install httpx pydantic\n</code></pre>"},{"location":"downloads/typed-client/#step-3-configure","title":"Step 3: Configure","text":"<p>Create a <code>.env</code> file:</p> <pre><code># .env\nSAAS_LITELLM_API_URL=http://your-domain:8003\nSAAS_LITELLM_TEAM_ID=your-team-id\nSAAS_LITELLM_VIRTUAL_KEY=sk-your-virtual-key-here\n</code></pre>"},{"location":"downloads/typed-client/#client-source-code","title":"Client Source Code","text":"<p>If you prefer to copy/paste, here's the complete client:</p> <pre><code>\"\"\"\nType-safe client for SaaS LiteLLM API\n\nThis client provides full type safety using Pydantic models and supports\nboth streaming and non-streaming responses.\n\"\"\"\nfrom typing import Optional, List, Dict, Any, AsyncGenerator, TypeVar, Type, Union\nfrom pydantic import BaseModel, Field\nimport httpx\nimport json\n\n\n# ============================================================================\n# Request/Response Models\n# ============================================================================\n\nclass Message(BaseModel):\n    \"\"\"Chat message\"\"\"\n    role: str = Field(..., description=\"Role: 'system', 'user', or 'assistant'\")\n    content: str = Field(..., description=\"Message content\")\n    name: Optional[str] = Field(None, description=\"Optional name of the speaker\")\n\n\nclass ChatChoice(BaseModel):\n    \"\"\"Single choice in chat completion\"\"\"\n    index: int\n    message: Optional[Dict[str, Any]] = None\n    delta: Optional[Dict[str, Any]] = None  # For streaming\n    finish_reason: Optional[str] = None\n\n\nclass Usage(BaseModel):\n    \"\"\"Token usage information\"\"\"\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n\n\nclass ChatCompletionResponse(BaseModel):\n    \"\"\"Non-streaming chat completion response\"\"\"\n    id: str\n    object: str = \"chat.completion\"\n    created: int\n    model: str\n    choices: List[ChatChoice]\n    usage: Optional[Usage] = None\n\n\nclass StreamChunk(BaseModel):\n    \"\"\"Streaming chat completion chunk\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int\n    model: str\n    choices: List[ChatChoice]\n    usage: Optional[Usage] = None\n\n\nclass JobResponse(BaseModel):\n    \"\"\"Job creation response\"\"\"\n    job_id: str\n    team_id: str\n    status: str\n    created_at: str\n\n\nclass JobCompletionResult(BaseModel):\n    \"\"\"Job completion response\"\"\"\n    job_id: str\n    status: str\n    credits_remaining: int\n    total_calls: int\n\n\n# ============================================================================\n# Type-safe Client\n# ============================================================================\n\nT = TypeVar('T', bound=BaseModel)\n\n\nclass SaaSLLMClient:\n    \"\"\"\n    Type-safe async client for SaaS LiteLLM API\n\n    Features:\n    - Type hints and Pydantic validation\n    - Context manager support (async with)\n    - Automatic job management\n    - Streaming and non-streaming\n    - Structured outputs with Pydantic models\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        team_id: str,\n        virtual_key: str,\n        timeout: float = 120.0\n    ):\n        \"\"\"\n        Initialize client\n\n        Args:\n            base_url: SaaS API base URL (e.g., \"http://localhost:8003\")\n            team_id: Your team ID\n            virtual_key: Your team's virtual API key\n            timeout: Request timeout in seconds (default: 120)\n        \"\"\"\n        self.base_url = base_url.rstrip('/')\n        self.team_id = team_id\n        self.virtual_key = virtual_key\n        self.timeout = timeout\n\n        # HTTP client\n        self.client = httpx.AsyncClient(\n            timeout=timeout,\n            headers={\n                \"Authorization\": f\"Bearer {virtual_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n        )\n\n    async def close(self):\n        \"\"\"Close HTTP client\"\"\"\n        await self.client.aclose()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n\n    # ========================================================================\n    # Job Management\n    # ========================================================================\n\n    async def create_job(\n        self,\n        job_type: str,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"\n        Create a new job\n\n        Args:\n            job_type: Type of job (e.g., \"chat\", \"analysis\", \"extraction\")\n            metadata: Optional metadata dictionary\n\n        Returns:\n            Job ID (UUID string)\n        \"\"\"\n        response = await self.client.post(\n            f\"{self.base_url}/api/jobs/create\",\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": job_type,\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        job_response = JobResponse(**response.json())\n        return job_response.job_id\n\n    async def complete_job(\n        self,\n        job_id: str,\n        status: str = \"completed\",\n        metadata: Optional[Dict[str, Any]] = None\n    ) -&gt; JobCompletionResult:\n        \"\"\"\n        Complete a job\n\n        Args:\n            job_id: Job ID from create_job()\n            status: Final status (\"completed\" or \"failed\")\n            metadata: Optional result metadata\n\n        Returns:\n            JobCompletionResult with credits remaining\n        \"\"\"\n        response = await self.client.post(\n            f\"{self.base_url}/api/jobs/{job_id}/complete\",\n            json={\n                \"status\": status,\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        return JobCompletionResult(**response.json())\n\n    # ========================================================================\n    # Chat Completions (Non-streaming)\n    # ========================================================================\n\n    async def chat(\n        self,\n        job_id: str,\n        messages: List[Union[Dict[str, str], Message]],\n        model: str = \"gpt-4\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        top_p: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; ChatCompletionResponse:\n        \"\"\"\n        Make a non-streaming chat completion call\n\n        Args:\n            job_id: Job ID from create_job()\n            messages: List of message dictionaries\n            model: Model name (e.g., \"gpt-4\", \"claude-3-opus\")\n            temperature: Sampling temperature (0.0-2.0)\n            max_tokens: Maximum tokens to generate\n            top_p: Nucleus sampling parameter\n            frequency_penalty: Reduce repetition (-2.0 to 2.0)\n            presence_penalty: Encourage new topics (-2.0 to 2.0)\n            stop: Stop sequence(s)\n\n        Returns:\n            ChatCompletionResponse with full response\n        \"\"\"\n        # Convert Message objects to dicts\n        message_dicts = [\n            msg.model_dump() if isinstance(msg, BaseModel) else msg\n            for msg in messages\n        ]\n\n        # Build request payload\n        payload = {\n            \"model\": model,\n            \"messages\": message_dicts,\n            \"temperature\": temperature,\n        }\n\n        # Add optional parameters\n        if max_tokens is not None:\n            payload[\"max_tokens\"] = max_tokens\n        if top_p is not None:\n            payload[\"top_p\"] = top_p\n        if frequency_penalty is not None:\n            payload[\"frequency_penalty\"] = frequency_penalty\n        if presence_penalty is not None:\n            payload[\"presence_penalty\"] = presence_penalty\n        if stop is not None:\n            payload[\"stop\"] = stop\n\n        response = await self.client.post(\n            f\"{self.base_url}/api/jobs/{job_id}/llm-call\",\n            json=payload\n        )\n        response.raise_for_status()\n\n        return ChatCompletionResponse(**response.json())\n\n    # ========================================================================\n    # Chat Completions (Streaming)\n    # ========================================================================\n\n    async def chat_stream(\n        self,\n        job_id: str,\n        messages: List[Union[Dict[str, str], Message]],\n        model: str = \"gpt-4\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        top_p: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n    ) -&gt; AsyncGenerator[StreamChunk, None]:\n        \"\"\"\n        Make a streaming chat completion call\n\n        Args:\n            Same as chat() method\n\n        Yields:\n            StreamChunk objects as they arrive\n        \"\"\"\n        # Convert Message objects to dicts\n        message_dicts = [\n            msg.model_dump() if isinstance(msg, BaseModel) else msg\n            for msg in messages\n        ]\n\n        # Build request payload\n        payload = {\n            \"model\": model,\n            \"messages\": message_dicts,\n            \"temperature\": temperature,\n        }\n\n        # Add optional parameters\n        if max_tokens is not None:\n            payload[\"max_tokens\"] = max_tokens\n        if top_p is not None:\n            payload[\"top_p\"] = top_p\n        if frequency_penalty is not None:\n            payload[\"frequency_penalty\"] = frequency_penalty\n        if presence_penalty is not None:\n            payload[\"presence_penalty\"] = presence_penalty\n        if stop is not None:\n            payload[\"stop\"] = stop\n\n        async with self.client.stream(\n            \"POST\",\n            f\"{self.base_url}/api/jobs/{job_id}/llm-call-stream\",\n            json=payload\n        ) as response:\n            response.raise_for_status()\n\n            async for line in response.aiter_lines():\n                if line.startswith(\"data: \"):\n                    chunk_data = line[6:]  # Remove \"data: \" prefix\n\n                    if chunk_data == \"[DONE]\":\n                        break\n\n                    try:\n                        chunk_json = json.loads(chunk_data)\n\n                        # Handle error chunks\n                        if \"error\" in chunk_json:\n                            raise Exception(f\"Stream error: {chunk_json['error']}\")\n\n                        yield StreamChunk(**chunk_json)\n\n                    except json.JSONDecodeError:\n                        continue\n\n    # ========================================================================\n    # Structured Outputs\n    # ========================================================================\n\n    async def structured_output(\n        self,\n        job_id: str,\n        messages: List[Union[Dict[str, str], Message]],\n        response_model: Type[T],\n        model: str = \"gpt-4\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n    ) -&gt; T:\n        \"\"\"\n        Get a type-safe structured output using Pydantic model\n\n        Args:\n            job_id: Job ID from create_job()\n            messages: List of messages\n            response_model: Pydantic model class for the response\n            model: Model name\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens\n\n        Returns:\n            Instance of response_model\n\n        Example:\n            class Person(BaseModel):\n                name: str\n                age: int\n                email: str\n\n            person = await client.structured_output(\n                job_id=job_id,\n                messages=[{\"role\": \"user\", \"content\": \"Extract: John, 30, john@example.com\"}],\n                response_model=Person\n            )\n        \"\"\"\n        # Build JSON schema from Pydantic model\n        schema = response_model.model_json_schema()\n\n        response = await self.chat(\n            job_id=job_id,\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n\n        # Parse response content as Pydantic model\n        content = response.choices[0].message.get(\"content\", \"\")\n        return response_model.model_validate_json(content)\n</code></pre>"},{"location":"downloads/typed-client/#usage-examples","title":"Usage Examples","text":""},{"location":"downloads/typed-client/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom saas_litellm_client import SaaSLLMClient\n\nasync def main():\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"your-team-id\",\n        virtual_key=\"sk-your-virtual-key\"\n    ) as client:\n\n        # Create job\n        job_id = await client.create_job(\"chat_example\")\n\n        # Make LLM call\n        response = await client.chat(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"What is Python?\"}\n            ]\n        )\n\n        print(response.choices[0].message[\"content\"])\n\n        # Complete job\n        await client.complete_job(job_id, \"completed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"downloads/typed-client/#streaming-example","title":"Streaming Example","text":"<pre><code>async def stream_example():\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"streaming_chat\")\n\n        print(\"Assistant: \", end=\"\", flush=True)\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}]\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                print(content, end=\"\", flush=True)\n\n        await client.complete_job(job_id, \"completed\")\n</code></pre>"},{"location":"downloads/typed-client/#structured-output-example","title":"Structured Output Example","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n\nasync def extract_person():\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"person_extraction\")\n\n        person = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": \"Extract: Sarah Johnson, 28, sarah@example.com\"\n            }],\n            response_model=Person\n        )\n\n        print(f\"Name: {person.name}, Age: {person.age}\")\n\n        await client.complete_job(job_id, \"completed\")\n</code></pre>"},{"location":"downloads/typed-client/#next-steps","title":"Next Steps","text":"<ul> <li>See Full Examples - More usage patterns</li> <li>Streaming Guide - Real-time responses</li> <li>Structured Outputs - Type-safe extraction</li> <li>Error Handling - Handle failures</li> </ul>"},{"location":"downloads/typed-client/#support","title":"Support","text":"<p>Having issues? Check the troubleshooting guide or contact support.</p>"},{"location":"examples/agent-integration/","title":"Agent Integration Examples","text":"<p>Learn how to integrate SaaS LiteLLM with popular AI agent frameworks like LangChain, AutoGen, and CrewAI.</p>"},{"location":"examples/agent-integration/#overview","title":"Overview","text":"<p>AI agent frameworks typically expect OpenAI-compatible APIs. SaaS LiteLLM provides this interface while adding:</p> <ul> <li>Job-based cost tracking</li> <li>Model group abstraction</li> <li>Credit management</li> <li>Team isolation</li> </ul>"},{"location":"examples/agent-integration/#integration-strategy","title":"Integration Strategy","text":"<p>All frameworks follow a similar pattern:</p> <ol> <li>Create job at the start of agent task</li> <li>Resolve model group to get actual model name</li> <li>Make LLM calls through the job</li> <li>Complete job when task is done</li> </ol>"},{"location":"examples/agent-integration/#langchain-integration","title":"LangChain Integration","text":""},{"location":"examples/agent-integration/#basic-langchain-wrapper","title":"Basic LangChain Wrapper","text":"<pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\nimport requests\nfrom typing import List, Optional, Dict, Any\n\nclass SaaSLangChainWrapper:\n    \"\"\"\n    Wrapper for using SaaS LiteLLM with LangChain.\n\n    This wrapper creates a job, resolves model groups, and makes calls\n    through the SaaS API while presenting an OpenAI-compatible interface\n    to LangChain.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n        virtual_key: str,\n        team_id: str,\n        model_group: str = \"ChatFast\",\n        job_type: str = \"langchain_agent\"\n    ):\n        self.api_url = api_url\n        self.virtual_key = virtual_key\n        self.team_id = team_id\n        self.model_group = model_group\n        self.job_type = job_type\n        self.job_id = None\n        self.resolved_model = None\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def start_job(self, metadata: Dict[str, Any] = None):\n        \"\"\"Create a job for this LangChain session\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/jobs/create\",\n            headers=self.headers,\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": self.job_type,\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        self.job_id = response.json()[\"job_id\"]\n\n        # Resolve model group to actual model\n        self._resolve_model_group()\n\n    def _resolve_model_group(self):\n        \"\"\"Resolve model group to actual model name\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/model-groups/{self.model_group}/resolve\",\n            headers=self.headers,\n            params={\"team_id\": self.team_id}\n        )\n        response.raise_for_status()\n        result = response.json()\n\n        if not result.get(\"team_has_access\"):\n            raise ValueError(\n                f\"Team does not have access to model group '{self.model_group}'\"\n            )\n\n        self.resolved_model = result[\"primary_model\"]\n\n    def get_langchain_llm(self, temperature: float = 0.7, **kwargs):\n        \"\"\"\n        Get a LangChain ChatOpenAI instance configured to use SaaS LiteLLM.\n\n        Note: This creates a custom callback to intercept calls.\n        \"\"\"\n        if not self.job_id:\n            self.start_job()\n\n        # Create custom ChatOpenAI with our job context\n        llm = SaaSChatOpenAI(\n            wrapper=self,\n            temperature=temperature,\n            model=self.resolved_model,\n            **kwargs\n        )\n\n        return llm\n\n    def make_call(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        purpose: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"Make an LLM call through the job\"\"\"\n        if not self.job_id:\n            raise ValueError(\"No job started. Call start_job() first.\")\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/llm-call\",\n            headers=self.headers,\n            json={\n                \"model_group\": self.model_group,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens,\n                \"purpose\": purpose\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n        return result[\"response\"][\"content\"]\n\n    def complete_job(self, status: str = \"completed\"):\n        \"\"\"Complete the job\"\"\"\n        if not self.job_id:\n            return\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/complete\",\n            headers=self.headers,\n            json={\"status\": status}\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def __enter__(self):\n        self.start_job()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is not None:\n            self.complete_job(status=\"failed\")\n        else:\n            self.complete_job(status=\"completed\")\n\n\nclass SaaSChatOpenAI(ChatOpenAI):\n    \"\"\"\n    Custom ChatOpenAI that routes calls through SaaS API.\n    \"\"\"\n\n    def __init__(self, wrapper: SaaSLangChainWrapper, **kwargs):\n        self.wrapper = wrapper\n        # Don't actually use OpenAI - we'll override the call\n        super().__init__(openai_api_key=\"not-used\", **kwargs)\n\n    def _call(self, messages: List, **kwargs) -&gt; str:\n        \"\"\"Override to use SaaS API\"\"\"\n        # Convert LangChain messages to API format\n        api_messages = []\n        for msg in messages:\n            if isinstance(msg, HumanMessage):\n                api_messages.append({\"role\": \"user\", \"content\": msg.content})\n            elif isinstance(msg, SystemMessage):\n                api_messages.append({\"role\": \"system\", \"content\": msg.content})\n            elif isinstance(msg, AIMessage):\n                api_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n\n        return self.wrapper.make_call(\n            messages=api_messages,\n            temperature=kwargs.get(\"temperature\", 0.7),\n            purpose=\"langchain_call\"\n        )\n\n\n# Example Usage\ndef langchain_example():\n    \"\"\"Complete LangChain example with SaaS LiteLLM\"\"\"\n    from langchain.chains import LLMChain\n    from langchain.prompts import PromptTemplate\n\n    # Create wrapper\n    with SaaSLangChainWrapper(\n        api_url=\"http://localhost:8003/api\",\n        virtual_key=\"sk-your-virtual-key\",\n        team_id=\"engineering-team\",\n        model_group=\"ChatFast\"\n    ) as wrapper:\n\n        # Get LangChain LLM\n        llm = wrapper.get_langchain_llm(temperature=0.7)\n\n        # Create a simple chain\n        prompt = PromptTemplate(\n            input_variables=[\"product\"],\n            template=\"Write a short marketing description for {product}\"\n        )\n\n        chain = LLMChain(llm=llm, prompt=prompt)\n\n        # Run the chain\n        result = chain.run(product=\"AI-powered resume parser\")\n\n        print(f\"Result: {result}\")\n\n    print(\"Job completed and tracked!\")\n\n\nif __name__ == \"__main__\":\n    langchain_example()\n</code></pre>"},{"location":"examples/agent-integration/#langchain-agent-example","title":"LangChain Agent Example","text":"<pre><code>from langchain.agents import initialize_agent, Tool, AgentType\nfrom langchain.memory import ConversationBufferMemory\n\ndef langchain_agent_example():\n    \"\"\"\n    Full agent with tools, memory, and SaaS LiteLLM tracking.\n    \"\"\"\n\n    # Custom tools\n    def search_database(query: str) -&gt; str:\n        \"\"\"Simulate database search\"\"\"\n        return f\"Found 3 results for '{query}'\"\n\n    def calculate(expression: str) -&gt; str:\n        \"\"\"Simple calculator\"\"\"\n        try:\n            result = eval(expression)\n            return f\"Result: {result}\"\n        except:\n            return \"Invalid expression\"\n\n    tools = [\n        Tool(\n            name=\"Database Search\",\n            func=search_database,\n            description=\"Search the database for information\"\n        ),\n        Tool(\n            name=\"Calculator\",\n            func=calculate,\n            description=\"Calculate mathematical expressions\"\n        )\n    ]\n\n    # Create SaaS LiteLLM wrapper\n    with SaaSLangChainWrapper(\n        api_url=\"http://localhost:8003/api\",\n        virtual_key=\"sk-your-virtual-key\",\n        team_id=\"engineering-team\",\n        model_group=\"ChatAdvanced\",\n        job_type=\"langchain_agent_with_tools\"\n    ) as wrapper:\n\n        # Get LLM\n        llm = wrapper.get_langchain_llm(temperature=0)\n\n        # Create memory\n        memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True\n        )\n\n        # Initialize agent\n        agent = initialize_agent(\n            tools=tools,\n            llm=llm,\n            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n            memory=memory,\n            verbose=True\n        )\n\n        # Run tasks\n        print(\"Task 1: Database search\")\n        response1 = agent.run(\"Search the database for 'Python developers'\")\n        print(f\"Response: {response1}\\n\")\n\n        print(\"Task 2: Calculation\")\n        response2 = agent.run(\"What is 15 * 23?\")\n        print(f\"Response: {response2}\\n\")\n\n        print(\"Task 3: Using memory\")\n        response3 = agent.run(\"What did I ask you to search for earlier?\")\n        print(f\"Response: {response3}\\n\")\n\n    print(\"Agent task completed and costs tracked!\")\n</code></pre>"},{"location":"examples/agent-integration/#autogen-integration","title":"AutoGen Integration","text":"<p>AutoGen is a framework for building multi-agent conversations.</p>"},{"location":"examples/agent-integration/#autogen-wrapper","title":"AutoGen Wrapper","text":"<pre><code>from typing import List, Dict, Any, Optional\nimport requests\n\nclass SaaSAutoGenWrapper:\n    \"\"\"\n    Wrapper for using SaaS LiteLLM with Microsoft AutoGen.\n\n    Provides job-based tracking for AutoGen multi-agent conversations.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n        virtual_key: str,\n        team_id: str,\n        model_group: str = \"ChatFast\"\n    ):\n        self.api_url = api_url\n        self.virtual_key = virtual_key\n        self.team_id = team_id\n        self.model_group = model_group\n        self.job_id = None\n        self.call_count = 0\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def start_conversation_job(self, metadata: Dict[str, Any] = None):\n        \"\"\"Start a job for this AutoGen conversation\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/jobs/create\",\n            headers=self.headers,\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": \"autogen_conversation\",\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        self.job_id = response.json()[\"job_id\"]\n        return self.job_id\n\n    def get_llm_config(self, temperature: float = 0.7) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get LLM config for AutoGen agents.\n\n        AutoGen expects a config dict with model and API settings.\n        We intercept this to route through our job.\n        \"\"\"\n        if not self.job_id:\n            self.start_conversation_job()\n\n        # Return custom config that AutoGen will use\n        return {\n            \"model\": self.model_group,\n            \"api_type\": \"saas_litellm\",\n            \"api_base\": self.api_url,\n            \"api_key\": self.virtual_key,\n            \"temperature\": temperature,\n            # Custom field for our wrapper\n            \"_saas_job_id\": self.job_id,\n            \"_saas_wrapper\": self\n        }\n\n    def make_agent_call(\n        self,\n        messages: List[Dict[str, str]],\n        agent_name: str,\n        temperature: float = 0.7\n    ) -&gt; str:\n        \"\"\"Make a call on behalf of an AutoGen agent\"\"\"\n        if not self.job_id:\n            raise ValueError(\"No conversation job started\")\n\n        self.call_count += 1\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/llm-call\",\n            headers=self.headers,\n            json={\n                \"model_group\": self.model_group,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"purpose\": f\"{agent_name}_message_{self.call_count}\"\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n        return result[\"response\"][\"content\"]\n\n    def complete_conversation(self, status: str = \"completed\"):\n        \"\"\"Complete the conversation job\"\"\"\n        if not self.job_id:\n            return\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/complete\",\n            headers=self.headers,\n            json={\"status\": status}\n        )\n        response.raise_for_status()\n        return response.json()\n\n\n# AutoGen Agent Example\ndef autogen_example():\n    \"\"\"\n    Multi-agent conversation with AutoGen and SaaS LiteLLM.\n\n    Note: This is a simplified example. In production, you'd integrate\n    more deeply with AutoGen's config system.\n    \"\"\"\n    import autogen\n\n    # Create SaaS wrapper\n    wrapper = SaaSAutoGenWrapper(\n        api_url=\"http://localhost:8003/api\",\n        virtual_key=\"sk-your-virtual-key\",\n        team_id=\"engineering-team\",\n        model_group=\"ChatAdvanced\"\n    )\n\n    # Start conversation job\n    job_id = wrapper.start_conversation_job(\n        metadata={\"conversation_type\": \"product_planning\"}\n    )\n    print(f\"Started AutoGen conversation job: {job_id}\")\n\n    # Get LLM config\n    llm_config = wrapper.get_llm_config(temperature=0.7)\n\n    # Create agents\n    user_proxy = autogen.UserProxyAgent(\n        name=\"User\",\n        human_input_mode=\"NEVER\",\n        max_consecutive_auto_reply=5,\n        code_execution_config={\"use_docker\": False}\n    )\n\n    product_manager = autogen.AssistantAgent(\n        name=\"ProductManager\",\n        system_message=\"You are a product manager. Focus on user needs and features.\",\n        llm_config=llm_config\n    )\n\n    engineer = autogen.AssistantAgent(\n        name=\"Engineer\",\n        system_message=\"You are a software engineer. Focus on technical feasibility.\",\n        llm_config=llm_config\n    )\n\n    # Simulate conversation\n    # In real AutoGen integration, you'd override the LLM client to use our wrapper\n    print(\"\\nStarting multi-agent conversation...\")\n\n    # Manually track calls (simplified)\n    messages = []\n\n    # User initiates\n    user_msg = \"We need to build a new feature for parsing resumes. What should we consider?\"\n    messages.append({\"role\": \"user\", \"content\": user_msg})\n\n    # Product Manager responds\n    pm_response = wrapper.make_agent_call(\n        messages=messages,\n        agent_name=\"ProductManager\",\n        temperature=0.7\n    )\n    print(f\"\\nProductManager: {pm_response}\")\n    messages.append({\"role\": \"assistant\", \"content\": pm_response})\n\n    # Engineer responds\n    eng_response = wrapper.make_agent_call(\n        messages=messages,\n        agent_name=\"Engineer\",\n        temperature=0.5\n    )\n    print(f\"\\nEngineer: {eng_response}\")\n\n    # Complete conversation\n    result = wrapper.complete_conversation()\n    print(f\"\\n\u2713 Conversation completed\")\n    print(f\"  Total calls: {result['costs']['total_calls']}\")\n    print(f\"  Total tokens: {result['costs']['total_tokens']}\")\n    print(f\"  Credits remaining: {result['costs']['credits_remaining']}\")\n\n\nif __name__ == \"__main__\":\n    autogen_example()\n</code></pre>"},{"location":"examples/agent-integration/#crewai-integration","title":"CrewAI Integration","text":"<p>CrewAI is a framework for orchestrating role-playing autonomous AI agents.</p>"},{"location":"examples/agent-integration/#crewai-wrapper","title":"CrewAI Wrapper","text":"<pre><code>from typing import List, Dict, Any, Optional\nimport requests\n\nclass SaaSCrewAIWrapper:\n    \"\"\"\n    Wrapper for using SaaS LiteLLM with CrewAI.\n\n    Tracks crew tasks as jobs with individual agent calls tracked.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n        virtual_key: str,\n        team_id: str,\n        model_group: str = \"ChatAdvanced\"\n    ):\n        self.api_url = api_url\n        self.virtual_key = virtual_key\n        self.team_id = team_id\n        self.model_group = model_group\n        self.current_job_id = None\n        self.agent_call_counts = {}\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def start_crew_task(self, task_name: str, metadata: Dict[str, Any] = None):\n        \"\"\"Start a job for a CrewAI task\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/jobs/create\",\n            headers=self.headers,\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": \"crewai_task\",\n                \"metadata\": {\n                    \"task_name\": task_name,\n                    **(metadata or {})\n                }\n            }\n        )\n        response.raise_for_status()\n        self.current_job_id = response.json()[\"job_id\"]\n        return self.current_job_id\n\n    def get_llm_config_for_agent(\n        self,\n        agent_role: str,\n        temperature: float = 0.7\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get LLM config for a specific CrewAI agent\"\"\"\n        return {\n            \"model_group\": self.model_group,\n            \"temperature\": temperature,\n            \"api_url\": self.api_url,\n            \"job_id\": self.current_job_id,\n            \"agent_role\": agent_role,\n            \"wrapper\": self\n        }\n\n    def make_agent_call(\n        self,\n        agent_role: str,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None\n    ) -&gt; str:\n        \"\"\"Make an LLM call for a specific agent\"\"\"\n        if not self.current_job_id:\n            raise ValueError(\"No task job started. Call start_crew_task() first.\")\n\n        # Track call count for this agent\n        if agent_role not in self.agent_call_counts:\n            self.agent_call_counts[agent_role] = 0\n        self.agent_call_counts[agent_role] += 1\n\n        call_num = self.agent_call_counts[agent_role]\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.current_job_id}/llm-call\",\n            headers=self.headers,\n            json={\n                \"model_group\": self.model_group,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens,\n                \"purpose\": f\"{agent_role}_call_{call_num}\"\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n        return result[\"response\"][\"content\"]\n\n    def complete_task(self, status: str = \"completed\", error_message: str = None):\n        \"\"\"Complete the crew task\"\"\"\n        if not self.current_job_id:\n            return\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.current_job_id}/complete\",\n            headers=self.headers,\n            json={\n                \"status\": status,\n                \"error_message\": error_message\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n\n        # Reset for next task\n        self.current_job_id = None\n        self.agent_call_counts = {}\n\n        return result\n\n\n# CrewAI Example\ndef crewai_example():\n    \"\"\"\n    Example using SaaS LiteLLM with CrewAI crew.\n\n    This shows how to track a multi-agent crew task through a single job.\n    \"\"\"\n    from crewai import Agent, Task, Crew\n\n    # Create wrapper\n    wrapper = SaaSCrewAIWrapper(\n        api_url=\"http://localhost:8003/api\",\n        virtual_key=\"sk-your-virtual-key\",\n        team_id=\"engineering-team\",\n        model_group=\"ChatAdvanced\"\n    )\n\n    # Start task\n    job_id = wrapper.start_crew_task(\n        task_name=\"market_research\",\n        metadata={\"project\": \"new_product_launch\"}\n    )\n    print(f\"Started CrewAI task job: {job_id}\")\n\n    # Define agents with SaaS LiteLLM integration\n    researcher = Agent(\n        role=\"Market Researcher\",\n        goal=\"Research market trends and customer needs\",\n        backstory=\"Expert in market analysis with 10 years experience\",\n        verbose=True,\n        # Pass our wrapper config\n        llm_config=wrapper.get_llm_config_for_agent(\"researcher\", temperature=0.7)\n    )\n\n    analyst = Agent(\n        role=\"Data Analyst\",\n        goal=\"Analyze research data and provide insights\",\n        backstory=\"Skilled data analyst with strong statistical background\",\n        verbose=True,\n        llm_config=wrapper.get_llm_config_for_agent(\"analyst\", temperature=0.5)\n    )\n\n    writer = Agent(\n        role=\"Content Writer\",\n        goal=\"Write compelling reports based on analysis\",\n        backstory=\"Professional writer with expertise in business content\",\n        verbose=True,\n        llm_config=wrapper.get_llm_config_for_agent(\"writer\", temperature=0.8)\n    )\n\n    # Define tasks\n    research_task = Task(\n        description=\"Research the AI resume parsing market\",\n        agent=researcher\n    )\n\n    analysis_task = Task(\n        description=\"Analyze the research findings and identify key opportunities\",\n        agent=analyst\n    )\n\n    writing_task = Task(\n        description=\"Write a one-page executive summary\",\n        agent=writer\n    )\n\n    # Create crew\n    crew = Crew(\n        agents=[researcher, analyst, writer],\n        tasks=[research_task, analysis_task, writing_task],\n        verbose=True\n    )\n\n    # In real CrewAI integration, you'd override the LLM client\n    # For this example, we'll simulate the calls\n    print(\"\\n=== Simulating Crew Execution ===\\n\")\n\n    # Researcher\n    research_result = wrapper.make_agent_call(\n        agent_role=\"researcher\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a market researcher. Research the AI resume parsing market.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"What are the key trends in AI resume parsing?\"\n            }\n        ],\n        temperature=0.7\n    )\n    print(f\"Researcher: {research_result[:200]}...\\n\")\n\n    # Analyst\n    analysis_result = wrapper.make_agent_call(\n        agent_role=\"analyst\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data analyst. Analyze market research.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Analyze this research: {research_result}\"\n            }\n        ],\n        temperature=0.5\n    )\n    print(f\"Analyst: {analysis_result[:200]}...\\n\")\n\n    # Writer\n    writing_result = wrapper.make_agent_call(\n        agent_role=\"writer\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a business writer. Write executive summaries.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Write an executive summary based on: {analysis_result}\"\n            }\n        ],\n        temperature=0.8\n    )\n    print(f\"Writer: {writing_result[:200]}...\\n\")\n\n    # Complete task\n    result = wrapper.complete_task()\n\n    print(\"=== Task Completed ===\")\n    print(f\"Job ID: {result['job_id']}\")\n    print(f\"Total calls: {result['costs']['total_calls']}\")\n    print(f\"  - Researcher: {wrapper.agent_call_counts.get('researcher', 0)} calls\")\n    print(f\"  - Analyst: {wrapper.agent_call_counts.get('analyst', 0)} calls\")\n    print(f\"  - Writer: {wrapper.agent_call_counts.get('writer', 0)} calls\")\n    print(f\"Total tokens: {result['costs']['total_tokens']}\")\n    print(f\"Total cost: ${result['costs']['total_cost_usd']:.6f}\")\n    print(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n\n\nif __name__ == \"__main__\":\n    crewai_example()\n</code></pre>"},{"location":"examples/agent-integration/#general-agent-wrapper-pattern","title":"General Agent Wrapper Pattern","text":"<p>Here's a reusable pattern for any agent framework:</p> <pre><code>from typing import Protocol, List, Dict, Any, Optional\nimport requests\nfrom contextlib import contextmanager\n\nclass AgentLLMWrapper(Protocol):\n    \"\"\"Protocol for agent framework LLM wrappers\"\"\"\n\n    def make_call(\n        self,\n        messages: List[Dict[str, str]],\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Make an LLM call\"\"\"\n        ...\n\n\nclass SaaSAgentBase:\n    \"\"\"\n    Base class for integrating any agent framework with SaaS LiteLLM.\n\n    Provides:\n    - Job lifecycle management\n    - Model group resolution\n    - Call tracking\n    - Error handling\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n        virtual_key: str,\n        team_id: str,\n        model_group: str,\n        job_type: str = \"agent_task\"\n    ):\n        self.api_url = api_url\n        self.virtual_key = virtual_key\n        self.team_id = team_id\n        self.model_group = model_group\n        self.job_type = job_type\n        self.job_id = None\n        self.call_count = 0\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def create_job(self, metadata: Dict[str, Any] = None) -&gt; str:\n        \"\"\"Create a job for this agent task\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/jobs/create\",\n            headers=self.headers,\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": self.job_type,\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        self.job_id = response.json()[\"job_id\"]\n        return self.job_id\n\n    def make_llm_call(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        purpose: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"Make an LLM call through the job\"\"\"\n        if not self.job_id:\n            self.create_job()\n\n        self.call_count += 1\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/llm-call\",\n            headers=self.headers,\n            json={\n                \"model_group\": self.model_group,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens,\n                \"purpose\": purpose or f\"call_{self.call_count}\"\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n        return result[\"response\"][\"content\"]\n\n    def complete_job(self, status: str = \"completed\", error: str = None) -&gt; Dict[str, Any]:\n        \"\"\"Complete the job and get cost summary\"\"\"\n        if not self.job_id:\n            return {}\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{self.job_id}/complete\",\n            headers=self.headers,\n            json={\n                \"status\": status,\n                \"error_message\": error\n            }\n        )\n        response.raise_for_status()\n        return response.json()\n\n    @contextmanager\n    def task_context(self, metadata: Dict[str, Any] = None):\n        \"\"\"\n        Context manager for agent tasks.\n\n        Usage:\n            with wrapper.task_context(metadata={\"task\": \"analysis\"}):\n                # Make LLM calls\n                result = wrapper.make_llm_call(messages)\n        \"\"\"\n        try:\n            self.create_job(metadata)\n            yield self\n            self.complete_job(status=\"completed\")\n        except Exception as e:\n            self.complete_job(status=\"failed\", error=str(e))\n            raise\n\n\n# Example usage with any framework\ndef generic_agent_example():\n    \"\"\"Example showing the general pattern\"\"\"\n\n    wrapper = SaaSAgentBase(\n        api_url=\"http://localhost:8003/api\",\n        virtual_key=\"sk-your-virtual-key\",\n        team_id=\"engineering-team\",\n        model_group=\"ChatAdvanced\",\n        job_type=\"custom_agent\"\n    )\n\n    # Use context manager for automatic job management\n    with wrapper.task_context(metadata={\"agent_type\": \"research\"}):\n\n        # Step 1: Initial research\n        research = wrapper.make_llm_call(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n                {\"role\": \"user\", \"content\": \"Research AI trends in 2024\"}\n            ],\n            temperature=0.7,\n            purpose=\"research_phase\"\n        )\n        print(f\"Research: {research[:100]}...\")\n\n        # Step 2: Analysis\n        analysis = wrapper.make_llm_call(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this: {research}\"}\n            ],\n            temperature=0.5,\n            purpose=\"analysis_phase\"\n        )\n        print(f\"Analysis: {analysis[:100]}...\")\n\n        # Step 3: Summary\n        summary = wrapper.make_llm_call(\n            messages=[\n                {\"role\": \"user\", \"content\": f\"Summarize: {analysis}\"}\n            ],\n            temperature=0.3,\n            purpose=\"summary_phase\"\n        )\n        print(f\"Summary: {summary}\")\n\n    print(\"Task completed automatically!\")\n</code></pre>"},{"location":"examples/agent-integration/#best-practices-for-agent-integration","title":"Best Practices for Agent Integration","text":""},{"location":"examples/agent-integration/#1-job-granularity","title":"1. Job Granularity","text":"<pre><code># \u2705 GOOD: One job per agent task/conversation\nwith wrapper.task_context():\n    # All calls for this task\n    result1 = wrapper.make_llm_call(...)\n    result2 = wrapper.make_llm_call(...)\n    result3 = wrapper.make_llm_call(...)\n\n# \u274c BAD: Creating a new job for each call\nfor query in queries:\n    wrapper.create_job()\n    wrapper.make_llm_call(...)\n    wrapper.complete_job()\n</code></pre>"},{"location":"examples/agent-integration/#2-purpose-tracking","title":"2. Purpose Tracking","text":"<pre><code># \u2705 GOOD: Descriptive purposes for each call\nwrapper.make_llm_call(\n    messages=messages,\n    purpose=\"initial_research\"\n)\n\nwrapper.make_llm_call(\n    messages=messages,\n    purpose=\"follow_up_analysis\"\n)\n\n# \u274c BAD: No purpose or generic purposes\nwrapper.make_llm_call(messages=messages)\nwrapper.make_llm_call(messages=messages, purpose=\"call\")\n</code></pre>"},{"location":"examples/agent-integration/#3-error-handling","title":"3. Error Handling","text":"<pre><code># \u2705 GOOD: Proper error handling\ntry:\n    wrapper.create_job(metadata={\"task\": \"analysis\"})\n    result = wrapper.make_llm_call(messages)\n    wrapper.complete_job(status=\"completed\")\nexcept Exception as e:\n    wrapper.complete_job(status=\"failed\", error=str(e))\n    raise\n</code></pre>"},{"location":"examples/agent-integration/#4-metadata-usage","title":"4. Metadata Usage","text":"<pre><code># \u2705 GOOD: Rich metadata for tracking\nwrapper.create_job(metadata={\n    \"agent_framework\": \"langchain\",\n    \"agent_type\": \"conversational\",\n    \"user_id\": \"user_123\",\n    \"session_id\": \"session_456\",\n    \"conversation_topic\": \"customer_support\"\n})\n</code></pre>"},{"location":"examples/agent-integration/#framework-comparison","title":"Framework Comparison","text":"Framework Best Use Case Integration Complexity Example LangChain Chains, RAG, general agents Medium Document analysis, Q&amp;A AutoGen Multi-agent conversations Medium Team collaboration, debates CrewAI Role-based agent crews High Complex workflows, tasks Custom Specific business logic Low Custom agent implementations"},{"location":"examples/agent-integration/#next-steps","title":"Next Steps","text":"<ol> <li>Full Chain Example - Complete workflow walkthrough</li> <li>Streaming Examples - Real-time agent responses</li> <li>Structured Outputs - Type-safe agent outputs</li> <li>Error Handling - Production error handling</li> <li>Best Practices - Production deployment guide</li> </ol>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>Simple, working code examples to get you started with SaaS LiteLLM.</p>"},{"location":"examples/basic-usage/#prerequisites","title":"Prerequisites","text":"<p>Before running these examples:</p> <ol> <li>Services Running:</li> <li>SaaS API on http://localhost:8003</li> <li> <p>LiteLLM Backend on http://localhost:8002</p> </li> <li> <p>Team Created:</p> </li> <li>Organization and team set up</li> <li>Virtual key obtained</li> <li> <p>Credits allocated</p> </li> <li> <p>Environment Variables:    <pre><code>export SAAS_LITELLM_VIRTUAL_KEY=\"sk-your-virtual-key\"\nexport SAAS_LITELLM_TEAM_ID=\"your-team-id\"\n</code></pre></p> </li> </ol>"},{"location":"examples/basic-usage/#example-1-simple-llm-call","title":"Example 1: Simple LLM Call","text":"<p>The most basic workflow: create job \u2192 make LLM call \u2192 complete job.</p> <pre><code>import requests\nimport os\n\n# Configuration\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ.get(\"SAAS_LITELLM_VIRTUAL_KEY\")\nTEAM_ID = os.environ.get(\"SAAS_LITELLM_TEAM_ID\")\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef simple_llm_call():\n    \"\"\"Simple LLM call example\"\"\"\n\n    # 1. Create job\n    print(\"Creating job...\")\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\n            \"team_id\": TEAM_ID,\n            \"job_type\": \"simple_query\"\n        }\n    )\n    job = job_response.json()\n    job_id = job[\"job_id\"]\n    print(f\"Created job: {job_id}\")\n\n    # 2. Make LLM call\n    print(\"\\nMaking LLM call...\")\n    llm_response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"What is Python programming?\"}\n            ]\n        }\n    )\n    llm_result = llm_response.json()\n    content = llm_result[\"response\"][\"content\"]\n    print(f\"\\nResponse:\\n{content}\")\n\n    # 3. Complete job\n    print(\"\\nCompleting job...\")\n    complete_response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/complete\",\n        headers=headers,\n        json={\"status\": \"completed\"}\n    )\n    result = complete_response.json()\n    print(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n\nif __name__ == \"__main__\":\n    simple_llm_call()\n</code></pre> <p>Expected Output: <pre><code>Creating job...\nCreated job: 550e8400-e29b-41d4-a716-446655440000\n\nMaking LLM call...\n\nResponse:\nPython is a high-level, interpreted programming language...\n\nCompleting job...\nCredits remaining: 95\n</code></pre></p>"},{"location":"examples/basic-usage/#example-2-multi-turn-conversation","title":"Example 2: Multi-Turn Conversation","text":"<p>Have a conversation with context:</p> <pre><code>import requests\nimport os\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef multi_turn_conversation():\n    \"\"\"Multi-turn conversation with context\"\"\"\n\n    # Create job\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\"team_id\": TEAM_ID, \"job_type\": \"conversation\"}\n    )\n    job_id = job_response.json()[\"job_id\"]\n\n    messages = []\n\n    # Turn 1\n    print(\"User: What is FastAPI?\")\n    messages.append({\"role\": \"user\", \"content\": \"What is FastAPI?\"})\n\n    response1 = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    ).json()\n\n    assistant_reply1 = response1[\"response\"][\"content\"]\n    print(f\"Assistant: {assistant_reply1}\\n\")\n    messages.append({\"role\": \"assistant\", \"content\": assistant_reply1})\n\n    # Turn 2 - Builds on previous context\n    print(\"User: How is it different from Flask?\")\n    messages.append({\"role\": \"user\", \"content\": \"How is it different from Flask?\"})\n\n    response2 = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    ).json()\n\n    assistant_reply2 = response2[\"response\"][\"content\"]\n    print(f\"Assistant: {assistant_reply2}\\n\")\n\n    # Complete job\n    result = requests.post(\n        f\"{API_URL}/jobs/{job_id}/complete\",\n        headers=headers,\n        json={\"status\": \"completed\"}\n    ).json()\n\n    print(f\"Conversation complete. Credits remaining: {result['costs']['credits_remaining']}\")\n\nif __name__ == \"__main__\":\n    multi_turn_conversation()\n</code></pre>"},{"location":"examples/basic-usage/#example-3-document-analysis","title":"Example 3: Document Analysis","text":"<p>Analyze a document with multiple LLM calls in one job:</p> <pre><code>import requests\nimport os\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef analyze_document(document_text: str):\n    \"\"\"Analyze a document: extract key points, generate summary\"\"\"\n\n    # Create job\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\n            \"team_id\": TEAM_ID,\n            \"job_type\": \"document_analysis\",\n            \"metadata\": {\"document_length\": len(document_text)}\n        }\n    )\n    job_id = job_response.json()[\"job_id\"]\n    print(f\"Analyzing document (Job ID: {job_id})...\\n\")\n\n    # Step 1: Extract key points\n    print(\"Extracting key points...\")\n    key_points_response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"Extract key points as bullet points.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Extract key points from:\\n\\n{document_text}\"\n                }\n            ],\n            \"temperature\": 0.3,\n            \"max_tokens\": 500\n        }\n    ).json()\n\n    key_points = key_points_response[\"response\"][\"content\"]\n    print(f\"Key Points:\\n{key_points}\\n\")\n\n    # Step 2: Generate summary\n    print(\"Generating summary...\")\n    summary_response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"Create concise summaries.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Summarize in 2-3 sentences:\\n\\n{document_text}\"\n                }\n            ],\n            \"temperature\": 0.5,\n            \"max_tokens\": 200\n        }\n    ).json()\n\n    summary = summary_response[\"response\"][\"content\"]\n    print(f\"Summary:\\n{summary}\\n\")\n\n    # Complete job\n    result = requests.post(\n        f\"{API_URL}/jobs/{job_id}/complete\",\n        headers=headers,\n        json={\"status\": \"completed\"}\n    ).json()\n\n    print(f\"Analysis complete!\")\n    print(f\"Total calls: {result['costs']['total_calls']}\")\n    print(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n\n    return {\n        \"key_points\": key_points,\n        \"summary\": summary\n    }\n\nif __name__ == \"__main__\":\n    document = \"\"\"\n    Artificial intelligence (AI) is transforming industries worldwide.\n    Machine learning algorithms can now process vast amounts of data\n    and identify patterns that humans might miss. This technology is\n    being applied in healthcare for disease diagnosis, in finance for\n    fraud detection, and in transportation for autonomous vehicles.\n    \"\"\"\n\n    result = analyze_document(document)\n</code></pre>"},{"location":"examples/basic-usage/#example-4-error-handling","title":"Example 4: Error Handling","text":"<p>Handle errors gracefully:</p> <pre><code>import requests\nimport os\nimport time\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef llm_call_with_error_handling(prompt: str, max_retries=3):\n    \"\"\"Make LLM call with comprehensive error handling\"\"\"\n\n    # Create job\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\"team_id\": TEAM_ID, \"job_type\": \"query\"}\n    )\n    job_id = job_response.json()[\"job_id\"]\n\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(\n                f\"{API_URL}/jobs/{job_id}/llm-call\",\n                headers=headers,\n                json={\n                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n                },\n                timeout=30\n            )\n            response.raise_for_status()\n\n            # Success\n            result = response.json()\n            content = result[\"response\"][\"content\"]\n\n            # Complete job\n            requests.post(\n                f\"{API_URL}/jobs/{job_id}/complete\",\n                headers=headers,\n                json={\"status\": \"completed\"}\n            )\n\n            return content\n\n        except requests.exceptions.Timeout:\n            print(f\"Timeout (attempt {attempt + 1}/{max_retries})\")\n            if attempt &lt; max_retries - 1:\n                time.sleep(2 ** attempt)\n            else:\n                # Mark job as failed\n                requests.post(\n                    f\"{API_URL}/jobs/{job_id}/complete\",\n                    headers=headers,\n                    json={\"status\": \"failed\"}\n                )\n                raise\n\n        except requests.exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            detail = e.response.json().get(\"detail\", \"Unknown error\")\n\n            if status_code == 401:\n                print(\"Authentication failed\")\n                raise\n\n            elif status_code == 403:\n                print(f\"Access denied: {detail}\")\n                raise\n\n            elif status_code == 429:\n                print(f\"Rate limited (attempt {attempt + 1}/{max_retries})\")\n                if attempt &lt; max_retries - 1:\n                    time.sleep(2 ** attempt)\n                else:\n                    raise\n\n            elif status_code in [500, 503]:\n                print(f\"Server error (attempt {attempt + 1}/{max_retries})\")\n                if attempt &lt; max_retries - 1:\n                    time.sleep(2 ** attempt)\n                else:\n                    raise\n\n            else:\n                print(f\"HTTP {status_code}: {detail}\")\n                raise\n\nif __name__ == \"__main__\":\n    try:\n        response = llm_call_with_error_handling(\"What is Python?\")\n        print(f\"Response: {response}\")\n    except Exception as e:\n        print(f\"Failed after retries: {e}\")\n</code></pre>"},{"location":"examples/basic-usage/#example-5-using-type-safe-client","title":"Example 5: Using Type-Safe Client","text":"<p>Use the provided Python client for easier integration:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def typed_client_example():\n    \"\"\"Example using the type-safe client\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key\"\n    ) as client:\n\n        # Create job\n        job_id = await client.create_job(\"simple_query\")\n        print(f\"Created job: {job_id}\")\n\n        # Make LLM call\n        response = await client.chat(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Explain recursion in Python\"}\n            ]\n        )\n\n        print(f\"\\nResponse:\\n{response.choices[0].message['content']}\")\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\nCredits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(typed_client_example())\n</code></pre>"},{"location":"examples/basic-usage/#example-6-batch-processing","title":"Example 6: Batch Processing","text":"<p>Process multiple items efficiently:</p> <pre><code>import requests\nimport os\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef batch_classify(texts: list):\n    \"\"\"Classify multiple texts in one job\"\"\"\n\n    # Create job\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\n            \"team_id\": TEAM_ID,\n            \"job_type\": \"batch_classification\",\n            \"metadata\": {\"count\": len(texts)}\n        }\n    )\n    job_id = job_response.json()[\"job_id\"]\n\n    results = []\n\n    # Process each text\n    for i, text in enumerate(texts, 1):\n        print(f\"Classifying text {i}/{len(texts)}...\")\n\n        response = requests.post(\n            f\"{API_URL}/jobs/{job_id}/llm-call\",\n            headers=headers,\n            json={\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Classify sentiment as positive, negative, or neutral.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": text\n                    }\n                ],\n                \"temperature\": 0.3\n            }\n        ).json()\n\n        classification = response[\"response\"][\"content\"].strip().lower()\n        results.append({\n            \"text\": text,\n            \"sentiment\": classification\n        })\n\n    # Complete job\n    complete_response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/complete\",\n        headers=headers,\n        json={\"status\": \"completed\"}\n    ).json()\n\n    print(f\"\\nBatch complete!\")\n    print(f\"Processed {len(texts)} texts\")\n    print(f\"Credits remaining: {complete_response['costs']['credits_remaining']}\")\n\n    return results\n\nif __name__ == \"__main__\":\n    texts = [\n        \"This product is amazing! I love it.\",\n        \"Terrible experience. Would not recommend.\",\n        \"It's okay, nothing special.\",\n    ]\n\n    results = batch_classify(texts)\n\n    print(\"\\nResults:\")\n    for result in results:\n        print(f\"  '{result['text']}' \u2192 {result['sentiment']}\")\n</code></pre>"},{"location":"examples/basic-usage/#example-7-check-credit-balance","title":"Example 7: Check Credit Balance","text":"<p>Monitor your team's credit balance:</p> <pre><code>import requests\nimport os\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef check_credits():\n    \"\"\"Check team credit balance\"\"\"\n\n    # Get team info\n    response = requests.get(\n        f\"{API_URL}/teams/{TEAM_ID}\",\n        headers=headers\n    )\n\n    team = response.json()\n\n    print(f\"Team: {team['team_id']}\")\n    print(f\"Organization: {team['organization_id']}\")\n    print(f\"Status: {team['status']}\")\n    print(f\"Credits allocated: {team['credits_allocated']}\")\n    print(f\"Credits remaining: {team['credits_remaining']}\")\n\n    # Calculate usage\n    credits_used = team['credits_allocated'] - team['credits_remaining']\n    usage_percent = (credits_used / team['credits_allocated']) * 100\n\n    print(f\"Credits used: {credits_used}\")\n    print(f\"Usage: {usage_percent:.1f}%\")\n\n    # Warnings\n    if team['credits_remaining'] &lt; team['credits_allocated'] * 0.1:\n        print(\"\\n\u26a0\ufe0f  WARNING: Less than 10% of credits remaining!\")\n\nif __name__ == \"__main__\":\n    check_credits()\n</code></pre>"},{"location":"examples/basic-usage/#running-the-examples","title":"Running the Examples","text":""},{"location":"examples/basic-usage/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>pip install requests\n</code></pre>"},{"location":"examples/basic-usage/#2-set-environment-variables","title":"2. Set Environment Variables","text":"<pre><code>export SAAS_LITELLM_VIRTUAL_KEY=\"sk-your-virtual-key\"\nexport SAAS_LITELLM_TEAM_ID=\"your-team-id\"\n</code></pre>"},{"location":"examples/basic-usage/#3-run-examples","title":"3. Run Examples","text":"<pre><code># Example 1: Simple LLM call\npython example1_simple.py\n\n# Example 2: Multi-turn conversation\npython example2_conversation.py\n\n# Example 3: Document analysis\npython example3_document.py\n\n# Example 4: Error handling\npython example4_errors.py\n\n# Example 5: Type-safe client\npython example5_typed_client.py\n\n# Example 6: Batch processing\npython example6_batch.py\n\n# Example 7: Check credits\npython example7_credits.py\n</code></pre>"},{"location":"examples/basic-usage/#common-patterns","title":"Common Patterns","text":""},{"location":"examples/basic-usage/#pattern-1-job-wrapper","title":"Pattern 1: Job Wrapper","text":"<p>Wrap job lifecycle management:</p> <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef managed_job(team_id: str, job_type: str):\n    \"\"\"Context manager for automatic job completion\"\"\"\n\n    # Create job\n    job_response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\"team_id\": team_id, \"job_type\": job_type}\n    )\n    job_id = job_response.json()[\"job_id\"]\n\n    try:\n        yield job_id\n        # Success - complete job\n        requests.post(\n            f\"{API_URL}/jobs/{job_id}/complete\",\n            headers=headers,\n            json={\"status\": \"completed\"}\n        )\n    except Exception as e:\n        # Failure - mark as failed\n        requests.post(\n            f\"{API_URL}/jobs/{job_id}/complete\",\n            headers=headers,\n            json={\"status\": \"failed\"}\n        )\n        raise\n\n# Usage\nwith managed_job(TEAM_ID, \"query\") as job_id:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}\n    )\n</code></pre>"},{"location":"examples/basic-usage/#pattern-2-response-helper","title":"Pattern 2: Response Helper","text":"<p>Extract content easily:</p> <pre><code>def get_llm_response(job_id: str, prompt: str) -&gt; str:\n    \"\"\"Make LLM call and extract content\"\"\"\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n        }\n    ).json()\n\n    return response[\"response\"][\"content\"]\n\n# Usage\nwith managed_job(TEAM_ID, \"query\") as job_id:\n    answer = get_llm_response(job_id, \"What is Python?\")\n    print(answer)\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you've seen basic usage:</p> <ol> <li>Try Streaming Examples - Real-time responses</li> <li>Try Structured Outputs - Type-safe responses</li> <li>See Full Chain Example - Complete application flow</li> <li>Review Integration Guides - Detailed documentation</li> <li>Explore API Reference - Complete API docs</li> </ol>"},{"location":"examples/full-chain/","title":"Full Chain Example","text":"<p>Complete end-to-end workflow demonstrating the full SaaS LiteLLM platform lifecycle.</p>"},{"location":"examples/full-chain/#overview","title":"Overview","text":"<p>This example shows the complete workflow from organization setup to making LLM calls:</p> <ol> <li>Create organization</li> <li>Create model groups</li> <li>Create team with credits</li> <li>Create job</li> <li>Make LLM calls</li> <li>Complete job</li> <li>Track credits</li> </ol>"},{"location":"examples/full-chain/#prerequisites","title":"Prerequisites","text":"<ul> <li>SaaS API running on <code>http://localhost:8003</code></li> <li>LiteLLM Backend running on <code>http://localhost:8002</code></li> <li>LiteLLM configured with at least one model (e.g., <code>gpt-3.5-turbo</code>)</li> </ul>"},{"location":"examples/full-chain/#python-example","title":"Python Example","text":""},{"location":"examples/full-chain/#complete-workflow","title":"Complete Workflow","text":"<pre><code>import requests\nimport json\nfrom typing import Dict, Any\n\n# Configuration\nAPI_URL = \"http://localhost:8003/api\"\nADMIN_KEY = \"sk-admin-key-change-me\"  # For org/team creation\n\nclass SaaSLiteLLMClient:\n    \"\"\"Complete client for SaaS LiteLLM platform\"\"\"\n\n    def __init__(self, api_url: str = \"http://localhost:8003/api\"):\n        self.api_url = api_url\n        self.admin_headers = {\n            \"Authorization\": f\"Bearer {ADMIN_KEY}\",\n            \"Content-Type\": \"application/json\"\n        }\n        self.team_headers = None\n        self.team_id = None\n        self.virtual_key = None\n\n    # ========================================================================\n    # Step 1: Organization Management\n    # ========================================================================\n\n    def create_organization(self, org_id: str, name: str, metadata: Dict = None) -&gt; Dict[str, Any]:\n        \"\"\"Create a new organization\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/organizations/create\",\n            headers=self.admin_headers,\n            json={\n                \"organization_id\": org_id,\n                \"name\": name,\n                \"metadata\": metadata or {}\n            }\n        )\n\n        if response.status_code == 400 and \"already exists\" in response.text:\n            print(f\"Organization {org_id} already exists, fetching details...\")\n            return self.get_organization(org_id)\n\n        response.raise_for_status()\n        return response.json()\n\n    def get_organization(self, org_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Get organization details\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/organizations/{org_id}\",\n            headers=self.admin_headers\n        )\n        response.raise_for_status()\n        return response.json()\n\n    # ========================================================================\n    # Step 2: Model Group Management\n    # ========================================================================\n\n    def create_model_group(\n        self,\n        group_name: str,\n        display_name: str,\n        description: str,\n        models: list\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Create a model group with prioritized models\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/model-groups/create\",\n            headers=self.admin_headers,\n            json={\n                \"group_name\": group_name,\n                \"display_name\": display_name,\n                \"description\": description,\n                \"models\": models\n            }\n        )\n\n        if response.status_code == 400 and \"already exists\" in response.text:\n            print(f\"Model group {group_name} already exists\")\n            return self.get_model_group(group_name)\n\n        response.raise_for_status()\n        return response.json()\n\n    def get_model_group(self, group_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get model group details\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/model-groups/{group_name}\",\n            headers=self.admin_headers\n        )\n        response.raise_for_status()\n        return response.json()\n\n    # ========================================================================\n    # Step 3: Team Management\n    # ========================================================================\n\n    def create_team(\n        self,\n        organization_id: str,\n        team_id: str,\n        team_alias: str,\n        model_groups: list,\n        credits_allocated: int\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Create team with LiteLLM integration and credits\"\"\"\n        response = requests.post(\n            f\"{self.api_url}/teams/create\",\n            headers=self.admin_headers,\n            json={\n                \"organization_id\": organization_id,\n                \"team_id\": team_id,\n                \"team_alias\": team_alias,\n                \"model_groups\": model_groups,\n                \"credits_allocated\": credits_allocated\n            }\n        )\n\n        if response.status_code == 400 and \"already exists\" in response.text:\n            print(f\"Team {team_id} already exists, fetching details...\")\n            return self.get_team(team_id)\n\n        response.raise_for_status()\n        result = response.json()\n\n        # Store virtual key for future requests\n        self.virtual_key = result.get(\"virtual_key\")\n        self.team_id = team_id\n        self.team_headers = {\n            \"Authorization\": f\"Bearer {self.virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        return result\n\n    def get_team(self, team_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Get team details\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/teams/{team_id}\",\n            headers=self.admin_headers\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def set_virtual_key(self, virtual_key: str, team_id: str):\n        \"\"\"Set virtual key for team operations\"\"\"\n        self.virtual_key = virtual_key\n        self.team_id = team_id\n        self.team_headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    # ========================================================================\n    # Step 4: Credit Management\n    # ========================================================================\n\n    def get_credit_balance(self) -&gt; Dict[str, Any]:\n        \"\"\"Get credit balance for authenticated team\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated. Call create_team or set_virtual_key first.\")\n\n        response = requests.get(\n            f\"{self.api_url}/credits/teams/{self.team_id}/balance\",\n            headers=self.team_headers\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def add_credits(self, credits: int, reason: str = \"Manual allocation\") -&gt; Dict[str, Any]:\n        \"\"\"Add credits to team\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated\")\n\n        response = requests.post(\n            f\"{self.api_url}/credits/teams/{self.team_id}/add\",\n            headers=self.team_headers,\n            json={\n                \"credits\": credits,\n                \"reason\": reason\n            }\n        )\n        response.raise_for_status()\n        return response.json()\n\n    # ========================================================================\n    # Step 5: Job &amp; LLM Call Workflow\n    # ========================================================================\n\n    def create_job(self, job_type: str, metadata: Dict = None) -&gt; str:\n        \"\"\"Create a new job\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated\")\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/create\",\n            headers=self.team_headers,\n            json={\n                \"team_id\": self.team_id,\n                \"job_type\": job_type,\n                \"metadata\": metadata or {}\n            }\n        )\n        response.raise_for_status()\n        result = response.json()\n        return result[\"job_id\"]\n\n    def make_llm_call(\n        self,\n        job_id: str,\n        model_group: str,\n        messages: list,\n        purpose: str = None,\n        temperature: float = 0.7,\n        max_tokens: int = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Make an LLM call within a job\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated\")\n\n        payload = {\n            \"model_group\": model_group,\n            \"messages\": messages,\n            \"temperature\": temperature\n        }\n\n        if purpose:\n            payload[\"purpose\"] = purpose\n        if max_tokens:\n            payload[\"max_tokens\"] = max_tokens\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{job_id}/llm-call\",\n            headers=self.team_headers,\n            json=payload\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def complete_job(\n        self,\n        job_id: str,\n        status: str = \"completed\",\n        error_message: str = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Complete a job and get cost summary\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated\")\n\n        payload = {\"status\": status}\n        if error_message:\n            payload[\"error_message\"] = error_message\n\n        response = requests.post(\n            f\"{self.api_url}/jobs/{job_id}/complete\",\n            headers=self.team_headers,\n            json=payload\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def get_job(self, job_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Get job details\"\"\"\n        if not self.team_headers:\n            raise ValueError(\"No team authenticated\")\n\n        response = requests.get(\n            f\"{self.api_url}/jobs/{job_id}\",\n            headers=self.team_headers\n        )\n        response.raise_for_status()\n        return response.json()\n\n\n# ============================================================================\n# Complete End-to-End Example\n# ============================================================================\n\ndef full_workflow_example():\n    \"\"\"\n    Complete workflow:\n    1. Create organization\n    2. Create model groups\n    3. Create team\n    4. Create job\n    5. Make LLM calls\n    6. Complete job\n    7. Check credits\n    \"\"\"\n\n    client = SaaSLiteLLMClient()\n\n    print(\"=\"*70)\n    print(\"SAAS LITELLM - FULL WORKFLOW EXAMPLE\")\n    print(\"=\"*70)\n\n    # Step 1: Create Organization\n    print(\"\\n[1/7] Creating Organization...\")\n    org = client.create_organization(\n        org_id=\"acme-corp\",\n        name=\"Acme Corporation\",\n        metadata={\"industry\": \"technology\", \"tier\": \"enterprise\"}\n    )\n    print(f\"\u2713 Organization created: {org['organization_id']}\")\n\n    # Step 2: Create Model Groups\n    print(\"\\n[2/7] Creating Model Groups...\")\n\n    # Fast model group for quick tasks\n    fast_group = client.create_model_group(\n        group_name=\"ChatFast\",\n        display_name=\"Fast Chat Model\",\n        description=\"Quick responses for simple queries\",\n        models=[\n            {\"model_name\": \"gpt-3.5-turbo\", \"priority\": 0},\n            {\"model_name\": \"gpt-4o-mini\", \"priority\": 1}\n        ]\n    )\n    print(f\"\u2713 Model group created: {fast_group['group_name']}\")\n\n    # Advanced model group for complex tasks\n    advanced_group = client.create_model_group(\n        group_name=\"ChatAdvanced\",\n        display_name=\"Advanced Chat Model\",\n        description=\"Powerful model for complex reasoning\",\n        models=[\n            {\"model_name\": \"gpt-4\", \"priority\": 0},\n            {\"model_name\": \"gpt-4-turbo\", \"priority\": 1}\n        ]\n    )\n    print(f\"\u2713 Model group created: {advanced_group['group_name']}\")\n\n    # Step 3: Create Team\n    print(\"\\n[3/7] Creating Team with Credits...\")\n    team = client.create_team(\n        organization_id=\"acme-corp\",\n        team_id=\"engineering-team\",\n        team_alias=\"Engineering Team\",\n        model_groups=[\"ChatFast\", \"ChatAdvanced\"],\n        credits_allocated=100\n    )\n    print(f\"\u2713 Team created: {team['team_id']}\")\n    print(f\"  Virtual Key: {team['virtual_key'][:50]}...\")\n    print(f\"  Credits: {team['credits_allocated']}\")\n    print(f\"  Model Groups: {', '.join(team['model_groups'])}\")\n\n    # Step 4: Check Initial Credits\n    print(\"\\n[4/7] Checking Initial Credit Balance...\")\n    credits = client.get_credit_balance()\n    print(f\"\u2713 Credits available: {credits['credits_remaining']}/{credits['credits_allocated']}\")\n\n    # Step 5: Create Job\n    print(\"\\n[5/7] Creating Job...\")\n    job_id = client.create_job(\n        job_type=\"customer_support\",\n        metadata={\n            \"customer_id\": \"customer_12345\",\n            \"priority\": \"high\"\n        }\n    )\n    print(f\"\u2713 Job created: {job_id}\")\n\n    # Step 6: Make LLM Calls\n    print(\"\\n[6/7] Making LLM Calls...\")\n\n    # First call - initial response\n    print(\"  \u2192 Call 1: Initial customer query...\")\n    response1 = client.make_llm_call(\n        job_id=job_id,\n        model_group=\"ChatFast\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful customer support assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How do I reset my password?\"\n            }\n        ],\n        purpose=\"initial_response\"\n    )\n    print(f\"    Response: {response1['response']['content'][:100]}...\")\n    print(f\"    Tokens: {response1['metadata']['tokens_used']}\")\n    print(f\"    Latency: {response1['metadata']['latency_ms']}ms\")\n\n    # Second call - follow-up\n    print(\"\\n  \u2192 Call 2: Follow-up question...\")\n    response2 = client.make_llm_call(\n        job_id=job_id,\n        model_group=\"ChatFast\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful customer support assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How do I reset my password?\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": response1['response']['content']\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"I didn't receive the reset email.\"\n            }\n        ],\n        purpose=\"follow_up\"\n    )\n    print(f\"    Response: {response2['response']['content'][:100]}...\")\n    print(f\"    Tokens: {response2['metadata']['tokens_used']}\")\n\n    # Third call - complex analysis using advanced model\n    print(\"\\n  \u2192 Call 3: Complex analysis...\")\n    response3 = client.make_llm_call(\n        job_id=job_id,\n        model_group=\"ChatAdvanced\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Analyze this customer interaction and suggest improvements to our password reset process.\"\n            }\n        ],\n        purpose=\"analysis\",\n        temperature=0.3\n    )\n    print(f\"    Response: {response3['response']['content'][:100]}...\")\n    print(f\"    Tokens: {response3['metadata']['tokens_used']}\")\n\n    # Step 7: Complete Job\n    print(\"\\n[7/7] Completing Job...\")\n    completion = client.complete_job(\n        job_id=job_id,\n        status=\"completed\"\n    )\n\n    print(f\"\u2713 Job completed: {completion['job_id']}\")\n    print(f\"\\n  Cost Summary:\")\n    print(f\"    Total calls: {completion['costs']['total_calls']}\")\n    print(f\"    Successful: {completion['costs']['successful_calls']}\")\n    print(f\"    Failed: {completion['costs']['failed_calls']}\")\n    print(f\"    Total tokens: {completion['costs']['total_tokens']}\")\n    print(f\"    Total cost: ${completion['costs']['total_cost_usd']:.6f}\")\n    print(f\"    Avg latency: {completion['costs']['avg_latency_ms']}ms\")\n    print(f\"    Credit applied: {completion['costs']['credit_applied']}\")\n    print(f\"    Credits remaining: {completion['costs']['credits_remaining']}\")\n\n    # Final credit check\n    print(\"\\n[FINAL] Credit Balance After Job...\")\n    final_credits = client.get_credit_balance()\n    print(f\"\u2713 Credits: {final_credits['credits_remaining']}/{final_credits['credits_allocated']}\")\n    print(f\"  Credits used: {final_credits['credits_used']}\")\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"WORKFLOW COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*70)\n\n    return {\n        \"organization\": org,\n        \"team\": team,\n        \"job_id\": job_id,\n        \"completion\": completion\n    }\n\n\nif __name__ == \"__main__\":\n    try:\n        result = full_workflow_example()\n        print(\"\\n\u2705 All steps completed successfully!\")\n    except requests.exceptions.ConnectionError as e:\n        print(f\"\\n\u274c Connection Error: Could not connect to API\")\n        print(\"Make sure SaaS API is running: http://localhost:8003\")\n    except Exception as e:\n        print(f\"\\n\u274c Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n</code></pre>"},{"location":"examples/full-chain/#javascriptnodejs-example","title":"JavaScript/Node.js Example","text":"<pre><code>const axios = require('axios');\n\nclass SaaSLiteLLMClient {\n    constructor(apiUrl = 'http://localhost:8003/api') {\n        this.apiUrl = apiUrl;\n        this.adminKey = 'sk-admin-key-change-me';\n        this.virtualKey = null;\n        this.teamId = null;\n    }\n\n    // Create Organization\n    async createOrganization(orgId, name, metadata = {}) {\n        try {\n            const response = await axios.post(\n                `${this.apiUrl}/organizations/create`,\n                {\n                    organization_id: orgId,\n                    name: name,\n                    metadata: metadata\n                },\n                {\n                    headers: {\n                        'Authorization': `Bearer ${this.adminKey}`,\n                        'Content-Type': 'application/json'\n                    }\n                }\n            );\n            return response.data;\n        } catch (error) {\n            if (error.response?.status === 400 &amp;&amp; error.response.data.detail?.includes('already exists')) {\n                return await this.getOrganization(orgId);\n            }\n            throw error;\n        }\n    }\n\n    // Create Model Group\n    async createModelGroup(groupName, displayName, description, models) {\n        try {\n            const response = await axios.post(\n                `${this.apiUrl}/model-groups/create`,\n                {\n                    group_name: groupName,\n                    display_name: displayName,\n                    description: description,\n                    models: models\n                },\n                {\n                    headers: {\n                        'Authorization': `Bearer ${this.adminKey}`,\n                        'Content-Type': 'application/json'\n                    }\n                }\n            );\n            return response.data;\n        } catch (error) {\n            if (error.response?.status === 400) {\n                return await this.getModelGroup(groupName);\n            }\n            throw error;\n        }\n    }\n\n    // Create Team\n    async createTeam(organizationId, teamId, teamAlias, modelGroups, creditsAllocated) {\n        const response = await axios.post(\n            `${this.apiUrl}/teams/create`,\n            {\n                organization_id: organizationId,\n                team_id: teamId,\n                team_alias: teamAlias,\n                model_groups: modelGroups,\n                credits_allocated: creditsAllocated\n            },\n            {\n                headers: {\n                    'Authorization': `Bearer ${this.adminKey}`,\n                    'Content-Type': 'application/json'\n                }\n            }\n        );\n\n        this.virtualKey = response.data.virtual_key;\n        this.teamId = teamId;\n        return response.data;\n    }\n\n    // Get team headers\n    getTeamHeaders() {\n        if (!this.virtualKey) {\n            throw new Error('No team authenticated');\n        }\n        return {\n            'Authorization': `Bearer ${this.virtualKey}`,\n            'Content-Type': 'application/json'\n        };\n    }\n\n    // Create Job\n    async createJob(jobType, metadata = {}) {\n        const response = await axios.post(\n            `${this.apiUrl}/jobs/create`,\n            {\n                team_id: this.teamId,\n                job_type: jobType,\n                metadata: metadata\n            },\n            { headers: this.getTeamHeaders() }\n        );\n        return response.data.job_id;\n    }\n\n    // Make LLM Call\n    async makeLLMCall(jobId, modelGroup, messages, options = {}) {\n        const response = await axios.post(\n            `${this.apiUrl}/jobs/${jobId}/llm-call`,\n            {\n                model_group: modelGroup,\n                messages: messages,\n                temperature: options.temperature || 0.7,\n                purpose: options.purpose,\n                max_tokens: options.maxTokens\n            },\n            { headers: this.getTeamHeaders() }\n        );\n        return response.data;\n    }\n\n    // Complete Job\n    async completeJob(jobId, status = 'completed', errorMessage = null) {\n        const response = await axios.post(\n            `${this.apiUrl}/jobs/${jobId}/complete`,\n            {\n                status: status,\n                error_message: errorMessage\n            },\n            { headers: this.getTeamHeaders() }\n        );\n        return response.data;\n    }\n\n    // Get Credit Balance\n    async getCreditBalance() {\n        const response = await axios.get(\n            `${this.apiUrl}/credits/teams/${this.teamId}/balance`,\n            { headers: this.getTeamHeaders() }\n        );\n        return response.data;\n    }\n}\n\n// Full workflow example\nasync function fullWorkflowExample() {\n    const client = new SaaSLiteLLMClient();\n\n    console.log('='.repeat(70));\n    console.log('SAAS LITELLM - FULL WORKFLOW EXAMPLE (JavaScript)');\n    console.log('='.repeat(70));\n\n    try {\n        // 1. Create Organization\n        console.log('\\n[1/6] Creating Organization...');\n        const org = await client.createOrganization(\n            'acme-corp-js',\n            'Acme Corporation (JS)',\n            { industry: 'technology' }\n        );\n        console.log(`\u2713 Organization: ${org.organization_id}`);\n\n        // 2. Create Model Group\n        console.log('\\n[2/6] Creating Model Group...');\n        const modelGroup = await client.createModelGroup(\n            'ChatFastJS',\n            'Fast Chat (JS)',\n            'Quick responses',\n            [\n                { model_name: 'gpt-3.5-turbo', priority: 0 },\n                { model_name: 'gpt-4o-mini', priority: 1 }\n            ]\n        );\n        console.log(`\u2713 Model Group: ${modelGroup.group_name}`);\n\n        // 3. Create Team\n        console.log('\\n[3/6] Creating Team...');\n        const team = await client.createTeam(\n            'acme-corp-js',\n            'js-team',\n            'JavaScript Team',\n            ['ChatFastJS'],\n            50\n        );\n        console.log(`\u2713 Team: ${team.team_id}`);\n        console.log(`  Credits: ${team.credits_allocated}`);\n\n        // 4. Create Job\n        console.log('\\n[4/6] Creating Job...');\n        const jobId = await client.createJob('demo', { example: 'javascript' });\n        console.log(`\u2713 Job: ${jobId}`);\n\n        // 5. Make LLM Call\n        console.log('\\n[5/6] Making LLM Call...');\n        const response = await client.makeLLMCall(\n            jobId,\n            'ChatFastJS',\n            [\n                { role: 'user', content: 'What is Node.js?' }\n            ],\n            { purpose: 'demo' }\n        );\n        console.log(`\u2713 Response: ${response.response.content.substring(0, 100)}...`);\n        console.log(`  Tokens: ${response.metadata.tokens_used}`);\n\n        // 6. Complete Job\n        console.log('\\n[6/6] Completing Job...');\n        const completion = await client.completeJob(jobId);\n        console.log(`\u2713 Job completed`);\n        console.log(`  Total calls: ${completion.costs.total_calls}`);\n        console.log(`  Credits remaining: ${completion.costs.credits_remaining}`);\n\n        console.log('\\n' + '='.repeat(70));\n        console.log('WORKFLOW COMPLETED SUCCESSFULLY!');\n        console.log('='.repeat(70));\n\n    } catch (error) {\n        console.error('Error:', error.response?.data || error.message);\n    }\n}\n\n// Run the example\nfullWorkflowExample();\n</code></pre>"},{"location":"examples/full-chain/#curl-example","title":"cURL Example","text":"<pre><code>#!/bin/bash\n\n# Configuration\nAPI_URL=\"http://localhost:8003/api\"\nADMIN_KEY=\"sk-admin-key-change-me\"\n\necho \"======================================================================\"\necho \"SAAS LITELLM - FULL WORKFLOW EXAMPLE (cURL)\"\necho \"======================================================================\"\n\n# 1. Create Organization\necho -e \"\\n[1/6] Creating Organization...\"\nORG_RESPONSE=$(curl -s -X POST \"${API_URL}/organizations/create\" \\\n  -H \"Authorization: Bearer ${ADMIN_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"acme-corp-curl\",\n    \"name\": \"Acme Corporation (cURL)\",\n    \"metadata\": {\"industry\": \"technology\"}\n  }')\necho \"\u2713 Organization created\"\n\n# 2. Create Model Group\necho -e \"\\n[2/6] Creating Model Group...\"\ncurl -s -X POST \"${API_URL}/model-groups/create\" \\\n  -H \"Authorization: Bearer ${ADMIN_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"group_name\": \"ChatFastCurl\",\n    \"display_name\": \"Fast Chat (cURL)\",\n    \"description\": \"Quick responses\",\n    \"models\": [\n      {\"model_name\": \"gpt-3.5-turbo\", \"priority\": 0}\n    ]\n  }' &gt; /dev/null\necho \"\u2713 Model Group: ChatFastCurl\"\n\n# 3. Create Team\necho -e \"\\n[3/6] Creating Team...\"\nTEAM_RESPONSE=$(curl -s -X POST \"${API_URL}/teams/create\" \\\n  -H \"Authorization: Bearer ${ADMIN_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"acme-corp-curl\",\n    \"team_id\": \"curl-team\",\n    \"team_alias\": \"cURL Team\",\n    \"model_groups\": [\"ChatFastCurl\"],\n    \"credits_allocated\": 50\n  }')\n\nVIRTUAL_KEY=$(echo $TEAM_RESPONSE | jq -r '.virtual_key')\necho \"\u2713 Team created\"\necho \"  Virtual Key: ${VIRTUAL_KEY:0:50}...\"\n\n# 4. Create Job\necho -e \"\\n[4/6] Creating Job...\"\nJOB_RESPONSE=$(curl -s -X POST \"${API_URL}/jobs/create\" \\\n  -H \"Authorization: Bearer ${VIRTUAL_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"curl-team\",\n    \"job_type\": \"demo\",\n    \"metadata\": {\"example\": \"curl\"}\n  }')\n\nJOB_ID=$(echo $JOB_RESPONSE | jq -r '.job_id')\necho \"\u2713 Job: $JOB_ID\"\n\n# 5. Make LLM Call\necho -e \"\\n[5/6] Making LLM Call...\"\nLLM_RESPONSE=$(curl -s -X POST \"${API_URL}/jobs/${JOB_ID}/llm-call\" \\\n  -H \"Authorization: Bearer ${VIRTUAL_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_group\": \"ChatFastCurl\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is cURL?\"}\n    ],\n    \"purpose\": \"demo\"\n  }')\n\nTOKENS=$(echo $LLM_RESPONSE | jq -r '.metadata.tokens_used')\necho \"\u2713 LLM Call completed\"\necho \"  Tokens: $TOKENS\"\n\n# 6. Complete Job\necho -e \"\\n[6/6] Completing Job...\"\nCOMPLETE_RESPONSE=$(curl -s -X POST \"${API_URL}/jobs/${JOB_ID}/complete\" \\\n  -H \"Authorization: Bearer ${VIRTUAL_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"status\": \"completed\"\n  }')\n\nCREDITS_REMAINING=$(echo $COMPLETE_RESPONSE | jq -r '.costs.credits_remaining')\necho \"\u2713 Job completed\"\necho \"  Credits remaining: $CREDITS_REMAINING\"\n\necho -e \"\\n======================================================================\"\necho \"WORKFLOW COMPLETED SUCCESSFULLY!\"\necho \"======================================================================\"\n</code></pre>"},{"location":"examples/full-chain/#expected-output","title":"Expected Output","text":"<pre><code>======================================================================\nSAAS LITELLM - FULL WORKFLOW EXAMPLE\n======================================================================\n\n[1/7] Creating Organization...\n\u2713 Organization created: acme-corp\n\n[2/7] Creating Model Groups...\n\u2713 Model group created: ChatFast\n\u2713 Model group created: ChatAdvanced\n\n[3/7] Creating Team with Credits...\n\u2713 Team created: engineering-team\n  Virtual Key: sk-b6f4a8c2d1e9f3a7b8c4d2e1f9a8b7c6d5e4f3a2b1c...\n  Credits: 100\n  Model Groups: ChatFast, ChatAdvanced\n\n[4/7] Checking Initial Credit Balance...\n\u2713 Credits available: 100/100\n\n[5/7] Creating Job...\n\u2713 Job created: 550e8400-e29b-41d4-a716-446655440000\n\n[6/7] Making LLM Calls...\n  \u2192 Call 1: Initial customer query...\n    Response: To reset your password, please click on the \"Forgot Password\" link on the login page...\n    Tokens: 156\n    Latency: 842ms\n\n  \u2192 Call 2: Follow-up question...\n    Response: If you didn't receive the reset email, please check your spam folder...\n    Tokens: 134\n\n  \u2192 Call 3: Complex analysis...\n    Response: Based on this interaction, I suggest the following improvements to your password reset...\n    Tokens: 287\n\n[7/7] Completing Job...\n\u2713 Job completed: 550e8400-e29b-41d4-a716-446655440000\n\n  Cost Summary:\n    Total calls: 3\n    Successful: 3\n    Failed: 0\n    Total tokens: 577\n    Total cost: $0.000867\n    Avg latency: 783ms\n    Credit applied: True\n    Credits remaining: 99\n\n[FINAL] Credit Balance After Job...\n\u2713 Credits: 99/100\n  Credits used: 1\n\n======================================================================\nWORKFLOW COMPLETED SUCCESSFULLY!\n======================================================================\n\n\u2705 All steps completed successfully!\n</code></pre>"},{"location":"examples/full-chain/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"examples/full-chain/#1-organization-hierarchy","title":"1. Organization Hierarchy","text":"<ul> <li>Organizations contain teams</li> <li>Teams have model groups and credits</li> <li>Isolation between organizations</li> </ul>"},{"location":"examples/full-chain/#2-model-groups","title":"2. Model Groups","text":"<ul> <li>Abstract model selection from clients</li> <li>Support fallback models</li> <li>Centralized model management</li> </ul>"},{"location":"examples/full-chain/#3-credit-system","title":"3. Credit System","text":"<ul> <li>Pre-allocated credits per team</li> <li>1 credit per successfully completed job</li> <li>Failed jobs don't consume credits</li> </ul>"},{"location":"examples/full-chain/#4-job-based-tracking","title":"4. Job-Based Tracking","text":"<ul> <li>Jobs group related LLM calls</li> <li>Track costs per business operation</li> <li>Associate metadata with jobs</li> </ul>"},{"location":"examples/full-chain/#5-authentication","title":"5. Authentication","text":"<ul> <li>Admin key for setup operations</li> <li>Virtual keys for team operations</li> <li>Team isolation enforced</li> </ul>"},{"location":"examples/full-chain/#next-steps","title":"Next Steps","text":"<ol> <li>Agent Integration - Use with AI agent frameworks</li> <li>Streaming Examples - Real-time streaming responses</li> <li>Structured Outputs - Type-safe structured data</li> <li>Error Handling - Production error handling</li> <li>Best Practices - Production deployment guide</li> </ol>"},{"location":"examples/streaming-examples/","title":"Streaming Examples","text":"<p>Real-world examples of using Server-Sent Events (SSE) streaming for real-time LLM responses.</p> <p>Built on LiteLLM</p> <p>Streaming works by forwarding chunks directly from LiteLLM through the SaaS API layer to your application with zero buffering for minimal latency.</p>"},{"location":"examples/streaming-examples/#prerequisites","title":"Prerequisites","text":"<pre><code># Install required packages\npip install httpx pydantic\n</code></pre>"},{"location":"examples/streaming-examples/#example-1-basic-streaming","title":"Example 1: Basic Streaming","text":"<p>The simplest streaming example - print response as it arrives:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def basic_streaming():\n    \"\"\"Stream a response and print it in real-time\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        # Create job\n        job_id = await client.create_job(\"streaming_demo\")\n\n        # Stream response\n        print(\"Assistant: \", end=\"\", flush=True)\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Write a short poem about Python\"}\n            ]\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                if content:\n                    print(content, end=\"\", flush=True)\n\n        print()  # New line after stream completes\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\nCredits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(basic_streaming())\n</code></pre> <p>Output: <pre><code>Assistant: In Python's realm where code takes flight,\nWith syntax clean and clear as light,\nWe build and dream, create with ease,\nA language that's designed to please.\n\nCredits remaining: 999\n</code></pre></p>"},{"location":"examples/streaming-examples/#example-2-accumulating-stream","title":"Example 2: Accumulating Stream","text":"<p>Collect the full response while displaying chunks:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def accumulating_stream():\n    \"\"\"Stream response while accumulating full text\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"chat\")\n\n        # Accumulate full response\n        full_response = \"\"\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences\"}\n            ],\n            temperature=0.7\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                if content:\n                    full_response += content\n                    print(content, end=\"\", flush=True)\n\n        print(f\"\\n\\nFull response length: {len(full_response)} characters\")\n\n        # You can now use full_response for further processing\n        await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(accumulating_stream())\n</code></pre>"},{"location":"examples/streaming-examples/#example-3-interactive-chat","title":"Example 3: Interactive Chat","text":"<p>Build an interactive chat interface with streaming:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def interactive_chat():\n    \"\"\"Interactive chat with streaming responses\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        # Create job for chat session\n        job_id = await client.create_job(\"chat_session\")\n        messages = []\n\n        print(\"Interactive Chat (type 'quit' to exit)\")\n        print(\"-\" * 50)\n\n        while True:\n            # Get user input\n            user_input = input(\"\\nYou: \")\n            if user_input.lower() in ['quit', 'exit', 'bye']:\n                print(\"Goodbye!\")\n                break\n\n            # Add to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            # Stream response\n            print(\"Assistant: \", end=\"\", flush=True)\n            assistant_response = \"\"\n\n            try:\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=messages,\n                    temperature=0.7\n                ):\n                    if chunk.choices:\n                        content = chunk.choices[0].delta.get(\"content\", \"\")\n                        if content:\n                            assistant_response += content\n                            print(content, end=\"\", flush=True)\n\n                # Add assistant response to conversation\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"\\nError: {e}\")\n                break\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\n\\nChat ended. Total messages: {len(messages)}\")\n        print(f\"Credits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(interactive_chat())\n</code></pre> <p>Sample Session: <pre><code>Interactive Chat (type 'quit' to exit)\n--------------------------------------------------\n\nYou: What is FastAPI?\nAssistant: FastAPI is a modern, fast web framework for building APIs with Python...\n\nYou: How does it compare to Flask?\nAssistant: FastAPI has several advantages over Flask...\n\nYou: quit\nGoodbye!\n\nChat ended. Total messages: 4\nCredits remaining: 998\n</code></pre></p>"},{"location":"examples/streaming-examples/#example-4-streaming-with-system-prompt","title":"Example 4: Streaming with System Prompt","text":"<p>Use a system prompt to control response style:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def streaming_with_system_prompt():\n    \"\"\"Stream with custom system prompt\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"tutoring\")\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a patient Python tutor. \"\n                               \"Explain concepts simply with code examples.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"How do I read a CSV file in Python?\"\n                }\n            ],\n            temperature=0.7\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                if content:\n                    print(content, end=\"\", flush=True)\n\n        print()\n        await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(streaming_with_system_prompt())\n</code></pre>"},{"location":"examples/streaming-examples/#example-5-multi-document-analysis","title":"Example 5: Multi-Document Analysis","text":"<p>Stream analysis of multiple documents:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def analyze_documents_streaming(documents: list[str]):\n    \"\"\"Analyze multiple documents with streaming\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"batch_analysis\")\n\n        for i, document in enumerate(documents, 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Document {i}/{len(documents)}\")\n            print(f\"{'='*60}\")\n            print(\"Analysis: \", end=\"\", flush=True)\n\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Provide concise analysis of documents\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this text:\\n\\n{document}\"\n                    }\n                ],\n                temperature=0.5,\n                max_tokens=200\n            ):\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    if content:\n                        print(content, end=\"\", flush=True)\n\n        print(f\"\\n\\n{'='*60}\")\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"Analysis complete! Analyzed {len(documents)} documents\")\n        print(f\"Credits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    documents = [\n        \"AI is transforming healthcare with predictive diagnostics...\",\n        \"Renewable energy sources are becoming more cost-effective...\",\n        \"Remote work has changed how companies operate globally...\"\n    ]\n\n    asyncio.run(analyze_documents_streaming(documents))\n</code></pre>"},{"location":"examples/streaming-examples/#example-6-streaming-with-progress-indicators","title":"Example 6: Streaming with Progress Indicators","text":"<p>Add progress indicators for better UX:</p> <pre><code>import asyncio\nimport time\nfrom examples.typed_client import SaaSLLMClient\n\nasync def streaming_with_progress():\n    \"\"\"Stream with visual progress indicators\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"story_generation\")\n\n        print(\"Generating story...\\n\")\n        print(\"-\" * 60)\n\n        start_time = time.time()\n        char_count = 0\n        chunk_count = 0\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Write a short sci-fi story (3 paragraphs)\"}\n            ],\n            temperature=1.0\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                if content:\n                    char_count += len(content)\n                    chunk_count += 1\n                    print(content, end=\"\", flush=True)\n\n        elapsed = time.time() - start_time\n\n        print(\"\\n\" + \"-\" * 60)\n        print(f\"\\nStatistics:\")\n        print(f\"  Chunks received: {chunk_count}\")\n        print(f\"  Characters: {char_count}\")\n        print(f\"  Time: {elapsed:.2f}s\")\n        print(f\"  Speed: {char_count/elapsed:.1f} chars/sec\")\n\n        await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(streaming_with_progress())\n</code></pre>"},{"location":"examples/streaming-examples/#example-7-error-handling-in-streams","title":"Example 7: Error Handling in Streams","text":"<p>Handle errors gracefully during streaming:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\nfrom httpx import HTTPStatusError\n\nasync def robust_streaming():\n    \"\"\"Stream with comprehensive error handling\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"resilient_chat\")\n\n        try:\n            accumulated = \"\"\n            chunk_count = 0\n\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=[\n                    {\"role\": \"user\", \"content\": \"Explain Docker containers\"}\n                ],\n                temperature=0.7\n            ):\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    if content:\n                        accumulated += content\n                        chunk_count += 1\n                        print(content, end=\"\", flush=True)\n\n            print(f\"\\n\\nStream completed successfully ({chunk_count} chunks)\")\n            await client.complete_job(job_id, \"completed\")\n\n        except HTTPStatusError as e:\n            print(f\"\\n\\nHTTP Error: {e.response.status_code}\")\n            if e.response.status_code == 403:\n                print(\"Insufficient credits or access denied\")\n            elif e.response.status_code == 429:\n                print(\"Rate limited - please wait and retry\")\n            await client.complete_job(job_id, \"failed\")\n\n        except asyncio.TimeoutError:\n            print(\"\\n\\nStream timed out\")\n            await client.complete_job(job_id, \"failed\")\n\n        except Exception as e:\n            print(f\"\\n\\nUnexpected error: {e}\")\n            await client.complete_job(job_id, \"failed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(robust_streaming())\n</code></pre>"},{"location":"examples/streaming-examples/#example-8-concurrent-streaming","title":"Example 8: Concurrent Streaming","text":"<p>Stream multiple requests concurrently:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def stream_question(client, job_id, question):\n    \"\"\"Stream answer to one question\"\"\"\n    print(f\"\\nQuestion: {question}\")\n    print(\"Answer: \", end=\"\", flush=True)\n\n    async for chunk in client.chat_stream(\n        job_id=job_id,\n        messages=[{\"role\": \"user\", \"content\": question}],\n        temperature=0.7\n    ):\n        if chunk.choices:\n            content = chunk.choices[0].delta.get(\"content\", \"\")\n            if content:\n                print(content, end=\"\", flush=True)\n\n    print()  # New line\n\nasync def concurrent_streaming():\n    \"\"\"Stream answers to multiple questions concurrently\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"faq_batch\")\n\n        questions = [\n            \"What is machine learning?\",\n            \"What is deep learning?\",\n            \"What is neural network?\"\n        ]\n\n        # Stream all questions concurrently\n        tasks = [stream_question(client, job_id, q) for q in questions]\n        await asyncio.gather(*tasks)\n\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\nAll questions answered! Credits: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(concurrent_streaming())\n</code></pre>"},{"location":"examples/streaming-examples/#example-9-streaming-to-file","title":"Example 9: Streaming to File","text":"<p>Save streamed response to a file:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def stream_to_file(output_file: str):\n    \"\"\"Stream LLM response directly to file\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"content_generation\")\n\n        with open(output_file, 'w') as f:\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"Write a technical blog post about microservices (500 words)\"\n                    }\n                ],\n                temperature=0.8\n            ):\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    if content:\n                        f.write(content)\n                        f.flush()  # Write to disk immediately\n                        print(content, end=\"\", flush=True)\n\n        print(f\"\\n\\nContent saved to: {output_file}\")\n        await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(stream_to_file(\"blog_post.txt\"))\n</code></pre>"},{"location":"examples/streaming-examples/#example-10-streaming-with-timeout","title":"Example 10: Streaming with Timeout","text":"<p>Set timeouts for long-running streams:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def streaming_with_timeout(timeout_seconds=30):\n    \"\"\"Stream with timeout protection\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"timeout_demo\")\n\n        try:\n            # Set timeout for the entire stream\n            async with asyncio.timeout(timeout_seconds):\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=[\n                        {\"role\": \"user\", \"content\": \"Explain blockchain technology\"}\n                    ]\n                ):\n                    if chunk.choices:\n                        content = chunk.choices[0].delta.get(\"content\", \"\")\n                        if content:\n                            print(content, end=\"\", flush=True)\n\n            print(f\"\\n\\nStream completed within {timeout_seconds}s\")\n            await client.complete_job(job_id, \"completed\")\n\n        except asyncio.TimeoutError:\n            print(f\"\\n\\nStream exceeded {timeout_seconds}s timeout\")\n            await client.complete_job(job_id, \"failed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(streaming_with_timeout(30))\n</code></pre>"},{"location":"examples/streaming-examples/#streaming-performance-tips","title":"Streaming Performance Tips","text":""},{"location":"examples/streaming-examples/#1-use-async-properly","title":"1. Use Async Properly","text":"<pre><code># \u2705 Good - Fully async\nasync with SaaSLLMClient(...) as client:\n    async for chunk in client.chat_stream(...):\n        # Process chunk\n        pass\n\n# \u274c Bad - Blocking sync call\nimport requests\nresponse = requests.post(..., stream=True)\nfor line in response.iter_lines():\n    # Blocks entire thread\n    pass\n</code></pre>"},{"location":"examples/streaming-examples/#2-flush-output-immediately","title":"2. Flush Output Immediately","text":"<pre><code># \u2705 Good - Real-time display\nprint(content, end=\"\", flush=True)\n\n# \u274c Bad - Buffered output\nprint(content, end=\"\")  # May not display immediately\n</code></pre>"},{"location":"examples/streaming-examples/#3-handle-empty-chunks","title":"3. Handle Empty Chunks","text":"<pre><code># \u2705 Good - Check for content\nasync for chunk in client.chat_stream(...):\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        if content:  # Only process non-empty\n            print(content, end=\"\", flush=True)\n\n# \u274c Bad - May crash on empty chunks\nasync for chunk in client.chat_stream(...):\n    content = chunk.choices[0].delta[\"content\"]  # KeyError if missing\n    print(content, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/streaming-examples/#4-set-reasonable-timeouts","title":"4. Set Reasonable Timeouts","text":"<pre><code># \u2705 Good - Timeout based on expected length\nasync with asyncio.timeout(60):  # 60s for long responses\n    async for chunk in client.chat_stream(...):\n        pass\n\n# \u274c Bad - No timeout (can hang forever)\nasync for chunk in client.chat_stream(...):\n    pass\n</code></pre>"},{"location":"examples/streaming-examples/#common-use-cases","title":"Common Use Cases","text":""},{"location":"examples/streaming-examples/#use-case-1-chat-applications","title":"Use Case 1: Chat Applications","text":"<p>Real-time responses for better UX - users see responses appear character-by-character like ChatGPT.</p>"},{"location":"examples/streaming-examples/#use-case-2-content-generation","title":"Use Case 2: Content Generation","text":"<p>Generate blog posts, articles, or documentation with live preview as it's being written.</p>"},{"location":"examples/streaming-examples/#use-case-3-code-generation","title":"Use Case 3: Code Generation","text":"<p>Stream code as it's generated so developers can start reviewing early.</p>"},{"location":"examples/streaming-examples/#use-case-4-long-form-responses","title":"Use Case 4: Long-Form Responses","text":"<p>For responses that take &gt;5 seconds, streaming provides much better perceived performance.</p>"},{"location":"examples/streaming-examples/#next-steps","title":"Next Steps","text":"<p>Now that you've seen streaming examples:</p> <ol> <li>Learn More About Streaming - Detailed streaming guide</li> <li>See Basic Examples - Non-streaming examples</li> <li>Try Structured Outputs - Type-safe responses</li> <li>Review Best Practices - Production tips</li> </ol>"},{"location":"examples/structured-outputs/","title":"Structured Output Examples","text":"<p>Working examples of extracting type-safe, validated data using Pydantic models.</p> <p>Prerequisites</p> <ul> <li>Have your virtual key from team creation</li> <li>SaaS API running on http://localhost:8003</li> <li>Typed client installed (<code>pip install httpx pydantic</code>)</li> </ul> <p> Learn about structured outputs</p>"},{"location":"examples/structured-outputs/#example-1-contact-information-extraction","title":"Example 1: Contact Information Extraction","text":"<p>Extract contact details from unstructured text:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, EmailStr, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass Contact(BaseModel):\n    \"\"\"Contact information extracted from text\"\"\"\n    name: str = Field(description=\"Full name\")\n    email: EmailStr = Field(description=\"Email address\")\n    phone: str = Field(description=\"Phone number\")\n    company: str = Field(description=\"Company name\")\n    job_title: str = Field(description=\"Job title/position\")\n\nasync def extract_contact_info():\n    \"\"\"Extract contact information from business card text\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key-here\"\n    ) as client:\n\n        job_id = await client.create_job(\"contact_extraction\")\n\n        business_card = \"\"\"\n        Sarah Johnson\n        Chief Technology Officer\n        TechVentures Inc.\n        sarah.johnson@techventures.com\n        Mobile: +1 (555) 987-6543\n        \"\"\"\n\n        contact = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract contact information from this business card:\\n\\n{business_card}\"\n            }],\n            response_model=Contact\n        )\n\n        # Now you have a fully typed, validated Contact object\n        print(f\"Name: {contact.name}\")\n        print(f\"Title: {contact.job_title}\")\n        print(f\"Company: {contact.company}\")\n        print(f\"Email: {contact.email}\")\n        print(f\"Phone: {contact.phone}\")\n\n        await client.complete_job(job_id, \"completed\")\n        return contact\n\nif __name__ == \"__main__\":\n    contact = asyncio.run(extract_contact_info())\n</code></pre> <p>Output: <pre><code>Name: Sarah Johnson\nTitle: Chief Technology Officer\nCompany: TechVentures Inc.\nEmail: sarah.johnson@techventures.com\nPhone: +1 (555) 987-6543\n</code></pre></p>"},{"location":"examples/structured-outputs/#example-2-resume-parser","title":"Example 2: Resume Parser","text":"<p>Parse resumes into structured data:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, EmailStr, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass Education(BaseModel):\n    degree: str = Field(description=\"Degree name (e.g., BS, MS, PhD)\")\n    field: str = Field(description=\"Field of study\")\n    institution: str = Field(description=\"School/university name\")\n    graduation_year: int = Field(description=\"Year graduated\")\n\nclass Experience(BaseModel):\n    title: str = Field(description=\"Job title\")\n    company: str = Field(description=\"Company name\")\n    start_date: str = Field(description=\"Start date (YYYY-MM or YYYY)\")\n    end_date: str = Field(description=\"End date or 'Present'\")\n    responsibilities: list[str] = Field(description=\"Key responsibilities\")\n\nclass Resume(BaseModel):\n    name: str\n    email: EmailStr\n    phone: str\n    summary: str = Field(description=\"Professional summary\")\n    education: list[Education]\n    experience: list[Experience]\n    skills: list[str]\n\nasync def parse_resume():\n    \"\"\"Parse a resume into structured format\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"resume_parsing\")\n\n        resume_text = \"\"\"\n        Michael Chen\n        michael.chen@email.com | (555) 123-4567\n\n        PROFESSIONAL SUMMARY\n        Senior Software Engineer with 8 years of experience in full-stack development,\n        specializing in Python, React, and cloud infrastructure.\n\n        EDUCATION\n        Master of Science in Computer Science\n        Stanford University, 2015\n\n        Bachelor of Science in Computer Engineering\n        UC Berkeley, 2013\n\n        EXPERIENCE\n        Senior Software Engineer | TechCorp Inc | 2018 - Present\n        - Led development of microservices architecture handling 10M+ requests/day\n        - Mentored team of 5 junior engineers\n        - Reduced API latency by 40% through optimization\n\n        Software Engineer | StartupXYZ | 2015 - 2018\n        - Built real-time analytics dashboard using React and WebSockets\n        - Implemented CI/CD pipeline reducing deployment time by 60%\n        - Developed RESTful APIs serving 100K+ daily users\n\n        SKILLS\n        Python, JavaScript, React, Node.js, PostgreSQL, Docker, Kubernetes, AWS\n        \"\"\"\n\n        resume = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Parse this resume into structured format:\\n\\n{resume_text}\"\n            }],\n            response_model=Resume\n        )\n\n        print(f\"\\n=== RESUME: {resume.name} ===\\n\")\n        print(f\"Email: {resume.email}\")\n        print(f\"Phone: {resume.phone}\")\n        print(f\"\\nSummary: {resume.summary}\\n\")\n\n        print(\"Education:\")\n        for edu in resume.education:\n            print(f\"  - {edu.degree} in {edu.field}, {edu.institution} ({edu.graduation_year})\")\n\n        print(\"\\nExperience:\")\n        for exp in resume.experience:\n            print(f\"  - {exp.title} at {exp.company} ({exp.start_date} - {exp.end_date})\")\n            for resp in exp.responsibilities[:2]:  # Show first 2\n                print(f\"    \u2022 {resp}\")\n\n        print(f\"\\nSkills: {', '.join(resume.skills)}\")\n\n        await client.complete_job(job_id, \"completed\")\n        return resume\n\nif __name__ == \"__main__\":\n    resume = asyncio.run(parse_resume())\n</code></pre>"},{"location":"examples/structured-outputs/#example-3-sentiment-analysis","title":"Example 3: Sentiment Analysis","text":"<p>Analyze sentiment with structured results:</p> <pre><code>import asyncio\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass Sentiment(str, Enum):\n    VERY_NEGATIVE = \"very_negative\"\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n    VERY_POSITIVE = \"very_positive\"\n\nclass SentimentAnalysis(BaseModel):\n    sentiment: Sentiment = Field(description=\"Overall sentiment\")\n    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n    key_phrases: list[str] = Field(description=\"Phrases that influenced sentiment\")\n    reasoning: str = Field(description=\"Brief explanation of sentiment\")\n\nasync def analyze_sentiment():\n    \"\"\"Analyze sentiment of product reviews\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        reviews = [\n            \"This product is absolutely amazing! Best purchase I've made all year. Highly recommend!\",\n            \"Terrible quality. Broke after one week. Total waste of money. Very disappointed.\",\n            \"It's okay. Does what it's supposed to do. Nothing special but not bad either.\"\n        ]\n\n        for i, review_text in enumerate(reviews, 1):\n            job_id = await client.create_job(f\"sentiment_analysis_{i}\")\n\n            analysis = await client.structured_output(\n                job_id=job_id,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": f\"Analyze the sentiment of this review:\\n\\n{review_text}\"\n                }],\n                response_model=SentimentAnalysis\n            )\n\n            print(f\"\\n--- Review {i} ---\")\n            print(f\"Text: {review_text}\")\n            print(f\"Sentiment: {analysis.sentiment.value.upper()}\")\n            print(f\"Confidence: {analysis.confidence:.2%}\")\n            print(f\"Key phrases: {', '.join(analysis.key_phrases)}\")\n            print(f\"Reasoning: {analysis.reasoning}\")\n\n            await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(analyze_sentiment())\n</code></pre> <p>Output: <pre><code>--- Review 1 ---\nText: This product is absolutely amazing! Best purchase I've made all year. Highly recommend!\nSentiment: VERY_POSITIVE\nConfidence: 95.00%\nKey phrases: absolutely amazing, Best purchase, Highly recommend\nReasoning: Strong positive language with superlatives and explicit recommendation\n\n--- Review 2 ---\nText: Terrible quality. Broke after one week. Total waste of money. Very disappointed.\nSentiment: VERY_NEGATIVE\nConfidence: 98.00%\nKey phrases: Terrible quality, waste of money, Very disappointed\nReasoning: Multiple negative descriptors and product failure mentioned\n\n--- Review 3 ---\nText: It's okay. Does what it's supposed to do. Nothing special but not bad either.\nSentiment: NEUTRAL\nConfidence: 90.00%\nKey phrases: It's okay, Nothing special, not bad\nReasoning: Balanced statements without strong positive or negative indicators\n</code></pre></p>"},{"location":"examples/structured-outputs/#example-4-invoice-data-extraction","title":"Example 4: Invoice Data Extraction","text":"<p>Extract structured data from invoices:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass LineItem(BaseModel):\n    description: str\n    quantity: int = Field(ge=1)\n    unit_price: float = Field(gt=0)\n    total: float = Field(gt=0)\n\nclass Invoice(BaseModel):\n    invoice_number: str\n    invoice_date: str\n    due_date: str\n    vendor_name: str\n    vendor_address: str\n    customer_name: str\n    customer_address: str\n    line_items: list[LineItem]\n    subtotal: float\n    tax: float\n    total: float\n\nasync def extract_invoice():\n    \"\"\"Extract structured data from invoice text\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"invoice_extraction\")\n\n        invoice_text = \"\"\"\n        INVOICE\n\n        Invoice #: INV-2024-00123\n        Date: October 14, 2024\n        Due Date: November 14, 2024\n\n        From:\n        TechSupplies Inc.\n        123 Supplier Street\n        San Francisco, CA 94105\n\n        To:\n        ACME Corporation\n        456 Business Ave\n        New York, NY 10001\n\n        ITEMS:\n        1. Laptop Computer - Model XPS 15\n           Qty: 5 @ $1,299.00 each = $6,495.00\n\n        2. Wireless Mouse - Model MX Master\n           Qty: 10 @ $99.00 each = $990.00\n\n        3. USB-C Hub - 7-in-1\n           Qty: 5 @ $49.99 each = $249.95\n\n        Subtotal: $7,734.95\n        Tax (8.5%): $657.47\n        TOTAL: $8,392.42\n        \"\"\"\n\n        invoice = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract invoice data:\\n\\n{invoice_text}\"\n            }],\n            response_model=Invoice\n        )\n\n        print(f\"\\n=== INVOICE {invoice.invoice_number} ===\\n\")\n        print(f\"Date: {invoice.invoice_date}\")\n        print(f\"Due: {invoice.due_date}\")\n        print(f\"\\nVendor: {invoice.vendor_name}\")\n        print(f\"Customer: {invoice.customer_name}\")\n        print(f\"\\nLine Items:\")\n\n        for item in invoice.line_items:\n            print(f\"  - {item.description}\")\n            print(f\"    {item.quantity} \u00d7 ${item.unit_price:.2f} = ${item.total:.2f}\")\n\n        print(f\"\\nSubtotal: ${invoice.subtotal:.2f}\")\n        print(f\"Tax: ${invoice.tax:.2f}\")\n        print(f\"TOTAL: ${invoice.total:.2f}\")\n\n        await client.complete_job(job_id, \"completed\")\n        return invoice\n\nif __name__ == \"__main__\":\n    invoice = asyncio.run(extract_invoice())\n</code></pre>"},{"location":"examples/structured-outputs/#example-5-email-classification","title":"Example 5: Email Classification","text":"<p>Classify and route emails automatically:</p> <pre><code>import asyncio\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass Category(str, Enum):\n    SPAM = \"spam\"\n    SUPPORT = \"support\"\n    SALES = \"sales\"\n    BILLING = \"billing\"\n    FEEDBACK = \"feedback\"\n    OTHER = \"other\"\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    URGENT = \"urgent\"\n\nclass EmailClassification(BaseModel):\n    category: Category = Field(description=\"Email category\")\n    priority: Priority = Field(description=\"Priority level\")\n    requires_response: bool = Field(description=\"Whether email needs a response\")\n    suggested_department: str = Field(description=\"Department that should handle this\")\n    key_topics: list[str] = Field(description=\"Main topics discussed\")\n    summary: str = Field(description=\"Brief summary of email content\")\n\nasync def classify_email():\n    \"\"\"Classify incoming emails for routing\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        emails = [\n            \"\"\"\n            Subject: URGENT: Production server down\n\n            Hi team,\n\n            Our production API server has been down for 15 minutes. Customers\n            are reporting errors when trying to access their accounts. This is\n            causing significant business impact. Please investigate immediately.\n\n            Thanks,\n            John from Operations\n            \"\"\",\n            \"\"\"\n            Subject: Question about pricing plans\n\n            Hello,\n\n            I'm interested in your Enterprise plan but have some questions:\n            1. What's the difference between Pro and Enterprise?\n            2. Do you offer annual billing discounts?\n            3. Can we get a demo of the admin dashboard?\n\n            Looking forward to hearing from you.\n\n            Best,\n            Sarah Mitchell\n            CTO, TechStartup Inc.\n            \"\"\",\n            \"\"\"\n            Subject: Feature request: Dark mode\n\n            Hey there!\n\n            Love your product! One suggestion - it would be great to have a\n            dark mode option. I work late at night and the bright interface\n            can be harsh on the eyes. Many of our team members would appreciate\n            this feature.\n\n            Keep up the great work!\n\n            Mike\n            \"\"\"\n        ]\n\n        for i, email_text in enumerate(emails, 1):\n            job_id = await client.create_job(f\"email_classification_{i}\")\n\n            classification = await client.structured_output(\n                job_id=job_id,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": f\"Classify this email:\\n\\n{email_text}\"\n                }],\n                response_model=EmailClassification\n            )\n\n            print(f\"\\n{'='*60}\")\n            print(f\"EMAIL {i}\")\n            print(f\"{'='*60}\")\n            print(f\"Category: {classification.category.value.upper()}\")\n            print(f\"Priority: {classification.priority.value.upper()}\")\n            print(f\"Requires Response: {'YES' if classification.requires_response else 'NO'}\")\n            print(f\"Route To: {classification.suggested_department}\")\n            print(f\"Topics: {', '.join(classification.key_topics)}\")\n            print(f\"Summary: {classification.summary}\")\n\n            await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(classify_email())\n</code></pre> <p>Output: <pre><code>============================================================\nEMAIL 1\n============================================================\nCategory: SUPPORT\nPriority: URGENT\nRequires Response: YES\nRoute To: Engineering/DevOps\nTopics: production server, downtime, API errors\nSummary: Critical production server outage affecting customers\n\n============================================================\nEMAIL 2\n============================================================\nCategory: SALES\nPriority: MEDIUM\nRequires Response: YES\nRoute To: Sales\nTopics: pricing, enterprise plan, demo request\nSummary: Prospective customer inquiring about enterprise pricing and demo\n\n============================================================\nEMAIL 3\n============================================================\nCategory: FEEDBACK\nPriority: LOW\nRequires Response: NO\nRoute To: Product Management\nTopics: feature request, dark mode, UI improvement\nSummary: User suggesting dark mode feature for better night-time usability\n</code></pre></p>"},{"location":"examples/structured-outputs/#example-6-product-data-normalization","title":"Example 6: Product Data Normalization","text":"<p>Normalize product listings from different sources:</p> <pre><code>import asyncio\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom examples.typed_client import SaaSLLMClient\n\nclass Category(str, Enum):\n    ELECTRONICS = \"electronics\"\n    CLOTHING = \"clothing\"\n    BOOKS = \"books\"\n    HOME = \"home\"\n    SPORTS = \"sports\"\n    OTHER = \"other\"\n\nclass Condition(str, Enum):\n    NEW = \"new\"\n    LIKE_NEW = \"like_new\"\n    GOOD = \"good\"\n    FAIR = \"fair\"\n    POOR = \"poor\"\n\nclass NormalizedProduct(BaseModel):\n    title: str = Field(description=\"Clean product title\")\n    category: Category = Field(description=\"Product category\")\n    brand: str = Field(description=\"Brand/manufacturer name\")\n    price: float = Field(gt=0, description=\"Price in USD\")\n    condition: Condition = Field(description=\"Product condition\")\n    description: str = Field(description=\"Clean description\")\n    features: list[str] = Field(description=\"Key product features\")\n    sku: Optional[str] = Field(default=None, description=\"SKU if available\")\n\nasync def normalize_products():\n    \"\"\"Normalize product listings from various sources\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        # Messy product listings from different sources\n        raw_listings = [\n            \"\"\"\n            Apple MacBook Pro 16\" M3 Max - BRAND NEW SEALED!!!\n            Price: $3499.99 USD\n            This is a BRAND NEW, factory sealed MacBook Pro with M3 Max chip.\n            Specs: 16GB RAM, 512GB SSD, Space Gray\n            Perfect for professionals! Fast shipping available.\n            SKU: MBP-M3-16-SG\n            \"\"\",\n            \"\"\"\n            nike air zoom pegasus 40 mens running shoes sz 10.5 like new condition\n            worn only twice | $89 | retail $140\n            Features: responsive cushioning, breathable mesh upper, rubber outsole\n            Color: Black/White\n            \"\"\"\n        ]\n\n        for i, listing in enumerate(raw_listings, 1):\n            job_id = await client.create_job(f\"product_normalization_{i}\")\n\n            product = await client.structured_output(\n                job_id=job_id,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": f\"Normalize this product listing:\\n\\n{listing}\"\n                }],\n                response_model=NormalizedProduct\n            )\n\n            print(f\"\\n{'='*60}\")\n            print(f\"PRODUCT {i}\")\n            print(f\"{'='*60}\")\n            print(f\"Title: {product.title}\")\n            print(f\"Brand: {product.brand}\")\n            print(f\"Category: {product.category.value}\")\n            print(f\"Price: ${product.price:.2f}\")\n            print(f\"Condition: {product.condition.value.replace('_', ' ').title()}\")\n            print(f\"SKU: {product.sku or 'N/A'}\")\n            print(f\"\\nDescription: {product.description}\")\n            print(f\"\\nFeatures:\")\n            for feature in product.features:\n                print(f\"  \u2022 {feature}\")\n\n            await client.complete_job(job_id, \"completed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(normalize_products())\n</code></pre>"},{"location":"examples/structured-outputs/#example-7-meeting-notes-structuring","title":"Example 7: Meeting Notes Structuring","text":"<p>Convert meeting transcripts into structured action items:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom examples.typed_client import SaaSLLMClient\n\nclass ActionItem(BaseModel):\n    task: str = Field(description=\"What needs to be done\")\n    assignee: str = Field(description=\"Who is responsible\")\n    due_date: str = Field(description=\"When it's due\")\n    priority: str = Field(description=\"Priority level\")\n\nclass Decision(BaseModel):\n    topic: str = Field(description=\"What was decided about\")\n    decision: str = Field(description=\"The decision made\")\n    rationale: str = Field(description=\"Why this decision was made\")\n\nclass MeetingNotes(BaseModel):\n    meeting_title: str\n    date: str\n    attendees: list[str]\n    summary: str = Field(description=\"Brief meeting summary\")\n    key_points: list[str] = Field(description=\"Main discussion points\")\n    decisions: list[Decision]\n    action_items: list[ActionItem]\n    next_meeting: str = Field(description=\"When to meet again\")\n\nasync def structure_meeting_notes():\n    \"\"\"Convert meeting transcript into structured notes\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"meeting_notes_structuring\")\n\n        transcript = \"\"\"\n        Product Planning Meeting - October 14, 2024\n\n        Attendees: Sarah (PM), Mike (Eng), Lisa (Design), John (Marketing)\n\n        Sarah: Let's discuss the Q4 roadmap. Our main priorities are the mobile app\n        and dark mode feature.\n\n        Mike: The mobile app will take about 8 weeks. I can start next week if we\n        finalize the requirements.\n\n        Lisa: I'll have the mobile designs ready by end of this week. Also working\n        on dark mode mockups.\n\n        Sarah: Great. Let's decide on launch date. What about December 15th for\n        the mobile app beta?\n\n        Everyone agrees.\n\n        John: I'll need at least 2 weeks before launch to prepare marketing materials.\n        So designs and copy by December 1st?\n\n        Sarah: Sounds good. Action items:\n        - Lisa: Finalize mobile designs by October 18th\n        - Mike: Start mobile development by October 21st\n        - John: Prepare marketing materials by December 1st\n        - Sarah: Set up beta testing program by November 15th\n\n        Next meeting: October 28th to review mobile app progress.\n        \"\"\"\n\n        notes = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Structure these meeting notes:\\n\\n{transcript}\"\n            }],\n            response_model=MeetingNotes\n        )\n\n        print(f\"\\n{'='*60}\")\n        print(f\"{notes.meeting_title}\")\n        print(f\"{'='*60}\")\n        print(f\"Date: {notes.date}\")\n        print(f\"Attendees: {', '.join(notes.attendees)}\\n\")\n        print(f\"Summary: {notes.summary}\\n\")\n\n        print(\"Key Points:\")\n        for point in notes.key_points:\n            print(f\"  \u2022 {point}\")\n\n        print(f\"\\nDecisions Made:\")\n        for decision in notes.decisions:\n            print(f\"  \u2022 {decision.topic}\")\n            print(f\"    Decision: {decision.decision}\")\n            print(f\"    Rationale: {decision.rationale}\\n\")\n\n        print(\"Action Items:\")\n        for item in notes.action_items:\n            print(f\"  \u2022 {item.task}\")\n            print(f\"    Assignee: {item.assignee} | Due: {item.due_date} | Priority: {item.priority}\\n\")\n\n        print(f\"Next Meeting: {notes.next_meeting}\")\n\n        await client.complete_job(job_id, \"completed\")\n        return notes\n\nif __name__ == \"__main__\":\n    notes = asyncio.run(structure_meeting_notes())\n</code></pre>"},{"location":"examples/structured-outputs/#example-8-batch-processing-with-error-handling","title":"Example 8: Batch Processing with Error Handling","text":"<p>Process multiple documents with proper error handling:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Optional\nfrom examples.typed_client import SaaSLLMClient\n\nclass Person(BaseModel):\n    name: str\n    email: str\n    phone: Optional[str] = None\n    company: Optional[str] = None\n\nasync def process_with_error_handling():\n    \"\"\"Process multiple contacts with proper error handling\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        contacts_text = [\n            \"John Doe, john@example.com, TechCorp\",\n            \"Jane Smith, jane.smith@company.com, +1-555-1234\",\n            \"Invalid data here\",  # Will fail\n            \"Bob Wilson, bob@example.com, (555) 999-8888, Wilson Industries\"\n        ]\n\n        results = []\n        errors = []\n\n        for i, text in enumerate(contacts_text, 1):\n            job_id = await client.create_job(f\"batch_contact_{i}\")\n\n            try:\n                person = await client.structured_output(\n                    job_id=job_id,\n                    messages=[{\n                        \"role\": \"user\",\n                        \"content\": f\"Extract contact info: {text}\"\n                    }],\n                    response_model=Person\n                )\n\n                results.append(person)\n                await client.complete_job(job_id, \"completed\")\n                print(f\"\u2705 Processed: {person.name}\")\n\n            except ValidationError as e:\n                errors.append({\"text\": text, \"error\": str(e)})\n                await client.complete_job(job_id, \"failed\")\n                print(f\"\u274c Validation failed for: {text}\")\n\n            except Exception as e:\n                errors.append({\"text\": text, \"error\": str(e)})\n                await client.complete_job(job_id, \"failed\")\n                print(f\"\u274c Error processing: {text}\")\n\n        print(f\"\\n{'='*60}\")\n        print(f\"Processed: {len(results)} successful, {len(errors)} failed\")\n        print(f\"{'='*60}\\n\")\n\n        print(\"Successful extractions:\")\n        for person in results:\n            print(f\"  \u2022 {person.name} - {person.email}\")\n\n        if errors:\n            print(f\"\\nFailed extractions:\")\n            for err in errors:\n                print(f\"  \u2022 {err['text'][:50]}...\")\n\n        return results, errors\n\nif __name__ == \"__main__\":\n    results, errors = asyncio.run(process_with_error_handling())\n</code></pre>"},{"location":"examples/structured-outputs/#best-practices-from-examples","title":"Best Practices from Examples","text":""},{"location":"examples/structured-outputs/#1-always-use-descriptive-models","title":"1. Always Use Descriptive Models","text":"<pre><code># \u2705 Good: Clear, descriptive model\nclass ProductReview(BaseModel):\n    product_name: str\n    rating: int = Field(ge=1, le=5)\n    review_text: str\n    would_recommend: bool\n</code></pre>"},{"location":"examples/structured-outputs/#2-add-field-descriptions","title":"2. Add Field Descriptions","text":"<pre><code># \u2705 Good: Helps LLM understand what to extract\nclass Invoice(BaseModel):\n    invoice_number: str = Field(description=\"Invoice ID, format INV-YYYY-NNNNN\")\n    total: float = Field(gt=0, description=\"Total amount in USD\")\n</code></pre>"},{"location":"examples/structured-outputs/#3-use-enums-for-fixed-options","title":"3. Use Enums for Fixed Options","text":"<pre><code># \u2705 Good: Constrains to valid values\nclass Status(str, Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n</code></pre>"},{"location":"examples/structured-outputs/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code># \u2705 Good: Proper error handling\ntry:\n    result = await client.structured_output(...)\n    await client.complete_job(job_id, \"completed\")\nexcept ValidationError as e:\n    await client.complete_job(job_id, \"failed\")\n    # Handle validation error\n</code></pre>"},{"location":"examples/structured-outputs/#5-make-optional-fields-optional","title":"5. Make Optional Fields Optional","text":"<pre><code># \u2705 Good: Not all data might be present\nclass Contact(BaseModel):\n    name: str                      # Required\n    email: str                     # Required\n    phone: Optional[str] = None    # Optional\n</code></pre>"},{"location":"examples/structured-outputs/#next-steps","title":"Next Steps","text":"<p>Now that you've seen structured output examples:</p> <ol> <li>Learn More Concepts - Deep dive into structured outputs</li> <li>See Streaming Examples - Real-time streaming</li> <li>Error Handling - Handle failures</li> <li>Best Practices - Production patterns</li> </ol>"},{"location":"getting-started/architecture/","title":"Architecture","text":"<p>Understand the layered architecture of SaaS LiteLLM and how it's built on top of LiteLLM to provide a SaaS-ready API.</p>"},{"location":"getting-started/architecture/#overview","title":"Overview","text":"<p>SaaS LiteLLM is built on top of LiteLLM and uses a layered architecture that adds a SaaS wrapper around the LiteLLM proxy, enabling:</p> <p>Built on LiteLLM</p> <p>SaaS LiteLLM leverages LiteLLM as its foundation for unified LLM API access to 100+ providers (OpenAI, Anthropic, Google, Azure, etc.). The SaaS layer adds job-based cost tracking, multi-tenancy, and business features on top of LiteLLM's core routing capabilities.</p> <ul> <li>Job-based cost tracking - Multiple LLM calls grouped into jobs</li> <li>Team isolation - Teams never directly access LiteLLM</li> <li>Hidden complexity - Model selection and pricing completely abstracted</li> <li>Per-job cost aggregation - Track costs per business operation</li> <li>Flexible pricing - Set your own markup and pricing strategy</li> </ul>"},{"location":"getting-started/architecture/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    A[Your SaaS Application] --&gt;|HTTP/JSON| B[SaaS API Layer :8003]\n    B --&gt;|Virtual Keys| C[LiteLLM Proxy :8002]\n    C --&gt;|API Calls| D[OpenAI]\n    C --&gt;|API Calls| E[Anthropic]\n    C --&gt;|API Calls| F[Google &amp; Others]\n\n    B -.Job Tracking.-&gt; G[PostgreSQL Database]\n    B -.Cost Analytics.-&gt; G\n    C -.Caching.-&gt; H[Redis Cache]\n\n    I[Admin Dashboard :3002] -.Management.-&gt; B\n\n    style A fill:#E3F2FD\n    style B fill:#4CAF50\n    style C fill:#2196F3\n    style G fill:#FF9800\n    style H fill:#F44336\n    style I fill:#9C27B0</code></pre>"},{"location":"getting-started/architecture/#component-breakdown","title":"Component Breakdown","text":""},{"location":"getting-started/architecture/#1-your-saas-application","title":"1. Your SaaS Application","text":"<p>What it is: - Your customer-facing application (web app, mobile app, API client) - The application that your teams/customers use directly</p> <p>What it does: - Makes API calls to the SaaS API layer using virtual keys - Implements your business logic and user interface - Never interacts with LiteLLM directly</p> <p>Example: <pre><code># Your application code\nresponse = requests.post(\n    \"https://your-saas-api.com/api/jobs/{job_id}/llm-call\",\n    headers={\"Authorization\": \"Bearer sk-your-virtual-key\"},\n    json={\"messages\": [{\"role\": \"user\", \"content\": \"Analyze...\"}]}\n)\n</code></pre></p>"},{"location":"getting-started/architecture/#2-saas-api-layer-port-8003","title":"2. SaaS API Layer (Port 8003)","text":"<p>What it is: - FastAPI application that wraps LiteLLM - The layer that provides job-based endpoints - This is what you expose to your teams</p> <p>What it does: - Authenticates requests using virtual keys - Creates and manages jobs for cost tracking - Proxies LLM calls to LiteLLM with job context - Aggregates costs per job in PostgreSQL - Enforces team budgets and access controls - Provides usage analytics</p> <p>Key Endpoints: - <code>POST /api/jobs/create</code> - Create a new job - <code>POST /api/jobs/{job_id}/llm-call</code> - Make an LLM call - <code>POST /api/jobs/{job_id}/llm-call-stream</code> - Streaming LLM call - <code>POST /api/jobs/{job_id}/complete</code> - Complete a job - <code>GET /api/teams/{team_id}/usage</code> - Get team usage stats</p>"},{"location":"getting-started/architecture/#3-litellm-proxy-port-8002","title":"3. LiteLLM Proxy (Port 8002)","text":"<p>What it is: - Standard LiteLLM proxy server (the foundation of SaaS LiteLLM) - Handles actual LLM routing to 100+ providers - This is internal only - never exposed to teams</p> <p>What it does: - Routes requests to appropriate LLM providers (OpenAI, Anthropic, Google, Azure, AWS Bedrock, etc.) - Provides unified OpenAI-compatible API across all providers - Manages rate limiting per team (TPM/RPM limits) - Handles caching in Redis for cost savings - Manages fallbacks and retries - Tracks usage in LiteLLM's own database tables - Load balances across multiple models/providers</p> <p>Why it's hidden: - Teams don't need to understand LiteLLM configuration - Model selection is abstracted away - Pricing is completely hidden - You maintain full control over infrastructure - Provider switching is transparent to teams</p>"},{"location":"getting-started/architecture/#4-postgresql-database","title":"4. PostgreSQL Database","text":"<p>What it stores:</p> <p>Your SaaS Tables: - <code>jobs</code> - Job metadata and status - <code>llm_calls</code> - Individual LLM calls per job - <code>job_cost_summaries</code> - Aggregated costs per job - <code>organizations</code> - Organization management - <code>teams</code> - Team management with credit allocation - <code>model_access_groups</code> - Control which teams access which models - <code>model_aliases</code> - Model configuration and pricing - <code>credit_transactions</code> - Credit history and transactions</p> <p>LiteLLM Tables (auto-created by LiteLLM): - <code>LiteLLM_VerificationToken</code> - Virtual keys - <code>LiteLLM_UserTable</code> - LiteLLM users - <code>LiteLLM_TeamTable</code> - LiteLLM teams - <code>LiteLLM_SpendLogs</code> - Usage tracking</p> <p>Benefits: - Historical tracking of all jobs and calls - Cost analytics per team, organization, or job type - Credit transaction history - Usage reporting and insights</p>"},{"location":"getting-started/architecture/#5-redis-cache","title":"5. Redis Cache","text":"<p>What it does: - Caches LLM responses for identical requests - Reduces costs by avoiding duplicate API calls - Improves latency for repeated queries</p> <p>Configuration: - Configurable TTL per model - Automatic cache key generation based on request - Transparent to your application code</p>"},{"location":"getting-started/architecture/#6-admin-dashboard-port-3002","title":"6. Admin Dashboard (Port 3002)","text":"<p>What it is: - Next.js application for platform management - Web UI for administrators</p> <p>What it does: - Create and manage organizations - Create and manage teams - Configure model access groups - Allocate credits to teams - Suspend/resume teams - Monitor usage and costs - View analytics and reports</p> <p>Who uses it: - Platform administrators - Finance/billing teams - Customer support</p>"},{"location":"getting-started/architecture/#data-flow","title":"Data Flow","text":"<p>Let's trace a typical request through the system:</p> <pre><code>sequenceDiagram\n    participant App as Your SaaS App\n    participant API as SaaS API :8003\n    participant DB as PostgreSQL\n    participant LLM as LiteLLM :8002\n    participant Provider as OpenAI/Anthropic\n    participant Cache as Redis\n\n    App-&gt;&gt;API: 1. POST /api/jobs/create\n    API-&gt;&gt;DB: Create job record\n    DB--&gt;&gt;API: job_id\n    API--&gt;&gt;App: Return job_id\n\n    App-&gt;&gt;API: 2. POST /api/jobs/{job_id}/llm-call\n    API-&gt;&gt;DB: Update job status to \"in_progress\"\n    API-&gt;&gt;LLM: Forward request with virtual key\n    LLM-&gt;&gt;Cache: Check cache\n\n    alt Cache hit\n        Cache--&gt;&gt;LLM: Return cached response\n    else Cache miss\n        LLM-&gt;&gt;Provider: Make API call\n        Provider--&gt;&gt;LLM: Return response\n        LLM-&gt;&gt;Cache: Store in cache\n    end\n\n    LLM--&gt;&gt;API: Return response + cost metadata\n    API-&gt;&gt;DB: Record llm_call with costs\n    API--&gt;&gt;App: Return response (no cost info)\n\n    App-&gt;&gt;API: 3. POST /api/jobs/{job_id}/complete\n    API-&gt;&gt;DB: Update job status to \"completed\"\n    API-&gt;&gt;DB: Aggregate costs for job\n    DB--&gt;&gt;API: Total costs\n    API--&gt;&gt;App: Return job summary</code></pre>"},{"location":"getting-started/architecture/#step-by-step-flow","title":"Step-by-Step Flow","text":"<ol> <li>Create Job</li> <li>Your app creates a job for tracking</li> <li>SaaS API stores job in PostgreSQL</li> <li> <p>Returns job_id to your app</p> </li> <li> <p>Make LLM Calls</p> </li> <li>Your app makes one or more LLM calls using the job_id</li> <li>SaaS API forwards to LiteLLM with virtual key</li> <li>LiteLLM checks Redis cache first</li> <li>If not cached, calls the provider (OpenAI, Anthropic, etc.)</li> <li>Response returned to your app (without cost info)</li> <li> <p>Cost metadata stored in PostgreSQL</p> </li> <li> <p>Complete Job</p> </li> <li>Your app marks the job as completed</li> <li>SaaS API aggregates all costs for the job</li> <li>Returns summary (for internal use only)</li> </ol>"},{"location":"getting-started/architecture/#why-this-architecture","title":"Why This Architecture?","text":""},{"location":"getting-started/architecture/#complete-abstraction","title":"\ud83c\udfaf Complete Abstraction","text":"<p>Problem: Teams shouldn't need to understand LiteLLM, models, tokens, or pricing.</p> <p>Solution: The SaaS API layer provides a simple, business-oriented interface. Teams only see jobs and calls, not models or costs.</p>"},{"location":"getting-started/architecture/#job-based-cost-tracking","title":"\ud83d\udcb0 Job-Based Cost Tracking","text":"<p>Problem: A single business operation (like \"analyze document\") often requires multiple LLM calls. How do you track total cost?</p> <p>Solution: Group related LLM calls into jobs. Track aggregate cost per job, not per API call.</p> <p>Example: <pre><code># One job = Multiple LLM calls = One aggregated cost\njob_id = create_job(\"document_analysis\")\n\nextract_text(job_id)       # Call 1: $0.005\nclassify_content(job_id)   # Call 2: $0.008\ngenerate_summary(job_id)   # Call 3: $0.010\n\ncomplete_job(job_id)\n# Total cost: $0.023\n# You charge: $0.10 (flat rate)\n# Your profit: $0.077\n</code></pre></p>"},{"location":"getting-started/architecture/#team-isolation","title":"\ud83d\udd12 Team Isolation","text":"<p>Problem: How do you manage multiple teams with different budgets and access levels?</p> <p>Solution: Organizations \u2192 Teams \u2192 Model Access Groups hierarchy with credit allocation and budget controls.</p>"},{"location":"getting-started/architecture/#flexible-pricing","title":"\ud83d\udcb5 Flexible Pricing","text":"<p>Problem: Your pricing strategy might not match actual LLM costs (e.g., you want flat-rate pricing).</p> <p>Solution: Actual costs are tracked internally but never exposed. You set your own pricing strategy.</p> <p>Pricing Options: - Flat rate per job - \"\\(0.10 per document analysis\" - **Tiered pricing** - \"\\)0.05 for first 100 jobs, $0.03 after\" - Markup pricing - \"Actual cost + 30% markup\" - Subscription - \"Unlimited jobs for $99/month\"</p>"},{"location":"getting-started/architecture/#usage-analytics","title":"\ud83d\udcca Usage Analytics","text":"<p>Problem: You need to understand which workflows are expensive, which teams use the most, and where to optimize.</p> <p>Solution: Detailed analytics per team, organization, job type, and time period.</p>"},{"location":"getting-started/architecture/#budget-protection","title":"\ud83d\udee1\ufe0f Budget Protection","text":"<p>Problem: Teams could accidentally run up huge costs.</p> <p>Solution: Credit allocation with suspend/pause capabilities. Teams can't exceed their budget.</p>"},{"location":"getting-started/architecture/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"getting-started/architecture/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>SaaS API Layer: - Stateless FastAPI application - Can run multiple instances behind a load balancer - Each instance connects to same PostgreSQL and Redis</p> <p>LiteLLM Proxy: - Also stateless - Can run multiple instances for high availability - Shares PostgreSQL and Redis for coordination</p>"},{"location":"getting-started/architecture/#database-optimization","title":"Database Optimization","text":"<p>PostgreSQL: - Index on <code>team_id</code>, <code>job_id</code>, <code>created_at</code> for fast queries - Partitioning for large <code>llm_calls</code> table - Read replicas for analytics queries</p> <p>Redis: - Separate cache instances for different regions - Eviction policy: LRU (Least Recently Used) - Monitoring for cache hit rates</p>"},{"location":"getting-started/architecture/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Redis caching reduces duplicate API calls</li> <li>Model fallbacks use cheaper models when appropriate</li> <li>Rate limiting prevents runaway costs</li> <li>Budget controls enforce hard limits</li> </ul>"},{"location":"getting-started/architecture/#security-model","title":"Security Model","text":""},{"location":"getting-started/architecture/#api-keys-and-authentication","title":"API Keys and Authentication","text":"Layer Authentication Purpose Your App \u2192 SaaS API Virtual keys (Bearer token) Team authentication SaaS API \u2192 LiteLLM Virtual keys (internal) Internal routing LiteLLM \u2192 Providers Provider API keys Provider authentication Admin \u2192 Dashboard Your auth system Admin access"},{"location":"getting-started/architecture/#data-isolation","title":"Data Isolation","text":"<ul> <li>All queries filtered by <code>team_id</code></li> <li>Job IDs are UUIDs (non-guessable)</li> <li>LiteLLM master key never exposed to teams</li> <li>Model pricing completely abstracted</li> <li>Internal tables (costs, models) never exposed via API</li> </ul>"},{"location":"getting-started/architecture/#access-controls","title":"Access Controls","text":"<ul> <li>Model Access Groups - Control which teams can use which models</li> <li>Credit Limits - Teams can't exceed allocated credits</li> <li>Rate Limits - TPM/RPM limits per team</li> <li>Suspend/Pause - Disable teams if needed</li> </ul>"},{"location":"getting-started/architecture/#deployment-modes","title":"Deployment Modes","text":""},{"location":"getting-started/architecture/#local-development","title":"Local Development","text":"<ul> <li>SaaS API on port 8003</li> <li>LiteLLM on port 8002</li> <li>PostgreSQL on port 5432</li> <li>Redis on port 6380</li> <li>Admin Dashboard on port 3002</li> </ul> <p> See Installation Guide</p>"},{"location":"getting-started/architecture/#production-railway","title":"Production (Railway)","text":"<ul> <li>SaaS API on Railway with custom domain</li> <li>LiteLLM as internal Railway service</li> <li>PostgreSQL as Railway addon</li> <li>Redis as Railway addon</li> <li>Admin Dashboard on Vercel or Railway</li> </ul> <p> See Deployment Guide</p>"},{"location":"getting-started/architecture/#docker-compose","title":"Docker Compose","text":"<ul> <li>All services in Docker containers</li> <li>Easy orchestration and scaling</li> <li>Volume mounts for persistence</li> </ul> <p> See Docker Guide</p>"},{"location":"getting-started/architecture/#key-benefits","title":"Key Benefits","text":"<p>\u2705 Teams never see LiteLLM - Complete abstraction layer</p> <p>\u2705 Job-based cost tracking - True cost per business operation</p> <p>\u2705 Model flexibility - Change models without affecting clients</p> <p>\u2705 Pricing control - Set your own markup and pricing strategy</p> <p>\u2705 Usage analytics - Detailed insights per team and job type</p> <p>\u2705 Budget protection - Prevent runaway costs with credit limits</p> <p>\u2705 Multi-call jobs - Track related LLM calls as a single unit</p> <p>\u2705 Streaming support - Server-Sent Events for real-time responses</p> <p>\u2705 Caching - Automatic response caching for cost savings</p> <p>\u2705 Rate limiting - Per-team TPM/RPM limits</p>"},{"location":"getting-started/architecture/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture:</p> <ol> <li>Complete the Installation - Get all services running</li> <li>Set Up Admin Dashboard - Create teams and organizations</li> <li>Learn Integration Patterns - Integrate into your app</li> <li>Try Examples - Run working code</li> </ol>"},{"location":"getting-started/architecture/#additional-resources","title":"Additional Resources","text":"<ul> <li>Database Schema - Detailed table schemas</li> <li>Credit System - How credits work</li> <li>Model Resolution - How models are selected</li> <li>Streaming Architecture - SSE implementation details</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Complete installation guide for SaaS LiteLLM, covering local development setup and all configuration options.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#required-software","title":"Required Software","text":"<ul> <li>Python 3.9+ - Programming language runtime</li> <li>Docker - Container platform for PostgreSQL and Redis</li> <li>Docker Compose - Multi-container orchestration</li> <li>Git - Version control (for cloning the repository)</li> <li>uv - Fast Python package installer (installed during setup)</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li>PostgreSQL client (psql) - For database inspection</li> <li>Redis client (redis-cli) - For cache inspection</li> </ul>"},{"location":"getting-started/installation/#hardware","title":"Hardware","text":"<ul> <li>RAM: 4GB minimum, 8GB recommended</li> <li>Storage: 2GB free space</li> <li>CPU: 2+ cores recommended</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":"<p>Choose the installation method that best fits your needs:</p> Local Development (Recommended)Docker OnlyRailway Deployment <p>Full development setup with all services running locally via Docker.</p> <p>Best for: - Development and testing - Learning the platform - Contributing to the project</p> <p>Run everything in Docker containers.</p> <p>Best for: - Production deployments - Isolated environments - CI/CD pipelines</p> <p>Deploy to Railway cloud platform.</p> <p>Best for: - Production hosting - Quick deployments - Managed infrastructure</p>"},{"location":"getting-started/installation/#local-development-setup","title":"Local Development Setup","text":""},{"location":"getting-started/installation/#step-1-clone-or-access-repository","title":"Step 1: Clone or Access Repository","text":"<p>If you have access to the repository:</p> <pre><code>cd /path/to/your/SaasLiteLLM\n</code></pre>"},{"location":"getting-started/installation/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>Install <code>uv</code> package manager and project dependencies:</p> <pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Reload shell to get uv in PATH\nsource ~/.bashrc  # or ~/.zshrc\n\n# Run setup script (creates venv and installs dependencies)\n./scripts/setup_local.sh\n</code></pre> <p>This script will: - Create a Python virtual environment in <code>.venv/</code> - Install all required dependencies - Set up the project for development</p> <p>Manual installation (alternative):</p> <pre><code># Create virtual environment\npython3 -m venv .venv\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Install dependencies\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#step-3-start-docker-services","title":"Step 3: Start Docker Services","text":"<p>Start PostgreSQL and Redis containers:</p> <pre><code>./scripts/docker_setup.sh\n</code></pre> <p>This will: - Pull PostgreSQL 15 and Redis 7 images - Create and start containers - Initialize PostgreSQL with the correct database</p> <p>Verify Docker services:</p> <pre><code>docker compose ps\n</code></pre> <p>Expected output: <pre><code>NAME                  STATUS\nlitellm-postgres      Up\nlitellm-redis         Up\n</code></pre></p>"},{"location":"getting-started/installation/#step-4-configure-environment-variables","title":"Step 4: Configure Environment Variables","text":"<p>Copy the local environment template:</p> <pre><code>cp .env.local .env\n</code></pre> <p>Edit <code>.env</code> and add your API keys:</p> <pre><code># Required: OpenAI API key\nOPENAI_API_KEY=sk-your-actual-openai-key-here\n\n# Optional: Other providers\nANTHROPIC_API_KEY=sk-ant-your-key-here\nGOOGLE_API_KEY=your-google-key-here\n</code></pre> <p>Environment file locations: - <code>.env</code> - Your actual environment (not in git) - <code>.env.local</code> - Template for local development - <code>.env.example</code> - Template for Railway deployment</p>"},{"location":"getting-started/installation/#step-5-run-database-migrations","title":"Step 5: Run Database Migrations","text":"<p>Create the job tracking tables:</p> <pre><code>./scripts/run_migrations.sh\n</code></pre> <p>Expected output: <pre><code>\u2705 Migration completed: 001_create_job_tracking_tables.sql\n\u2705 Migration completed: 002_add_teams_table.sql\n\u2705 Migration completed: 003_add_organizations.sql\n...\n\ud83c\udf89 All migrations completed successfully!\n</code></pre></p> <p>Manual migration (alternative):</p> <pre><code>for file in scripts/migrations/*.sql; do\n    docker exec -i litellm-postgres sh -c \\\n        'PGPASSWORD=litellm_password psql -U litellm_user -d litellm' &lt; \"$file\"\n    echo \"\u2705 Migration completed: $(basename $file)\"\ndone\n</code></pre>"},{"location":"getting-started/installation/#step-6-start-the-services","title":"Step 6: Start the Services","text":"<p>You'll need two terminal windows.</p> <p>Terminal 1: LiteLLM Backend</p> <pre><code>source .venv/bin/activate\npython scripts/start_local.py\n</code></pre> <p>Wait for: <pre><code>\ud83d\ude80 Starting LiteLLM proxy server...\n\ud83c\udf10 Server will be available at: http://0.0.0.0:8002\n</code></pre></p> <p>Terminal 2: SaaS API</p> <pre><code>source .venv/bin/activate\npython scripts/start_saas_api.py\n</code></pre> <p>Wait for: <pre><code>\ud83d\ude80 Starting SaaS API wrapper service...\n\ud83c\udf10 SaaS API will be available at: http://0.0.0.0:8003\n</code></pre></p>"},{"location":"getting-started/installation/#step-7-verify-installation","title":"Step 7: Verify Installation","text":"<p>Check that all services are running:</p> <pre><code># Check LiteLLM backend\ncurl http://localhost:8002/health\n\n# Check SaaS API\ncurl http://localhost:8003/health\n\n# Check PostgreSQL\ndocker exec litellm-postgres pg_isready -U litellm_user\n\n# Check Redis\ndocker exec litellm-redis redis-cli ping\n</code></pre> <p>All should return successful responses.</p>"},{"location":"getting-started/installation/#database-setup","title":"Database Setup","text":""},{"location":"getting-started/installation/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<p>The Docker setup creates a PostgreSQL database with these settings:</p> Setting Value Host localhost Port 5432 Database litellm User litellm_user Password litellm_password"},{"location":"getting-started/installation/#connecting-to-postgresql","title":"Connecting to PostgreSQL","text":"<pre><code># Using Docker exec\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm\n\n# Using local psql client\nPGPASSWORD=litellm_password psql -h localhost -U litellm_user -d litellm\n</code></pre> <p>Useful psql commands:</p> <pre><code>\\dt                  -- List all tables\n\\d table_name        -- Describe a table\n\\l                   -- List databases\n\\du                  -- List users\n\\q                   -- Quit\n</code></pre>"},{"location":"getting-started/installation/#database-tables","title":"Database Tables","text":"<p>After migrations, you'll have these tables:</p> <p>Your SaaS Tables: - <code>jobs</code> - Job tracking - <code>llm_calls</code> - Individual LLM call records - <code>job_cost_summaries</code> - Aggregated costs - <code>organizations</code> - Organization management - <code>teams</code> - Team management - <code>model_access_groups</code> - Model access control - <code>model_aliases</code> - Model configuration - <code>credit_transactions</code> - Credit history</p> <p>LiteLLM Tables (auto-created): - <code>LiteLLM_VerificationToken</code> - API keys - <code>LiteLLM_UserTable</code> - Users - <code>LiteLLM_TeamTable</code> - Teams - <code>LiteLLM_SpendLogs</code> - Usage tracking</p>"},{"location":"getting-started/installation/#inspecting-data","title":"Inspecting Data","text":"<pre><code># View recent jobs\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm \\\n    -c \"SELECT job_id, team_id, status, created_at FROM jobs ORDER BY created_at DESC LIMIT 5;\"\n\n# View teams\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm \\\n    -c \"SELECT team_id, organization_id, credits_allocated, credits_remaining FROM teams;\"\n</code></pre>"},{"location":"getting-started/installation/#redis-setup","title":"Redis Setup","text":""},{"location":"getting-started/installation/#redis-configuration","title":"Redis Configuration","text":"Setting Value Host localhost Port 6380 (non-standard to avoid conflicts) Password None"},{"location":"getting-started/installation/#connecting-to-redis","title":"Connecting to Redis","text":"<pre><code># Using Docker exec\ndocker exec -it litellm-redis redis-cli\n\n# Using local redis-cli\nredis-cli -p 6380\n</code></pre> <p>Useful Redis commands:</p> <pre><code>PING                 -- Test connection\nKEYS *               -- List all keys (dev only!)\nGET key_name         -- Get a value\nFLUSHALL             -- Clear all data (use with caution!)\nINFO                 -- Server information\nQUIT                 -- Exit\n</code></pre>"},{"location":"getting-started/installation/#port-configuration","title":"Port Configuration","text":"<p>Default ports for local development:</p> Service Port URL SaaS API 8003 http://localhost:8003 SaaS API Docs 8003 http://localhost:8003/docs LiteLLM Proxy 8002 http://localhost:8002 LiteLLM Admin UI 8002 http://localhost:8002/ui PostgreSQL 5432 localhost:5432 Redis 6380 localhost:6380 Admin Dashboard 3002 http://localhost:3002 <p>Why non-standard ports?</p> <p>We use ports 8002/8003 locally to avoid conflicts with other services. In production (Railway), standard port 8000 is used.</p>"},{"location":"getting-started/installation/#optional-admin-dashboard","title":"Optional: Admin Dashboard","text":"<p>If you want to use the Next.js admin dashboard:</p> <pre><code>cd admin-dashboard\n\n# Install dependencies\nnpm install\n\n# Start development server\nnpm run dev\n</code></pre> <p>Access at: http://localhost:3002</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#uv-command-not-found","title":"\"uv: command not found\"","text":"<p>Install uv manually:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nsource ~/.bashrc  # or ~/.zshrc\n</code></pre>"},{"location":"getting-started/installation/#port-already-in-use","title":"\"Port already in use\"","text":"<p>Find and kill the process:</p> <pre><code># Find what's using the port\nlsof -i :8003\n\n# Kill it\nkill -9 &lt;PID&gt;\n</code></pre>"},{"location":"getting-started/installation/#docker-daemon-not-running","title":"\"Docker daemon not running\"","text":"<p>Start Docker Desktop or the Docker daemon:</p> <pre><code># macOS\nopen -a Docker\n\n# Linux\nsudo systemctl start docker\n</code></pre>"},{"location":"getting-started/installation/#postgresql-connection-refused","title":"\"PostgreSQL connection refused\"","text":"<p>Check PostgreSQL is running:</p> <pre><code>docker compose ps postgres\ndocker compose logs postgres\n\n# Restart if needed\ndocker compose restart postgres\n</code></pre>"},{"location":"getting-started/installation/#module-not-found-errors","title":"\"Module not found\" errors","text":"<p>Reinstall dependencies:</p> <pre><code>source .venv/bin/activate\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#migration-already-exists","title":"\"Migration already exists\"","text":"<p>Migrations are idempotent. If you need to reset:</p> <pre><code># Remove all data\ndocker compose down -v\n\n# Start fresh\ndocker compose up -d postgres redis\nsleep 10\n./scripts/run_migrations.sh\n</code></pre>"},{"location":"getting-started/installation/#common-operations","title":"Common Operations","text":""},{"location":"getting-started/installation/#restart-everything","title":"Restart Everything","text":"<pre><code># Stop Docker services\ndocker compose down\n\n# Start Docker services\ndocker compose up -d\n\n# Restart Python services (Ctrl+C in each terminal, then restart)\n</code></pre>"},{"location":"getting-started/installation/#view-logs","title":"View Logs","text":"<pre><code># PostgreSQL logs\ndocker compose logs -f postgres\n\n# Redis logs\ndocker compose logs -f redis\n\n# SaaS API logs (if redirected to file)\ntail -f logs/saas_api.log\n</code></pre>"},{"location":"getting-started/installation/#reset-database","title":"Reset Database","text":"<pre><code># Complete reset\ndocker compose down -v\ndocker compose up -d\nsleep 10\n./scripts/run_migrations.sh\n</code></pre>"},{"location":"getting-started/installation/#update-dependencies","title":"Update Dependencies","text":"<pre><code>source .venv/bin/activate\nuv pip install -e . --upgrade\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":""},{"location":"getting-started/installation/#pgadmin-optional","title":"pgAdmin (Optional)","text":"<p>Web-based PostgreSQL management:</p> <pre><code>docker compose --profile pgadmin up -d\n</code></pre> <p>Access at: http://localhost:5050 - Email: admin@litellm.local - Password: admin</p>"},{"location":"getting-started/installation/#redis-commander-optional","title":"Redis Commander (Optional)","text":"<p>Add to <code>docker-compose.yml</code> if needed for Redis visualization.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that installation is complete:</p> <ol> <li>Follow the Quickstart - Test your installation</li> <li>Learn the Architecture - Understand the system</li> <li>Integration Guide - Start building</li> <li>Set up Admin Dashboard - Manage teams</li> </ol>"},{"location":"getting-started/installation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Troubleshooting Guide - Common issues</li> <li>Environment Variables - Configuration reference</li> <li>Docker Guide - Docker deployment details</li> </ul>"},{"location":"getting-started/introduction/","title":"Introduction to SaaS LiteLLM","text":""},{"location":"getting-started/introduction/#what-is-saas-litellm","title":"What is SaaS LiteLLM?","text":"<p>SaaS LiteLLM is a production-ready platform built on top of LiteLLM specifically designed for multi-tenant SaaS applications. It provides a complete abstraction layer over LiteLLM with job-based cost tracking, allowing you to build LLM-powered SaaS products without exposing infrastructure complexity to your customers.</p> <p>Built on LiteLLM</p> <p>SaaS LiteLLM is built on top of LiteLLM, which provides unified API access to 100+ LLM providers including OpenAI, Anthropic, Google, Azure, Cohere, and many more. LiteLLM handles the provider routing, while SaaS LiteLLM adds the SaaS-ready features on top.</p> <p>SaaS LiteLLM wraps the LiteLLM proxy with a SaaS-oriented API layer that:</p> <ul> <li>Groups multiple LLM calls into jobs for better cost tracking</li> <li>Completely hides models, pricing, and infrastructure from your teams</li> <li>Provides per-team isolation with independent budgets and access controls</li> <li>Enables flexible pricing strategies while tracking actual provider costs</li> <li>Includes an admin dashboard for managing teams and model access</li> </ul>"},{"location":"getting-started/introduction/#why-use-saas-litellm","title":"Why Use SaaS LiteLLM?","text":""},{"location":"getting-started/introduction/#the-problem","title":"The Problem","text":"<p>If you're building a SaaS application that uses LLMs, you face several challenges:</p> <ol> <li>Cost Attribution - How do you track costs per customer or per business operation when a single workflow makes multiple LLM calls?</li> <li>Pricing Strategy - How do you charge customers without exposing your actual LLM costs?</li> <li>Multi-Tenancy - How do you isolate teams with different budgets and access levels?</li> <li>Complexity - Teams shouldn't need to understand models, tokens, or LiteLLM configuration</li> </ol>"},{"location":"getting-started/introduction/#the-solution","title":"The Solution","text":"<p>SaaS LiteLLM solves these problems by:</p> <ul> <li>Job-Based Tracking - Group related LLM calls into jobs (e.g., \"document_analysis\", \"chat_session\") and track aggregate costs</li> <li>Cost Abstraction - Teams never see actual costs or models - you can implement any pricing strategy</li> <li>Built-in Multi-Tenancy - Organizations, teams, model access groups, and credit allocation</li> <li>Simple API - Clean, business-oriented API instead of raw LLM endpoints</li> </ul>"},{"location":"getting-started/introduction/#key-features","title":"Key Features","text":""},{"location":"getting-started/introduction/#saas-ready-architecture","title":"\ud83c\udfaf SaaS-Ready Architecture","text":"<ul> <li>Job-Based Workflow - Create a job, make multiple LLM calls, complete the job, get aggregated costs</li> <li>Hidden Complexity - Teams interact with your SaaS API, never seeing LiteLLM, models, or pricing</li> <li>Cost Aggregation - Track true costs per business operation, not per API call</li> <li>Usage Analytics - Detailed insights per team, organization, and job type</li> </ul>"},{"location":"getting-started/introduction/#business-features","title":"\ud83d\udcb0 Business Features","text":"<ul> <li>Cost Transparency - See actual LiteLLM costs vs. what you charge customers</li> <li>Flexible Pricing - Implement flat rate, tiered, markup-based, or custom pricing</li> <li>Budget Controls - Per-team credit allocation with suspend/pause capabilities</li> <li>Profit Tracking - Calculate margins per job, team, or organization</li> </ul>"},{"location":"getting-started/introduction/#technical-features","title":"\ud83d\udd27 Technical Features","text":"<ul> <li>Multi-Tenant - Organizations \u2192 Teams \u2192 Model Access Groups architecture</li> <li>Model Access Control - Control which teams can access which models via access groups</li> <li>Virtual Keys - Automatic virtual key generation (completely hidden from teams)</li> <li>Multiple Providers - Support for OpenAI, Anthropic, Google, and 100+ models via LiteLLM</li> <li>Streaming Support - Server-Sent Events (SSE) for real-time streaming responses</li> <li>Redis Caching - Automatic response caching for cost savings and performance</li> <li>Rate Limiting - Per-team TPM/RPM limits</li> <li>Admin Dashboard - Next.js dashboard for managing the platform</li> <li>Type Safety - Pydantic models throughout for request/response validation</li> <li>Production Ready - Deploy to Railway with Docker in minutes</li> </ul>"},{"location":"getting-started/introduction/#architecture-overview","title":"Architecture Overview","text":"<p>SaaS LiteLLM uses a layered architecture that abstracts LiteLLM behind your SaaS API:</p> <pre><code>graph TD\n    A[Your SaaS Application] --&gt; B[SaaS API :8003]\n    B --&gt; C[LiteLLM Proxy :8002]\n    C --&gt; D[PostgreSQL Database]\n    C --&gt; E[Redis Cache]\n    C --&gt; F[OpenAI]\n    C --&gt; G[Anthropic]\n    C --&gt; H[Other Providers]\n\n    B -.Job Tracking.-&gt; D\n\n    style B fill:#4CAF50\n    style C fill:#2196F3\n    style D fill:#FF9800\n    style E fill:#F44336</code></pre>"},{"location":"getting-started/introduction/#component-breakdown","title":"Component Breakdown","text":""},{"location":"getting-started/introduction/#1-your-saas-application","title":"1. Your SaaS Application","text":"<ul> <li>Your customer-facing application (web app, mobile app, etc.)</li> <li>Makes API calls to the SaaS API layer</li> <li>Teams never see LiteLLM or models directly</li> </ul>"},{"location":"getting-started/introduction/#2-saas-api-layer-port-8003","title":"2. SaaS API Layer (Port 8003)","text":"<ul> <li>FastAPI application that wraps LiteLLM</li> <li>Provides job-based endpoints: <code>/api/jobs/create</code>, <code>/api/jobs/{id}/llm-call</code>, etc.</li> <li>Handles authentication, team isolation, and cost tracking</li> <li>This is what you expose to your teams</li> </ul>"},{"location":"getting-started/introduction/#3-litellm-proxy-port-8002","title":"3. LiteLLM Proxy (Port 8002)","text":"<ul> <li>Standard LiteLLM proxy server</li> <li>Handles actual LLM routing to providers (OpenAI, Anthropic, etc.)</li> <li>Manages virtual keys, rate limiting, and caching</li> <li>This is internal only - never exposed to teams</li> </ul>"},{"location":"getting-started/introduction/#4-postgresql-database","title":"4. PostgreSQL Database","text":"<ul> <li>Stores jobs, LLM calls, teams, organizations, and usage data</li> <li>Provides cost aggregation and analytics</li> <li>Enables historical tracking and reporting</li> </ul>"},{"location":"getting-started/introduction/#5-redis-cache","title":"5. Redis Cache","text":"<ul> <li>Caches LLM responses for identical requests</li> <li>Reduces costs and improves latency</li> <li>Configurable TTL per model</li> </ul>"},{"location":"getting-started/introduction/#6-admin-dashboard-port-3002","title":"6. Admin Dashboard (Port 3002)","text":"<ul> <li>Next.js application for platform management</li> <li>Create organizations, teams, model access groups</li> <li>Allocate credits, suspend/resume teams</li> <li>Monitor usage and costs</li> </ul>"},{"location":"getting-started/introduction/#use-cases","title":"Use Cases","text":"<p>SaaS LiteLLM is perfect for these scenarios:</p>"},{"location":"getting-started/introduction/#document-processing-saas","title":"Document Processing SaaS","text":"<ul> <li>Job: Document analysis workflow</li> <li>LLM Calls: Extract text \u2192 Summarize \u2192 Classify \u2192 Generate insights</li> <li>Benefit: Track total cost per document, not per API call</li> </ul>"},{"location":"getting-started/introduction/#chat-application","title":"Chat Application","text":"<ul> <li>Job: Chat session (conversation with context)</li> <li>LLM Calls: Multiple messages in a conversation</li> <li>Benefit: Track cost per session, charge per conversation</li> </ul>"},{"location":"getting-started/introduction/#data-extraction-platform","title":"Data Extraction Platform","text":"<ul> <li>Job: Extract structured data from unstructured text</li> <li>LLM Calls: Parse \u2192 Validate \u2192 Transform \u2192 Enrich</li> <li>Benefit: Flat-rate pricing regardless of text length</li> </ul>"},{"location":"getting-started/introduction/#ai-writing-assistant","title":"AI Writing Assistant","text":"<ul> <li>Job: Content generation task</li> <li>LLM Calls: Research \u2192 Outline \u2192 Write \u2192 Edit \u2192 Polish</li> <li>Benefit: Predictable pricing per content piece</li> </ul>"},{"location":"getting-started/introduction/#api-translation-service","title":"API Translation Service","text":"<ul> <li>Job: Multi-language translation task</li> <li>LLM Calls: One call per language</li> <li>Benefit: Track cost per translation job</li> </ul>"},{"location":"getting-started/introduction/#how-it-works","title":"How It Works","text":"<p>Here's a simple workflow:</p> <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\n\n# 1. Create job for tracking\njob = requests.post(f\"{API}/jobs/create\", json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"document_analysis\",\n    \"metadata\": {\"document_id\": \"doc_123\"}\n}).json()\n\njob_id = job[\"job_id\"]\n\n# 2. Make LLM calls within the job\nresponse = requests.post(f\"{API}/jobs/{job_id}/llm-call\", json={\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Analyze this document...\"}\n    ]\n}).json()\n\n# 3. Complete job and get costs\nresult = requests.post(f\"{API}/jobs/{job_id}/complete\", json={\n    \"status\": \"completed\"\n}).json()\n\n# Internal tracking shows:\n# - actual_cost_usd: $0.0234\n# - You can charge: $0.10 (flat rate)\n# - Your profit: $0.0766\n</code></pre> <p>Key Points: - Teams only see your SaaS API, never LiteLLM - No model names, token counts, or costs exposed - You control pricing strategy completely - All costs tracked per job automatically</p>"},{"location":"getting-started/introduction/#what-is-litellm","title":"What is LiteLLM?","text":"<p>LiteLLM is an open-source library that provides a unified interface to 100+ LLM providers. It standardizes the API across different providers so you can easily switch between:</p> <ul> <li>OpenAI (GPT-4, GPT-3.5-turbo, etc.)</li> <li>Anthropic (Claude 3 Opus, Sonnet, Haiku, etc.)</li> <li>Google (Gemini Pro, PaLM, etc.)</li> <li>Azure OpenAI Service</li> <li>AWS Bedrock (Claude, Llama, etc.)</li> <li>Cohere, Replicate, Hugging Face, Together AI</li> <li>And 95+ more providers</li> </ul> <p>What LiteLLM Provides:</p> <ul> <li>\u2705 Unified API format (OpenAI-compatible)</li> <li>\u2705 Provider-specific authentication handling</li> <li>\u2705 Rate limiting and automatic retries</li> <li>\u2705 Fallback routing between models</li> <li>\u2705 Cost tracking per API call</li> <li>\u2705 Response caching</li> <li>\u2705 Load balancing across providers</li> </ul>"},{"location":"getting-started/introduction/#what-saas-litellm-adds-on-top","title":"What SaaS LiteLLM Adds on Top","text":"<p>SaaS LiteLLM takes LiteLLM's powerful routing capabilities and adds a complete SaaS-ready layer:</p> <p>\u2705 Job-Based Cost Tracking - Group multiple LLM calls into business operations (e.g., \"document_analysis\"), not individual API calls</p> <p>\u2705 Multi-Tenant Architecture - Full organization \u2192 teams \u2192 model access groups hierarchy with credit allocation</p> <p>\u2705 Simplified Billing - Charge 1 credit per job instead of tracking tokens per call</p> <p>\u2705 Team Isolation - Completely hide models, pricing, and infrastructure from your teams</p> <p>\u2705 Admin Dashboard - Web UI for managing teams, credits, model access, and monitoring usage</p> <p>\u2705 SaaS API Layer - Clean REST API designed for customer-facing applications</p>"},{"location":"getting-started/introduction/#comparison-with-standard-litellm","title":"Comparison with Standard LiteLLM","text":"Feature Standard LiteLLM SaaS LiteLLM (Built on LiteLLM) Foundation Core routing library LiteLLM + SaaS wrapper API Style Raw LLM endpoints Job-based workflow Cost Tracking Per API call Per business operation (job) Team Visibility See models, costs Hidden - abstracted away Pricing Model Pass-through Flexible - set your own Multi-Tenancy Virtual keys only Organizations + Teams + Access Groups Admin Interface Basic UI Full dashboard with credit management Budget Controls Rate limits Credits, suspend/pause, budget modes Billing Token-based Credit-based (per job) Use Case Admin/internal use Customer-facing SaaS"},{"location":"getting-started/introduction/#what-makes-this-saas-ready","title":"What Makes This \"SaaS-Ready\"?","text":"<ol> <li>Complete Abstraction - Teams never interact with LiteLLM directly</li> <li>Business-Oriented - API designed around jobs/tasks, not models/tokens</li> <li>Cost Management - Built-in credit system with allocation and tracking</li> <li>Multi-Tenant - Full organization/team hierarchy with isolation</li> <li>Admin Tools - Dashboard for managing teams and monitoring usage</li> <li>Flexible Pricing - Decouple what you charge from what you pay</li> <li>Production Features - Streaming, caching, rate limiting, error handling</li> </ol>"},{"location":"getting-started/introduction/#next-steps","title":"Next Steps","text":"<p>Ready to get started?</p> <ul> <li>Quickstart Guide - Get up and running in 5 minutes</li> <li>Installation Guide - Detailed setup instructions</li> <li>Architecture Deep Dive - Understand the full system design</li> <li>Integration Guide - Integrate into your app</li> </ul>"},{"location":"getting-started/introduction/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples - Working code examples</li> <li>API Reference - Complete API documentation</li> <li>Admin Dashboard Guide - Manage your platform</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get SaaS LiteLLM running on your local machine in 5 minutes. This guide will walk you through the complete setup process.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Python 3.9+ installed</li> <li>Docker and Docker Compose installed</li> <li>Git installed</li> <li>An OpenAI API key (or other LLM provider key)</li> </ul>"},{"location":"getting-started/quickstart/#step-1-access-the-repository","title":"Step 1: Access the Repository","text":"<p>Navigate to your SaasLiteLLM project directory:</p> <pre><code>cd SaasLiteLLM\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<p>We use <code>uv</code> as our package manager for faster installs:</p> <pre><code># Install uv package manager (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install dependencies\n./scripts/setup_local.sh\n</code></pre> <p>What does setup_local.sh do?</p> <ul> <li>Creates a Python virtual environment in <code>.venv/</code></li> <li>Installs all required dependencies using <code>uv</code></li> <li>Sets up the project for local development</li> </ul>"},{"location":"getting-started/quickstart/#step-3-start-docker-services","title":"Step 3: Start Docker Services","text":"<p>Start PostgreSQL and Redis using Docker Compose:</p> <pre><code>./scripts/docker_setup.sh\n</code></pre> <p>This will start: - PostgreSQL 15 on port 5432 - Redis 7 on port 6380</p> <p>Wait a few seconds for the services to initialize.</p>"},{"location":"getting-started/quickstart/#step-4-create-database-tables","title":"Step 4: Create Database Tables","text":"<p>Run the database migrations to create the job tracking tables:</p> <pre><code>./scripts/run_migrations.sh\n</code></pre> <p>You should see: <pre><code>\u2705 Migration completed: 001_create_job_tracking_tables.sql\n\u2705 Migration completed: 002_add_teams_table.sql\n...\n\ud83c\udf89 All migrations completed successfully!\n</code></pre></p> <p>What tables are created?</p> <ul> <li><code>jobs</code> - Job tracking</li> <li><code>llm_calls</code> - Individual LLM call records</li> <li><code>job_cost_summaries</code> - Aggregated costs per job</li> <li><code>team_usage_summaries</code> - Team usage analytics</li> <li><code>organizations</code> - Organization management</li> <li><code>teams</code> - Team management</li> <li><code>model_access_groups</code> - Model access control</li> <li><code>model_aliases</code> - Model configuration</li> </ul>"},{"location":"getting-started/quickstart/#step-5-configure-api-keys","title":"Step 5: Configure API Keys","text":"<p>Create a <code>.env</code> file from the template:</p> <pre><code>cp .env.local .env\n</code></pre> <p>Edit the <code>.env</code> file and add your API keys:</p> <pre><code>nano .env\n</code></pre> <p>Update these lines with your actual keys:</p> <pre><code># Required\nOPENAI_API_KEY=sk-your-actual-openai-key-here\n\n# Optional (add if you want to use these providers)\nANTHROPIC_API_KEY=sk-ant-your-actual-anthropic-key-here\nGOOGLE_API_KEY=your-google-api-key-here\n</code></pre> <p>Keep your API keys secure</p> <p>Never commit your <code>.env</code> file to git. It's already in <code>.gitignore</code>.</p>"},{"location":"getting-started/quickstart/#step-6-start-the-services","title":"Step 6: Start the Services","text":"<p>You'll need two terminal windows to run the services.</p>"},{"location":"getting-started/quickstart/#terminal-1-start-litellm-backend","title":"Terminal 1: Start LiteLLM Backend","text":"<pre><code>source .venv/bin/activate\npython scripts/start_local.py\n</code></pre> <p>You should see: <pre><code>\ud83d\ude80 Starting LiteLLM proxy server...\n\ud83c\udf10 Server will be available at: http://0.0.0.0:8002\n</code></pre></p> <p>First time only</p> <p>LiteLLM will automatically create its own tables in PostgreSQL on first run. This is normal and expected.</p>"},{"location":"getting-started/quickstart/#terminal-2-start-saas-api","title":"Terminal 2: Start SaaS API","text":"<p>Open a new terminal and run:</p> <pre><code>source .venv/bin/activate\npython scripts/start_saas_api.py\n</code></pre> <p>You should see: <pre><code>\ud83d\ude80 Starting SaaS API wrapper service...\n\ud83c\udf10 SaaS API will be available at: http://0.0.0.0:8003\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-7-verify-everything-works","title":"Step 7: Verify Everything Works","text":""},{"location":"getting-started/quickstart/#health-checks","title":"Health Checks","text":"<p>In a new terminal, check that both services are running:</p> SaaS APILiteLLM Backend <pre><code>curl http://localhost:8003/health\n</code></pre> <p>Expected response: <pre><code>{\"status\": \"healthy\"}\n</code></pre></p> <pre><code>curl http://localhost:8002/health\n</code></pre> <p>Expected response: <pre><code>{\"status\": \"healthy\"}\n</code></pre></p>"},{"location":"getting-started/quickstart/#check-database-tables","title":"Check Database Tables","text":"<p>Verify that all tables were created:</p> <pre><code>docker exec litellm-postgres sh -c 'PGPASSWORD=litellm_password psql -U litellm_user -d litellm -c \"\\dt\"'\n</code></pre> <p>You should see both your tables and LiteLLM's tables:</p> <pre><code> public | jobs                          (your tables)\n public | llm_calls\n public | organizations\n public | teams\n public | model_access_groups\n public | model_aliases\n public | LiteLLM_VerificationToken    (LiteLLM's tables)\n public | LiteLLM_TeamTable\n ...\n</code></pre>"},{"location":"getting-started/quickstart/#create-a-test-job","title":"Create a Test Job","text":"<p>Let's create a simple test job to verify the API is working:</p> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"test-team\",\n    \"user_id\": \"test-user\",\n    \"job_type\": \"test\",\n    \"metadata\": {\"test\": true}\n  }'\n</code></pre> <p>Expected response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2024-10-09T01:30:00.000Z\"\n}\n</code></pre></p> <p>Congratulations!</p> <p>If you got this response, everything is working correctly!</p>"},{"location":"getting-started/quickstart/#access-points","title":"Access Points","text":"<p>Here are all the URLs you can access:</p> Service URL Description SaaS API http://localhost:8003 Main API for your teams SaaS API Docs http://localhost:8003/docs Interactive Swagger UI SaaS API ReDoc http://localhost:8003/redoc Beautiful API documentation LiteLLM Backend http://localhost:8002 Internal only (admin) LiteLLM Admin UI http://localhost:8002/ui Admin dashboard LiteLLM API Docs http://localhost:8002/docs LiteLLM API reference PostgreSQL localhost:5432 Database (litellm_user/litellm_password) Redis localhost:6380 Cache"},{"location":"getting-started/quickstart/#daily-usage","title":"Daily Usage","text":"<p>After the initial setup, you only need to start the services:</p> <pre><code># Terminal 1: LiteLLM Backend\nsource .venv/bin/activate &amp;&amp; python scripts/start_local.py\n\n# Terminal 2: SaaS API\nsource .venv/bin/activate &amp;&amp; python scripts/start_saas_api.py\n</code></pre> <p>Faster startup</p> <p>You can create shell aliases to make this even faster: <pre><code>alias start-litellm=\"cd /path/to/SaasLiteLLM &amp;&amp; source .venv/bin/activate &amp;&amp; python scripts/start_local.py\"\nalias start-saas=\"cd /path/to/SaasLiteLLM &amp;&amp; source .venv/bin/activate &amp;&amp; python scripts/start_saas_api.py\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#port-already-in-use","title":"Port Already in Use","text":"<p>If you see \"Address already in use\" errors:</p> <pre><code># Find what's using the port\nlsof -i :8002\nlsof -i :8003\n\n# Kill the process\nkill -9 &lt;PID&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#database-connection-failed","title":"Database Connection Failed","text":"<p>If services can't connect to PostgreSQL:</p> <pre><code># Check if PostgreSQL is running\ndocker compose ps postgres\n\n# Restart PostgreSQL\ndocker compose restart postgres\n\n# View logs\ndocker compose logs postgres\n</code></pre>"},{"location":"getting-started/quickstart/#tables-not-found","title":"Tables Not Found","text":"<p>If you see \"relation does not exist\" errors:</p> <pre><code># Re-run migrations\n./scripts/run_migrations.sh\n</code></pre>"},{"location":"getting-started/quickstart/#migration-script-not-executable","title":"Migration Script Not Executable","text":"<p>If you get \"Permission denied\" errors:</p> <pre><code>chmod +x scripts/run_migrations.sh\nchmod +x scripts/docker_setup.sh\nchmod +x scripts/setup_local.sh\n</code></pre>"},{"location":"getting-started/quickstart/#python-import-errors","title":"Python Import Errors","text":"<p>If you see import errors:</p> <pre><code># Reinstall dependencies\nsource .venv/bin/activate\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/quickstart/#common-commands","title":"Common Commands","text":""},{"location":"getting-started/quickstart/#restart-database-fresh-start","title":"Restart Database (Fresh Start)","text":"<p>To completely reset the database:</p> <pre><code># Stop and remove all data\ndocker compose down -v\n\n# Start fresh\ndocker compose up -d postgres redis\n\n# Wait for PostgreSQL to initialize\nsleep 10\n\n# Re-create job tracking tables\n./scripts/run_migrations.sh\n\n# Start services (LiteLLM will recreate its tables)\nsource .venv/bin/activate\npython scripts/start_local.py\n</code></pre>"},{"location":"getting-started/quickstart/#view-database","title":"View Database","text":"<p>Connect to PostgreSQL to inspect data:</p> <pre><code># Connect to PostgreSQL\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm\n</code></pre> <p>Useful commands inside <code>psql</code>: <pre><code>\\dt                           -- List all tables\n\\d jobs                       -- Describe jobs table\nSELECT * FROM jobs LIMIT 5;   -- Query jobs\nSELECT * FROM llm_calls;      -- Query LLM calls\n\\q                            -- Quit\n</code></pre></p>"},{"location":"getting-started/quickstart/#view-logs","title":"View Logs","text":"<p>Monitor service logs:</p> <pre><code># PostgreSQL logs\ndocker compose logs -f postgres\n\n# Redis logs\ndocker compose logs -f redis\n</code></pre>"},{"location":"getting-started/quickstart/#stop-everything","title":"Stop Everything","text":"<pre><code># Stop Docker services (keeps data)\ndocker compose down\n\n# Stop and remove all data\ndocker compose down -v\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have SaaS LiteLLM running, here's what to do next:</p> <ol> <li>Set up the Admin Dashboard - Create organizations and teams</li> <li>Learn the Integration Workflow - Understand how to use the API</li> <li>Try the Examples - Run working code examples</li> <li>Explore the API - Interactive API documentation</li> </ol>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the Troubleshooting Guide</li> <li>Review Common Errors</li> </ul>"},{"location":"getting-started/quickstart/#optional-admin-dashboard","title":"Optional: Admin Dashboard","text":"<p>If you want to use the Next.js admin dashboard:</p> <pre><code>cd admin-dashboard\nnpm install\nnpm run dev\n</code></pre> <p>The dashboard will be available at http://localhost:3002</p> <p>See the Admin Dashboard Guide for more details.</p>"},{"location":"integration/authentication/","title":"Team Authentication","text":"<p>Learn how to authenticate API requests using virtual keys, best practices for key management, and how to handle authentication errors.</p> <p>Admin vs Team Authentication</p> <p>This guide covers team authentication using virtual keys for making LLM requests.</p> <p>For admin authentication (managing organizations, teams, models), see Admin Authentication.</p>"},{"location":"integration/authentication/#overview","title":"Overview","text":"<p>All team API endpoints in SaaS LiteLLM require authentication using virtual keys. Virtual keys are team-specific API keys that:</p> <ul> <li>Authenticate your team with the SaaS API</li> <li>Track usage and costs per team</li> <li>Enforce credit limits and access controls</li> <li>Never expose the underlying LiteLLM infrastructure</li> </ul> <p>Server-Side Clients Only</p> <p>Team API clients should be server-side (Python, Node.js, Go, curl, etc.) for security and to avoid CORS restrictions.</p> <p>CORS is a browser-only security feature - server-side HTTP clients completely ignore CORS.  Learn more about CORS vs Authentication</p>"},{"location":"integration/authentication/#authentication-types","title":"Authentication Types","text":"<p>SaaS LiteLLM uses two separate authentication systems:</p> Type Key Format Header Used For Documentation Admin <code>MASTER_KEY</code> <code>X-Admin-Key</code> Managing organizations, teams, models Admin Auth Team Virtual Key (per-team) <code>Authorization: Bearer</code> Making LLM requests This guide <p>Important: These are completely separate systems with different keys and purposes!</p>"},{"location":"integration/authentication/#virtual-keys","title":"Virtual Keys","text":""},{"location":"integration/authentication/#what-is-a-virtual-key","title":"What is a Virtual Key?","text":"<p>A virtual key is a Bearer token in the format:</p> <pre><code>sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Each team has a unique virtual key that:</p> <ul> <li>Identifies the team - Associates requests with a specific team</li> <li>Enforces budgets - Ensures teams don't exceed credit limits</li> <li>Controls access - Limits which models the team can use via model access groups</li> <li>Tracks usage - Records all LLM calls and costs per team</li> </ul> <p>Virtual Keys are Team-Specific</p> <p>Each virtual key is tied to exactly one team. One team cannot use another team's virtual key.</p>"},{"location":"integration/authentication/#getting-your-virtual-key","title":"Getting Your Virtual Key","text":"<p>When you create a team (via the admin dashboard or API), you receive a virtual key in the response.</p>"},{"location":"integration/authentication/#via-api","title":"Via API","text":"<pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-corp\",\n    \"team_alias\": \"ACME Corp Team\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"organization_id\": \"org_acme\",\n  \"virtual_key\": \"sk-1234567890abcdef1234567890abcdef\",\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 1000,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"integration/authentication/#via-admin-dashboard","title":"Via Admin Dashboard","text":"<ol> <li>Navigate to http://localhost:3002 (or your production URL)</li> <li>Go to Teams section</li> <li>Click Create Team</li> <li>Fill in team details and click Create</li> <li>Copy the virtual key from the response</li> </ol> <p>Save Your Virtual Key Immediately</p> <p>The virtual key is only shown once during team creation. Store it securely immediately.</p>"},{"location":"integration/authentication/#viewing-existing-keys","title":"Viewing Existing Keys","text":"<p>If you need to retrieve a team's virtual key:</p> <pre><code>curl http://localhost:8003/api/teams/acme-corp \\\n  -H \"Content-Type: application/json\"\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"organization_id\": \"org_acme\",\n  \"virtual_key\": \"sk-1234567890abcdef1234567890abcdef\",\n  \"credits_remaining\": 850,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"integration/authentication/#using-virtual-keys","title":"Using Virtual Keys","text":""},{"location":"integration/authentication/#http-header-authentication","title":"HTTP Header Authentication","text":"<p>Include the virtual key in the <code>Authorization</code> header with the <code>Bearer</code> scheme:</p> PythonJavaScriptcURLGo <pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-1234567890abcdef1234567890abcdef\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"document_analysis\"\n    }\n)\n\nprint(response.json())\n</code></pre> <pre><code>const API_URL = \"http://localhost:8003/api\";\nconst VIRTUAL_KEY = \"sk-1234567890abcdef1234567890abcdef\";\n\nconst headers = {\n  \"Authorization\": `Bearer ${VIRTUAL_KEY}`,\n  \"Content-Type\": \"application/json\"\n};\n\nconst response = await fetch(`${API_URL}/jobs/create`, {\n  method: \"POST\",\n  headers: headers,\n  body: JSON.stringify({\n    team_id: \"acme-corp\",\n    job_type: \"document_analysis\"\n  })\n});\n\nconst data = await response.json();\nconsole.log(data);\n</code></pre> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-1234567890abcdef1234567890abcdef\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"document_analysis\"\n  }'\n</code></pre> <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"net/http\"\n)\n\nfunc main() {\n    apiURL := \"http://localhost:8003/api\"\n    virtualKey := \"sk-1234567890abcdef1234567890abcdef\"\n\n    data := map[string]string{\n        \"team_id\":  \"acme-corp\",\n        \"job_type\": \"document_analysis\",\n    }\n\n    jsonData, _ := json.Marshal(data)\n    req, _ := http.NewRequest(\"POST\", apiURL+\"/jobs/create\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Authorization\", \"Bearer \"+virtualKey)\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &amp;http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n}\n</code></pre>"},{"location":"integration/authentication/#type-safe-python-client","title":"Type-Safe Python Client","text":"<p>If you're using Python, we provide a type-safe client that handles authentication automatically:</p> <pre><code>from saas_litellm_client import SaasLiteLLMClient\n\nasync with SaasLiteLLMClient(\n    base_url=\"http://localhost:8003\",\n    team_id=\"acme-corp\",\n    virtual_key=\"sk-1234567890abcdef1234567890abcdef\"\n) as client:\n    # Authentication is handled automatically\n    job = await client.create_job(\"document_analysis\")\n    print(f\"Created job: {job.job_id}\")\n</code></pre> <p> Learn more about the typed client</p>"},{"location":"integration/authentication/#authentication-errors","title":"Authentication Errors","text":""},{"location":"integration/authentication/#401-unauthorized","title":"401 Unauthorized","text":"<p>Error Response: <pre><code>{\n  \"detail\": \"Invalid or missing API key\"\n}\n</code></pre></p> <p>Causes: - Missing <code>Authorization</code> header - Invalid virtual key format - Virtual key doesn't exist - Virtual key has been revoked</p> <p>Solution: <pre><code># \u274c Wrong - Missing Authorization header\nresponse = requests.post(\n    \"http://localhost:8003/api/jobs/create\",\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n)\n\n# \u2705 Correct - Include Authorization header\nheaders = {\"Authorization\": \"Bearer sk-your-virtual-key\"}\nresponse = requests.post(\n    \"http://localhost:8003/api/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n)\n</code></pre></p>"},{"location":"integration/authentication/#403-forbidden","title":"403 Forbidden","text":"<p>Error Response: <pre><code>{\n  \"detail\": \"Team suspended or insufficient credits\"\n}\n</code></pre></p> <p>Causes: - Team has been suspended by an administrator - Team has run out of credits - Team is in \"pause\" mode</p> <p>Solution: 1. Check team status: <code>GET /api/teams/{team_id}</code> 2. Contact administrator to add credits or reactivate team 3. Check credit balance: <code>GET /api/credits/balance?team_id={team_id}</code></p>"},{"location":"integration/authentication/#403-model-access-denied","title":"403 Model Access Denied","text":"<p>Error Response: <pre><code>{\n  \"detail\": \"Team does not have access to the requested model\"\n}\n</code></pre></p> <p>Causes: - Team's model access group doesn't include the requested model - Model alias not configured for the team's access group</p> <p>Solution: 1. Check team's access groups: <code>GET /api/teams/{team_id}</code> 2. Contact administrator to update access groups 3. Use a model alias the team has access to</p>"},{"location":"integration/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"integration/authentication/#1-environment-variables","title":"1. Environment Variables","text":"<p>Never hardcode virtual keys in your source code. Use environment variables:</p> PythonJavaScript.env File <pre><code>import os\n\nVIRTUAL_KEY = os.environ.get(\"SAAS_LITELLM_VIRTUAL_KEY\")\n\nif not VIRTUAL_KEY:\n    raise ValueError(\"SAAS_LITELLM_VIRTUAL_KEY environment variable not set\")\n\nheaders = {\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"}\n</code></pre> <pre><code>const VIRTUAL_KEY = process.env.SAAS_LITELLM_VIRTUAL_KEY;\n\nif (!VIRTUAL_KEY) {\n  throw new Error(\"SAAS_LITELLM_VIRTUAL_KEY environment variable not set\");\n}\n\nconst headers = {\n  \"Authorization\": `Bearer ${VIRTUAL_KEY}`\n};\n</code></pre> <pre><code># .env\nSAAS_LITELLM_VIRTUAL_KEY=sk-1234567890abcdef1234567890abcdef\n</code></pre>"},{"location":"integration/authentication/#2-secrets-management","title":"2. Secrets Management","text":"<p>Use a secrets management service for production:</p> <ul> <li>AWS Secrets Manager - For AWS deployments</li> <li>Google Secret Manager - For Google Cloud</li> <li>HashiCorp Vault - For on-premise or multi-cloud</li> <li>Railway Variables - For Railway deployments</li> <li>Vercel Environment Variables - For Vercel deployments</li> </ul> <p>Example with AWS Secrets Manager: <pre><code>import boto3\nimport json\n\ndef get_virtual_key():\n    client = boto3.client('secretsmanager', region_name='us-west-2')\n    response = client.get_secret_value(SecretId='saas-litellm-virtual-key')\n    secret = json.loads(response['SecretString'])\n    return secret['virtual_key']\n\nVIRTUAL_KEY = get_virtual_key()\n</code></pre></p>"},{"location":"integration/authentication/#3-key-rotation","title":"3. Key Rotation","text":"<p>Rotate virtual keys periodically:</p> <ol> <li>Create a new team (or update the existing team to generate a new key)</li> <li>Update your application to use the new key</li> <li>Verify the new key works</li> <li>Deactivate the old team (optional)</li> </ol> <p>Recommended rotation schedule: - Development: Every 90 days - Production: Every 30-60 days - After any security incident: Immediately</p>"},{"location":"integration/authentication/#4-separate-keys-per-environment","title":"4. Separate Keys Per Environment","text":"<p>Use different teams (and thus different virtual keys) for each environment:</p> <pre><code># Development environment\nSAAS_LITELLM_VIRTUAL_KEY_DEV=sk-dev-key-here\n\n# Staging environment\nSAAS_LITELLM_VIRTUAL_KEY_STAGING=sk-staging-key-here\n\n# Production environment\nSAAS_LITELLM_VIRTUAL_KEY_PROD=sk-prod-key-here\n</code></pre>"},{"location":"integration/authentication/#5-least-privilege-access","title":"5. Least Privilege Access","text":"<ul> <li>Separate teams for different applications - Don't share keys across apps</li> <li>Limit model access - Only grant access to models the team needs</li> <li>Set appropriate credit limits - Prevent runaway costs</li> <li>Monitor usage - Track which teams are using the most resources</li> </ul>"},{"location":"integration/authentication/#6-never-expose-keys","title":"6. Never Expose Keys","text":"<p>\u274c DON'T: - Commit keys to version control - Include keys in client-side code (JavaScript in browsers) - Log keys in application logs - Share keys via email or chat - Store keys in plaintext files</p> <p>\u2705 DO: - Use environment variables or secrets management - Keep keys on the server-side only - Use <code>.gitignore</code> for <code>.env</code> files - Rotate keys regularly - Audit key usage</p>"},{"location":"integration/authentication/#7-https-only","title":"7. HTTPS Only","text":"<p>Always use HTTPS in production:</p> <pre><code># \u274c Development only\nAPI_URL = \"http://localhost:8003/api\"\n\n# \u2705 Production\nAPI_URL = \"https://api.your-saas.com/api\"\n</code></pre> <p>HTTP transmits the virtual key in plaintext, which is vulnerable to interception.</p>"},{"location":"integration/authentication/#testing-authentication","title":"Testing Authentication","text":""},{"location":"integration/authentication/#health-check-no-auth-required","title":"Health Check (No Auth Required)","text":"<p>Test that the API is reachable:</p> <pre><code>curl http://localhost:8003/health\n</code></pre> <p>Expected response: <pre><code>{\"status\": \"healthy\"}\n</code></pre></p>"},{"location":"integration/authentication/#authenticated-request","title":"Authenticated Request","text":"<p>Test that your virtual key works:</p> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"test\"\n  }'\n</code></pre> <p>Expected response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2024-10-14T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"integration/authentication/#check-team-info","title":"Check Team Info","text":"<p>Verify your team exists and is active:</p> <pre><code>curl http://localhost:8003/api/teams/acme-corp\n</code></pre> <p>Expected response: <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"organization_id\": \"org_acme\",\n  \"status\": \"active\",\n  \"credits_remaining\": 850,\n  \"credits_allocated\": 1000\n}\n</code></pre></p>"},{"location":"integration/authentication/#error-handling","title":"Error Handling","text":""},{"location":"integration/authentication/#retry-logic","title":"Retry Logic","text":"<p>Implement retry logic for transient authentication errors:</p> <pre><code>import requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\ndef create_session_with_retries():\n    session = requests.Session()\n\n    retry = Retry(\n        total=3,\n        backoff_factor=1,\n        status_forcelist=[500, 502, 503, 504],\n        allowed_methods=[\"POST\", \"GET\"]\n    )\n\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    return session\n\n# Use the session\nsession = create_session_with_retries()\nheaders = {\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"}\n\nresponse = session.post(\n    \"http://localhost:8003/api/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n)\n</code></pre>"},{"location":"integration/authentication/#handling-401403-errors","title":"Handling 401/403 Errors","text":"<pre><code>import requests\n\ndef make_authenticated_request(endpoint, data):\n    headers = {\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"}\n\n    try:\n        response = requests.post(\n            f\"{API_URL}/{endpoint}\",\n            headers=headers,\n            json=data\n        )\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 401:\n            print(\"Authentication failed. Check your virtual key.\")\n            # Maybe try to refresh the key or notify admin\n        elif e.response.status_code == 403:\n            print(\"Access denied. Check team status and credits.\")\n            # Maybe check credit balance or team status\n        else:\n            print(f\"HTTP error: {e}\")\n        raise\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        raise\n\n# Usage\nresult = make_authenticated_request(\"jobs/create\", {\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"test\"\n})\n</code></pre>"},{"location":"integration/authentication/#advanced-topics","title":"Advanced Topics","text":""},{"location":"integration/authentication/#custom-authentication-middleware","title":"Custom Authentication Middleware","text":"<p>If you're building a wrapper service, you might want custom authentication:</p> <pre><code>from fastapi import FastAPI, HTTPException, Security\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\napp = FastAPI()\nsecurity = HTTPBearer()\n\nasync def verify_virtual_key(\n    credentials: HTTPAuthorizationCredentials = Security(security)\n) -&gt; str:\n    \"\"\"Verify the virtual key and return team_id\"\"\"\n    virtual_key = credentials.credentials\n\n    # Verify with SaaS API\n    response = requests.get(\n        f\"http://localhost:8003/api/teams/verify\",\n        headers={\"Authorization\": f\"Bearer {virtual_key}\"}\n    )\n\n    if response.status_code != 200:\n        raise HTTPException(status_code=401, detail=\"Invalid virtual key\")\n\n    return response.json()[\"team_id\"]\n\n@app.post(\"/my-endpoint\")\nasync def my_endpoint(team_id: str = Security(verify_virtual_key)):\n    return {\"message\": f\"Authenticated as team: {team_id}\"}\n</code></pre>"},{"location":"integration/authentication/#caching-team-info","title":"Caching Team Info","text":"<p>Cache team information to reduce authentication overhead:</p> <pre><code>from functools import lru_cache\nimport time\n\n@lru_cache(maxsize=100)\ndef get_team_info(virtual_key: str, cache_time: int):\n    \"\"\"\n    Cache team info for 5 minutes.\n    cache_time is passed to invalidate cache every 5 minutes.\n    \"\"\"\n    response = requests.get(\n        f\"http://localhost:8003/api/teams/verify\",\n        headers={\"Authorization\": f\"Bearer {virtual_key}\"}\n    )\n    return response.json()\n\n# Usage - cache is invalidated every 5 minutes\ncache_key = int(time.time() / 300)  # 300 seconds = 5 minutes\nteam_info = get_team_info(VIRTUAL_KEY, cache_key)\n</code></pre>"},{"location":"integration/authentication/#next-steps","title":"Next Steps","text":"<p>Now that you understand authentication:</p> <ol> <li>Learn the Job Workflow - Create jobs and make LLM calls</li> <li>Try Non-Streaming Calls - Standard LLM requests</li> <li>Try Streaming Calls - Real-time responses</li> <li>See Examples - Working code examples</li> </ol>"},{"location":"integration/authentication/#additional-resources","title":"Additional Resources","text":"<ul> <li>Error Handling Guide - Comprehensive error handling</li> <li>Best Practices - Security and performance tips</li> <li>Admin Dashboard Guide - Create and manage teams</li> </ul>"},{"location":"integration/best-practices/","title":"Best Practices","text":"<p>Learn best practices for building robust, secure, and cost-effective applications with SaaS LiteLLM.</p>"},{"location":"integration/best-practices/#overview","title":"Overview","text":"<p>This guide covers: - Workflow Selection - Choose the right endpoint for your use case - Performance Optimization - Reduce latency and improve throughput - Security Best Practices - Protect your application and data - Cost Optimization - Minimize LLM costs - Development Practices - Write maintainable code - Production Readiness - Deploy with confidence</p>"},{"location":"integration/best-practices/#workflow-selection","title":"Workflow Selection","text":""},{"location":"integration/best-practices/#choose-the-right-endpoint","title":"Choose the Right Endpoint","text":"<p>SaaS LiteLLM offers two workflow patterns optimized for different use cases:</p>"},{"location":"integration/best-practices/#1-single-call-workflow-apijobscreate-and-call","title":"1. Single-Call Workflow (<code>/api/jobs/create-and-call</code>)","text":"<p>\u2705 Use When: - Your workflow requires only ONE LLM call - You need minimal latency (chat apps, real-time responses) - Simplicity is important - You want automatic error handling</p> <p>Example Use Cases: <pre><code># Chat applications\nresponse = requests.post(f\"{API}/jobs/create-and-call\", json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"chat\",\n    \"model\": \"gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": user_message}]\n})\n\n# Simple classification\nresponse = requests.post(f\"{API}/jobs/create-and-call\", json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"sentiment_analysis\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": f\"Classify sentiment: {text}\"}]\n})\n\n# Single-turn text generation\nresponse = requests.post(f\"{API}/jobs/create-and-call\", json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"summarization\",\n    \"model\": \"gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": f\"Summarize: {document}\"}]\n})\n</code></pre></p> <p>Performance: 1 API call ~1.5s</p>"},{"location":"integration/best-practices/#2-multi-step-workflow-create-call-complete","title":"2. Multi-Step Workflow (Create \u2192 Call \u2192 Complete)","text":"<p>\u2705 Use When: - Your workflow requires MULTIPLE LLM calls - You need granular control over each step - You want to track intermediate results - Building complex agentic workflows</p> <p>Example Use Cases: <pre><code># Multi-step document analysis\njob_id = create_job(\"document_analysis\")\nextract_text(job_id)      # Call 1\nclassify_content(job_id)  # Call 2\ngenerate_summary(job_id)  # Call 3\ncomplete_job(job_id)\n\n# Agentic workflow with decisions\njob_id = create_job(\"research_agent\")\ninitial_response = llm_call(job_id, \"Research topic X\")\nif needs_more_info(initial_response):\n    deep_dive = llm_call(job_id, \"Deep dive into...\")\nfinal_report = llm_call(job_id, \"Compile report from...\")\ncomplete_job(job_id)\n\n# Batch processing with retry logic\njob_id = create_job(\"batch_processing\")\nfor item in items:\n    try:\n        llm_call(job_id, process_prompt(item))\n    except:\n        retry_with_fallback(job_id, item)\ncomplete_job(job_id)\n</code></pre></p> <p>Performance: 3+ API calls ~4.5s+</p>"},{"location":"integration/best-practices/#decision-tree","title":"Decision Tree","text":"<pre><code>Does your workflow require multiple LLM calls?\n\u251c\u2500 NO  \u2192 Use /api/jobs/create-and-call (faster, simpler)\n\u2514\u2500 YES \u2192 Use Create \u2192 Call \u2192 Complete (more control)\n   \u251c\u2500 Sequential processing needed? \u2192 Multi-step\n   \u251c\u2500 Need to track intermediate results? \u2192 Multi-step\n   \u2514\u2500 Complex agent logic? \u2192 Multi-step\n</code></pre>"},{"location":"integration/best-practices/#performance-comparison","title":"Performance Comparison","text":"Metric Single-Call Multi-Step API Calls 1 3+ Latency ~1.5s ~4.5s+ Code Complexity Low Medium Error Handling Automatic Manual Best For Chat, simple tasks Agents, complex workflows"},{"location":"integration/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"integration/best-practices/#1-use-streaming-for-interactive-applications","title":"1. Use Streaming for Interactive Applications","text":"<p>\u274c Non-Streaming (Perceived Latency: ~2000ms) <pre><code>response = await client.chat(job_id, messages)\nprint(response.choices[0].message[\"content\"])\n</code></pre></p> <p>\u2705 Streaming (Perceived Latency: ~300-500ms) <pre><code>async for chunk in client.chat_stream(job_id, messages):\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n</code></pre></p> <p>When to use each: - Streaming: Chat apps, real-time generation, long responses - Non-streaming: Batch processing, structured outputs, simple tasks</p>"},{"location":"integration/best-practices/#2-reuse-jobs-for-related-calls","title":"2. Reuse Jobs for Related Calls","text":"<p>Group related LLM calls into a single job:</p> <p>\u2705 Good - One job, multiple calls <pre><code>job_id = await client.create_job(\"document_analysis\")\n\n# All related calls in one job\nextract_text(job_id)\nclassify_content(job_id)\ngenerate_summary(job_id)\n\nawait client.complete_job(job_id, \"completed\")\n# Cost: 1 credit\n</code></pre></p> <p>\u274c Bad - Separate jobs <pre><code># Creates unnecessary overhead\njob1 = await client.create_job(\"extract\")\nextract_text(job1)\nawait client.complete_job(job1, \"completed\")\n\njob2 = await client.create_job(\"classify\")\nclassify_content(job2)\nawait client.complete_job(job2, \"completed\")\n# Cost: 2 credits\n</code></pre></p>"},{"location":"integration/best-practices/#3-set-reasonable-timeouts","title":"3. Set Reasonable Timeouts","text":"<pre><code># \u2705 Good - Set appropriate timeout\nresponse = requests.post(\n    url,\n    json=data,\n    timeout=30  # 30 seconds\n)\n\n# \u274c Bad - No timeout (can hang forever)\nresponse = requests.post(url, json=data)\n</code></pre> <p>Recommended Timeouts: - Non-streaming calls: 30-60 seconds - Streaming calls: 60-120 seconds - Simple requests: 10-30 seconds</p>"},{"location":"integration/best-practices/#4-use-async-for-concurrency","title":"4. Use Async for Concurrency","text":"<p>\u2705 Good - Async allows concurrent operations <pre><code>import asyncio\n\nasync def process_batch(documents):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"batch_processing\")\n\n        # Process documents concurrently\n        tasks = [\n            analyze_document(client, job_id, doc)\n            for doc in documents\n        ]\n        results = await asyncio.gather(*tasks)\n\n        await client.complete_job(job_id, \"completed\")\n        return results\n</code></pre></p> <p>\u274c Bad - Sequential processing <pre><code>def process_batch_sync(documents):\n    results = []\n    for doc in documents:\n        result = analyze_document_sync(doc)\n        results.append(result)\n    return results\n</code></pre></p>"},{"location":"integration/best-practices/#5-cache-common-requests","title":"5. Cache Common Requests","text":"<p>Implement application-level caching for frequently repeated requests:</p> <pre><code>import hashlib\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef get_cached_response(prompt: str):\n    \"\"\"Cache responses for identical prompts\"\"\"\n    # Make LLM call\n    response = make_llm_call(prompt)\n    return response\n\n# Usage\nresponse = get_cached_response(\"What is Python?\")  # First call\nresponse = get_cached_response(\"What is Python?\")  # Cached!\n</code></pre> <p>Redis Caching</p> <p>SaaS LiteLLM automatically caches responses in Redis. Application-level caching is an additional optimization.</p>"},{"location":"integration/best-practices/#6-batch-similar-requests","title":"6. Batch Similar Requests","text":"<p>When possible, batch similar requests together:</p> <pre><code>async def batch_classify(texts):\n    \"\"\"Classify multiple texts in one call\"\"\"\n    job_id = await client.create_job(\"batch_classification\")\n\n    # Combine into single prompt\n    prompt = \"Classify each of these texts as positive/negative/neutral:\\n\\n\"\n    for i, text in enumerate(texts, 1):\n        prompt += f\"{i}. {text}\\n\"\n\n    response = await client.chat(job_id, [\n        {\"role\": \"user\", \"content\": prompt}\n    ])\n\n    await client.complete_job(job_id, \"completed\")\n    # Parse response for individual classifications\n    return parse_batch_response(response)\n</code></pre>"},{"location":"integration/best-practices/#7-use-connection-pooling","title":"7. Use Connection Pooling","text":"<pre><code>import requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\ndef create_session():\n    \"\"\"Create session with connection pooling and retries\"\"\"\n    session = requests.Session()\n\n    # Connection pooling\n    adapter = HTTPAdapter(\n        pool_connections=10,\n        pool_maxsize=20\n    )\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    return session\n\n# Reuse session across requests\nsession = create_session()\nresponse = session.post(url, json=data, headers=headers)\n</code></pre>"},{"location":"integration/best-practices/#security-best-practices","title":"Security Best Practices","text":""},{"location":"integration/best-practices/#1-never-hardcode-api-keys","title":"1. Never Hardcode API Keys","text":"<p>\u274c Bad - Hardcoded keys <pre><code>VIRTUAL_KEY = \"sk-1234567890abcdef\"  # DON'T DO THIS!\n</code></pre></p> <p>\u2705 Good - Environment variables <pre><code>import os\n\nVIRTUAL_KEY = os.environ.get(\"SAAS_LITELLM_VIRTUAL_KEY\")\nif not VIRTUAL_KEY:\n    raise ValueError(\"SAAS_LITELLM_VIRTUAL_KEY not set\")\n</code></pre></p>"},{"location":"integration/best-practices/#2-use-https-in-production","title":"2. Use HTTPS in Production","text":"<p>\u274c Development only <pre><code>API_URL = \"http://localhost:8003/api\"\n</code></pre></p> <p>\u2705 Production <pre><code>API_URL = os.environ.get(\n    \"SAAS_API_URL\",\n    \"https://api.your-saas.com/api\"  # Always HTTPS\n)\n</code></pre></p>"},{"location":"integration/best-practices/#3-rotate-keys-regularly","title":"3. Rotate Keys Regularly","text":"<pre><code># Implement key rotation\ndef rotate_virtual_key():\n    \"\"\"Rotate virtual key every 30 days\"\"\"\n    # 1. Create new team or regenerate key\n    # 2. Update environment variables\n    # 3. Test new key\n    # 4. Deactivate old key\n    pass\n</code></pre> <p>Rotation Schedule: - Development: Every 90 days - Production: Every 30-60 days - After security incidents: Immediately</p>"},{"location":"integration/best-practices/#4-separate-keys-per-environment","title":"4. Separate Keys Per Environment","text":"<pre><code># .env.development\nSAAS_LITELLM_VIRTUAL_KEY=sk-dev-key-here\nSAAS_API_URL=http://localhost:8003/api\n\n# .env.production\nSAAS_LITELLM_VIRTUAL_KEY=sk-prod-key-here\nSAAS_API_URL=https://api.your-saas.com/api\n</code></pre>"},{"location":"integration/best-practices/#5-validate-and-sanitize-user-input","title":"5. Validate and Sanitize User Input","text":"<pre><code>def sanitize_user_input(text: str) -&gt; str:\n    \"\"\"Sanitize user input before sending to LLM\"\"\"\n    # Remove excessive whitespace\n    text = \" \".join(text.split())\n\n    # Limit length\n    MAX_LENGTH = 10000\n    if len(text) &gt; MAX_LENGTH:\n        text = text[:MAX_LENGTH]\n\n    # Remove potentially harmful content\n    # (implement based on your use case)\n\n    return text\n\n# Usage\nuser_message = sanitize_user_input(request.data[\"message\"])\n</code></pre>"},{"location":"integration/best-practices/#6-implement-rate-limiting","title":"6. Implement Rate Limiting","text":"<p>Protect your application from abuse:</p> <pre><code>from datetime import datetime, timedelta\nfrom collections import defaultdict\n\nclass RateLimiter:\n    \"\"\"Simple rate limiter\"\"\"\n\n    def __init__(self, max_requests=10, window_seconds=60):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = defaultdict(list)\n\n    def allow_request(self, user_id: str) -&gt; bool:\n        \"\"\"Check if request is allowed\"\"\"\n        now = datetime.now()\n        cutoff = now - timedelta(seconds=self.window_seconds)\n\n        # Remove old requests\n        self.requests[user_id] = [\n            ts for ts in self.requests[user_id]\n            if ts &gt; cutoff\n        ]\n\n        # Check limit\n        if len(self.requests[user_id]) &gt;= self.max_requests:\n            return False\n\n        # Allow request\n        self.requests[user_id].append(now)\n        return True\n\n# Usage\nlimiter = RateLimiter(max_requests=10, window_seconds=60)\n\nif not limiter.allow_request(user_id):\n    raise Exception(\"Rate limit exceeded\")\n</code></pre>"},{"location":"integration/best-practices/#7-log-security-events","title":"7. Log Security Events","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef log_security_event(event_type: str, details: dict):\n    \"\"\"Log security-relevant events\"\"\"\n    logger.warning(\n        f\"Security event: {event_type}\",\n        extra={\n            \"event_type\": event_type,\n            \"timestamp\": datetime.now().isoformat(),\n            **details\n        }\n    )\n\n# Usage\nif response.status_code == 401:\n    log_security_event(\"authentication_failed\", {\n        \"team_id\": team_id,\n        \"ip_address\": request.client.host\n    })\n</code></pre>"},{"location":"integration/best-practices/#cost-optimization","title":"Cost Optimization","text":""},{"location":"integration/best-practices/#1-use-lower-cost-models-when-possible","title":"1. Use Lower-Cost Models When Possible","text":"<pre><code># Use cheaper models for simple tasks\nTASK_MODELS = {\n    \"simple_classification\": \"gpt-3.5-turbo\",  # Cheaper\n    \"complex_analysis\": \"gpt-4\",               # More expensive\n    \"code_generation\": \"gpt-4\",                # More expensive\n}\n\nmodel = TASK_MODELS.get(task_type, \"gpt-3.5-turbo\")\n</code></pre>"},{"location":"integration/best-practices/#2-set-max-tokens-to-avoid-runaway-costs","title":"2. Set Max Tokens to Avoid Runaway Costs","text":"<pre><code># \u2705 Good - Limit response length\nresponse = await client.chat(\n    job_id,\n    messages,\n    max_tokens=500  # Limit response\n)\n\n# \u274c Bad - Unlimited response\nresponse = await client.chat(job_id, messages)\n</code></pre>"},{"location":"integration/best-practices/#3-monitor-and-alert-on-high-usage","title":"3. Monitor and Alert on High Usage","text":"<pre><code>def check_credit_balance(team_id: str):\n    \"\"\"Alert when credits are low\"\"\"\n    response = requests.get(\n        f\"{API_URL}/teams/{team_id}\",\n        headers=headers\n    )\n    team = response.json()\n\n    credits_remaining = team[\"credits_remaining\"]\n    credits_allocated = team[\"credits_allocated\"]\n\n    # Alert at 20% remaining\n    if credits_remaining &lt; credits_allocated * 0.2:\n        send_low_credit_alert(team_id, credits_remaining)\n\n    # Alert at 10% remaining\n    if credits_remaining &lt; credits_allocated * 0.1:\n        send_critical_credit_alert(team_id, credits_remaining)\n</code></pre>"},{"location":"integration/best-practices/#4-use-caching-strategically","title":"4. Use Caching Strategically","text":"<p>Leverage Redis caching for repeated queries:</p> <pre><code># Identical requests are automatically cached\nresponse1 = await client.chat(job_id, messages)  # Cache miss\nresponse2 = await client.chat(job_id, messages)  # Cache hit (no cost!)\n</code></pre>"},{"location":"integration/best-practices/#5-optimize-prompts-for-efficiency","title":"5. Optimize Prompts for Efficiency","text":"<p>\u274c Inefficient - Verbose prompt <pre><code>prompt = \"\"\"\nPlease analyze the following text and provide a comprehensive summary\nincluding all the key points, important details, and main conclusions.\nMake sure to cover every aspect thoroughly and provide deep insights\ninto the content...\n\n[long text]\n\"\"\"\n</code></pre></p> <p>\u2705 Efficient - Concise prompt <pre><code>prompt = \"\"\"\nSummarize the key points:\n\n[long text]\n\"\"\"\n</code></pre></p>"},{"location":"integration/best-practices/#6-track-costs-per-feature","title":"6. Track Costs Per Feature","text":"<pre><code>def track_feature_cost(feature: str, actual_cost: float):\n    \"\"\"Track costs per feature for optimization\"\"\"\n    # Log to analytics/metrics system\n    metrics.record(\"feature_cost\", actual_cost, tags={\"feature\": feature})\n\n# Usage\nresult = await client.complete_job(job_id, \"completed\")\nactual_cost = result.costs.get(\"total_cost_usd\", 0)\ntrack_feature_cost(\"document_analysis\", actual_cost)\n</code></pre>"},{"location":"integration/best-practices/#7-implement-job-timeouts","title":"7. Implement Job Timeouts","text":"<p>Prevent jobs from running indefinitely:</p> <pre><code>async def process_with_timeout(job_id, max_duration_seconds=300):\n    \"\"\"Process job with timeout\"\"\"\n    try:\n        async with asyncio.timeout(max_duration_seconds):\n            # Process job\n            result = await process_job(job_id)\n            await client.complete_job(job_id, \"completed\")\n            return result\n    except asyncio.TimeoutError:\n        # Mark as failed to avoid credit charge\n        await client.complete_job(job_id, \"failed\")\n        raise\n</code></pre>"},{"location":"integration/best-practices/#development-practices","title":"Development Practices","text":""},{"location":"integration/best-practices/#1-use-type-hints","title":"1. Use Type Hints","text":"<pre><code>from typing import List, Dict, Optional\n\nasync def analyze_documents(\n    documents: List[str],\n    team_id: str,\n    options: Optional[Dict[str, Any]] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Analyze multiple documents\"\"\"\n    # Implementation with clear types\n    pass\n</code></pre>"},{"location":"integration/best-practices/#2-write-comprehensive-tests","title":"2. Write Comprehensive Tests","text":"<pre><code>import pytest\n\n@pytest.mark.asyncio\nasync def test_document_analysis():\n    \"\"\"Test document analysis workflow\"\"\"\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"test_analysis\")\n\n        response = await client.chat(\n            job_id,\n            [{\"role\": \"user\", \"content\": \"Test input\"}]\n        )\n\n        assert response.choices[0].message[\"content\"]\n\n        result = await client.complete_job(job_id, \"completed\")\n        assert result.costs.credits_remaining &gt;= 0\n</code></pre>"},{"location":"integration/best-practices/#3-use-context-managers","title":"3. Use Context Managers","text":"<pre><code># \u2705 Good - Automatic cleanup\nasync with SaaSLLMClient(...) as client:\n    # Client is automatically closed\n\n# \u274c Bad - Manual cleanup\nclient = SaaSLLMClient(...)\ntry:\n    # Use client\n    pass\nfinally:\n    await client.close()\n</code></pre>"},{"location":"integration/best-practices/#4-handle-partial-failures","title":"4. Handle Partial Failures","text":"<pre><code>async def process_batch_with_partial_failure(documents):\n    \"\"\"Process batch even if some fail\"\"\"\n    results = []\n    failures = []\n\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"batch_processing\")\n\n        for doc in documents:\n            try:\n                result = await process_document(client, job_id, doc)\n                results.append(result)\n            except Exception as e:\n                failures.append({\"document\": doc, \"error\": str(e)})\n                logger.error(f\"Failed to process document: {e}\")\n\n        # Complete job even with partial failures\n        status = \"completed\" if len(results) &gt; 0 else \"failed\"\n        await client.complete_job(job_id, status)\n\n    return {\n        \"results\": results,\n        \"failures\": failures,\n        \"success_rate\": len(results) / len(documents)\n    }\n</code></pre>"},{"location":"integration/best-practices/#5-use-structured-logging","title":"5. Use Structured Logging","text":"<pre><code>import logging\nimport json\n\nlogger = logging.getLogger(__name__)\n\ndef log_api_call(job_id: str, endpoint: str, latency_ms: float, success: bool):\n    \"\"\"Log API calls with structured data\"\"\"\n    logger.info(\n        \"API call completed\",\n        extra={\n            \"job_id\": job_id,\n            \"endpoint\": endpoint,\n            \"latency_ms\": latency_ms,\n            \"success\": success,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    )\n</code></pre>"},{"location":"integration/best-practices/#6-implement-health-checks","title":"6. Implement Health Checks","text":"<pre><code>async def check_api_health() -&gt; bool:\n    \"\"\"Check if SaaS API is healthy\"\"\"\n    try:\n        response = requests.get(\n            f\"{API_URL.replace('/api', '')}/health\",\n            timeout=5\n        )\n        return response.status_code == 200\n    except Exception as e:\n        logger.error(f\"Health check failed: {e}\")\n        return False\n\n# Run health checks periodically\nif not await check_api_health():\n    alert_ops_team(\"SaaS API is unhealthy\")\n</code></pre>"},{"location":"integration/best-practices/#production-readiness","title":"Production Readiness","text":""},{"location":"integration/best-practices/#1-use-environment-specific-configuration","title":"1. Use Environment-Specific Configuration","text":"<pre><code>import os\n\nclass Config:\n    \"\"\"Environment-specific configuration\"\"\"\n\n    def __init__(self):\n        self.env = os.environ.get(\"ENVIRONMENT\", \"development\")\n\n        if self.env == \"production\":\n            self.api_url = os.environ[\"SAAS_API_URL\"]\n            self.virtual_key = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\n            self.timeout = 60\n            self.max_retries = 3\n        else:\n            self.api_url = \"http://localhost:8003/api\"\n            self.virtual_key = os.environ.get(\"SAAS_LITELLM_VIRTUAL_KEY\", \"dev-key\")\n            self.timeout = 30\n            self.max_retries = 1\n\nconfig = Config()\n</code></pre>"},{"location":"integration/best-practices/#2-implement-circuit-breakers","title":"2. Implement Circuit Breakers","text":"<pre><code>from datetime import datetime, timedelta\n\nclass CircuitBreaker:\n    \"\"\"Prevent cascading failures\"\"\"\n\n    def __init__(self, failure_threshold=5, timeout_seconds=60):\n        self.failure_threshold = failure_threshold\n        self.timeout_seconds = timeout_seconds\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = \"closed\"  # closed, open, half-open\n\n    def call(self, func, *args, **kwargs):\n        \"\"\"Execute function with circuit breaker\"\"\"\n        if self.state == \"open\":\n            # Check if timeout has passed\n            if datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.timeout_seconds):\n                self.state = \"half-open\"\n            else:\n                raise Exception(\"Circuit breaker is open\")\n\n        try:\n            result = func(*args, **kwargs)\n            # Success - reset\n            self.failure_count = 0\n            self.state = \"closed\"\n            return result\n\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = datetime.now()\n\n            if self.failure_count &gt;= self.failure_threshold:\n                self.state = \"open\"\n\n            raise\n</code></pre>"},{"location":"integration/best-practices/#3-monitor-key-metrics","title":"3. Monitor Key Metrics","text":"<pre><code>import prometheus_client as prom\n\n# Define metrics\nrequest_count = prom.Counter(\n    'saas_llm_requests_total',\n    'Total number of requests',\n    ['endpoint', 'status']\n)\n\nrequest_latency = prom.Histogram(\n    'saas_llm_request_duration_seconds',\n    'Request latency',\n    ['endpoint']\n)\n\ncredits_remaining = prom.Gauge(\n    'saas_llm_credits_remaining',\n    'Credits remaining',\n    ['team_id']\n)\n\n# Use metrics\nrequest_count.labels(endpoint='/llm-call', status='success').inc()\n</code></pre>"},{"location":"integration/best-practices/#4-implement-graceful-degradation","title":"4. Implement Graceful Degradation","text":"<pre><code>async def get_response_with_fallback(prompt: str) -&gt; str:\n    \"\"\"Get LLM response with fallback\"\"\"\n    try:\n        # Try primary model\n        response = await client.chat(job_id, messages)\n        return response.choices[0].message[\"content\"]\n\n    except Exception as e:\n        logger.warning(f\"Primary model failed: {e}\")\n\n        try:\n            # Fallback to cached response\n            cached = get_cached_response(prompt)\n            if cached:\n                return cached\n        except:\n            pass\n\n        # Final fallback to default response\n        return \"I'm sorry, I'm having trouble processing your request right now. Please try again later.\"\n</code></pre>"},{"location":"integration/best-practices/#summary-checklist","title":"Summary Checklist","text":""},{"location":"integration/best-practices/#performance","title":"Performance","text":"<ul> <li> Use streaming for interactive applications</li> <li> Reuse jobs for related calls</li> <li> Set reasonable timeouts</li> <li> Use async for concurrency</li> <li> Implement caching</li> <li> Batch similar requests</li> <li> Use connection pooling</li> </ul>"},{"location":"integration/best-practices/#security","title":"Security","text":"<ul> <li> Never hardcode API keys</li> <li> Use HTTPS in production</li> <li> Rotate keys regularly</li> <li> Separate keys per environment</li> <li> Validate user input</li> <li> Implement rate limiting</li> <li> Log security events</li> </ul>"},{"location":"integration/best-practices/#cost","title":"Cost","text":"<ul> <li> Use appropriate models</li> <li> Set max_tokens limits</li> <li> Monitor credit usage</li> <li> Leverage caching</li> <li> Optimize prompts</li> <li> Track costs per feature</li> <li> Implement job timeouts</li> </ul>"},{"location":"integration/best-practices/#development","title":"Development","text":"<ul> <li> Use type hints</li> <li> Write comprehensive tests</li> <li> Use context managers</li> <li> Handle partial failures</li> <li> Use structured logging</li> <li> Implement health checks</li> </ul>"},{"location":"integration/best-practices/#production","title":"Production","text":"<ul> <li> Environment-specific config</li> <li> Circuit breakers</li> <li> Monitor metrics</li> <li> Graceful degradation</li> <li> Error tracking</li> <li> Alerting system</li> </ul>"},{"location":"integration/best-practices/#next-steps","title":"Next Steps","text":"<p>Now that you understand best practices:</p> <ol> <li>Try the Examples - See best practices in action</li> <li>Review Error Handling - Comprehensive error handling</li> <li>Deploy to Production - Production deployment guide</li> <li>Monitor Your Application - Testing and monitoring</li> </ol>"},{"location":"integration/error-handling/","title":"Error Handling","text":"<p>Learn how to handle errors gracefully when integrating with the SaaS LiteLLM API.</p>"},{"location":"integration/error-handling/#overview","title":"Overview","text":"<p>Proper error handling is critical for building robust applications. This guide covers all possible errors, their causes, and how to handle them.</p> <p>Error Categories: - Authentication errors (401, 403) - Client errors (400, 404, 422) - Rate limiting (429) - Server errors (500, 503) - Network errors (timeouts, connection failures) - Streaming errors (interrupted streams)</p>"},{"location":"integration/error-handling/#http-status-codes","title":"HTTP Status Codes","text":""},{"location":"integration/error-handling/#400-bad-request","title":"400 Bad Request","text":"<p>Cause: Invalid request format or parameters</p> <p>Example: <pre><code>{\n  \"detail\": \"Invalid message format\"\n}\n</code></pre></p> <p>Common Causes: - Missing required fields - Invalid message structure - Unsupported parameters - Invalid JSON format</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 400:\n        error_detail = e.response.json()\n        print(f\"Bad request: {error_detail['detail']}\")\n        # Fix the request format\n        # Log the issue for debugging\n</code></pre></p>"},{"location":"integration/error-handling/#401-unauthorized","title":"401 Unauthorized","text":"<p>Cause: Invalid or missing virtual key</p> <p>Example: <pre><code>{\n  \"detail\": \"Invalid or missing API key\"\n}\n</code></pre></p> <p>Common Causes: - Missing <code>Authorization</code> header - Invalid virtual key format - Virtual key doesn't exist - Expired or revoked key</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n        json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 401:\n        print(\"Authentication failed. Check your virtual key.\")\n        # Attempt to refresh the key\n        # Notify admin\n        # Fall back to cached data if possible\n</code></pre></p> <p> Learn more about authentication</p>"},{"location":"integration/error-handling/#403-forbidden","title":"403 Forbidden","text":"<p>Cause: Team suspended, insufficient credits, or access denied</p> <p>Example: <pre><code>{\n  \"detail\": \"Team suspended or insufficient credits\"\n}\n</code></pre></p> <p>Common Causes: - Team has been suspended by admin - Team has run out of credits - Team doesn't have access to the requested model - Team is in \"pause\" mode</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 403:\n        error_detail = e.response.json()\n        print(f\"Access denied: {error_detail['detail']}\")\n\n        # Check team status\n        team_response = requests.get(\n            f\"{API_URL}/teams/{team_id}\",\n            headers=headers\n        )\n        team_info = team_response.json()\n\n        if team_info['status'] == 'suspended':\n            print(\"Team is suspended. Contact administrator.\")\n        elif team_info['credits_remaining'] &lt;= 0:\n            print(\"Out of credits. Please add credits to continue.\")\n        else:\n            print(\"Model access denied. Check access groups.\")\n</code></pre></p>"},{"location":"integration/error-handling/#404-not-found","title":"404 Not Found","text":"<p>Cause: Job, team, or resource doesn't exist</p> <p>Example: <pre><code>{\n  \"detail\": \"Job not found\"\n}\n</code></pre></p> <p>Common Causes: - Invalid job_id - Job was deleted - Wrong team_id - Typo in endpoint URL</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 404:\n        print(f\"Job {job_id} not found\")\n        # Create a new job\n        job = create_new_job()\n        # Retry with the new job_id\n</code></pre></p>"},{"location":"integration/error-handling/#422-unprocessable-entity","title":"422 Unprocessable Entity","text":"<p>Cause: Request validation failed</p> <p>Example: <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"messages\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n</code></pre></p> <p>Common Causes: - Missing required fields - Invalid field types - Validation errors on request data</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\"team_id\": \"acme-corp\"}  # Missing job_type\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 422:\n        errors = e.response.json()['detail']\n        for error in errors:\n            field = '.'.join(str(x) for x in error['loc'])\n            print(f\"Validation error on {field}: {error['msg']}\")\n</code></pre></p>"},{"location":"integration/error-handling/#429-too-many-requests","title":"429 Too Many Requests","text":"<p>Cause: Rate limit exceeded</p> <p>Example: <pre><code>{\n  \"detail\": \"Rate limit exceeded\"\n}\n</code></pre></p> <p>Common Causes: - Too many requests per minute (RPM limit) - Too many tokens per minute (TPM limit) - Burst limit exceeded</p> <p>Solution: <pre><code>import time\n\ndef make_request_with_retry(url, headers, data, max_retries=3):\n    \"\"\"Make request with exponential backoff for rate limits\"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(url, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                if attempt &lt; max_retries - 1:\n                    # Exponential backoff: 1s, 2s, 4s\n                    wait_time = 2 ** attempt\n                    print(f\"Rate limited. Waiting {wait_time}s...\")\n                    time.sleep(wait_time)\n                else:\n                    print(\"Rate limit exceeded after all retries\")\n                    raise\n            else:\n                raise\n</code></pre></p>"},{"location":"integration/error-handling/#500-internal-server-error","title":"500 Internal Server Error","text":"<p>Cause: Server-side error</p> <p>Example: <pre><code>{\n  \"detail\": \"Internal server error\"\n}\n</code></pre></p> <p>Common Causes: - Unexpected server-side error - Database connection issue - LiteLLM proxy unavailable</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 500:\n        print(\"Server error. Retrying...\")\n        # Retry with exponential backoff\n        time.sleep(5)\n        # Retry the request\n</code></pre></p>"},{"location":"integration/error-handling/#503-service-unavailable","title":"503 Service Unavailable","text":"<p>Cause: Service temporarily unavailable</p> <p>Example: <pre><code>{\n  \"detail\": \"Service temporarily unavailable\"\n}\n</code></pre></p> <p>Common Causes: - Service maintenance - Server overload - Dependency unavailable</p> <p>Solution: <pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 503:\n        print(\"Service unavailable. Please try again later.\")\n        # Implement retry with longer delays\n</code></pre></p>"},{"location":"integration/error-handling/#network-errors","title":"Network Errors","text":""},{"location":"integration/error-handling/#connection-errors","title":"Connection Errors","text":"<pre><code>import requests\nfrom requests.exceptions import ConnectionError, Timeout\n\ntry:\n    response = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\"team_id\": \"acme-corp\", \"job_type\": \"test\"},\n        timeout=30  # 30 second timeout\n    )\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\n    # Check network connectivity\n    # Retry with different endpoint if available\nexcept Timeout as e:\n    print(f\"Request timed out: {e}\")\n    # Retry with longer timeout\n</code></pre>"},{"location":"integration/error-handling/#timeout-errors","title":"Timeout Errors","text":"<pre><code>try:\n    response = requests.post(\n        f\"{API_URL}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\"messages\": messages},\n        timeout=30\n    )\nexcept Timeout:\n    print(\"Request timed out after 30 seconds\")\n    # Increase timeout\n    # Or switch to streaming for long responses\n</code></pre>"},{"location":"integration/error-handling/#streaming-errors","title":"Streaming Errors","text":""},{"location":"integration/error-handling/#interrupted-streams","title":"Interrupted Streams","text":"<pre><code>import asyncio\n\nasync def robust_streaming():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"chat\")\n\n        try:\n            accumulated = \"\"\n            chunk_count = 0\n\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n            ):\n                chunk_count += 1\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    accumulated += content\n                    print(content, end=\"\", flush=True)\n\n            print(f\"\\n\\nStream completed successfully ({chunk_count} chunks)\")\n            await client.complete_job(job_id, \"completed\")\n\n        except asyncio.TimeoutError:\n            print(\"\\n\\nStream timed out\")\n            await client.complete_job(job_id, \"failed\")\n\n        except Exception as e:\n            print(f\"\\n\\nStream error: {e}\")\n            await client.complete_job(job_id, \"failed\")\n\n            # Log the partial response\n            if accumulated:\n                print(f\"Partial response received: {accumulated[:100]}...\")\n</code></pre>"},{"location":"integration/error-handling/#stream-timeout","title":"Stream Timeout","text":"<pre><code>import asyncio\n\nasync def streaming_with_timeout():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"chat\")\n\n        try:\n            # Set overall timeout for the stream\n            async with asyncio.timeout(60):  # 60 seconds\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}]\n                ):\n                    # Process chunks\n                    pass\n\n        except asyncio.TimeoutError:\n            print(\"Stream exceeded 60 second timeout\")\n            await client.complete_job(job_id, \"failed\")\n</code></pre>"},{"location":"integration/error-handling/#comprehensive-error-handler","title":"Comprehensive Error Handler","text":"<p>Here's a complete error handling utility:</p> <pre><code>import requests\nimport time\nfrom typing import Optional, Dict, Any\n\nclass APIError(Exception):\n    \"\"\"Base exception for API errors\"\"\"\n    pass\n\nclass AuthenticationError(APIError):\n    \"\"\"Authentication failed\"\"\"\n    pass\n\nclass InsufficientCreditsError(APIError):\n    \"\"\"Team out of credits\"\"\"\n    pass\n\nclass RateLimitError(APIError):\n    \"\"\"Rate limit exceeded\"\"\"\n    pass\n\nclass ServerError(APIError):\n    \"\"\"Server-side error\"\"\"\n    pass\n\ndef make_api_request(\n    url: str,\n    headers: Dict[str, str],\n    data: Dict[str, Any],\n    max_retries: int = 3,\n    timeout: int = 30\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Make API request with comprehensive error handling and retries.\n\n    Args:\n        url: API endpoint URL\n        headers: Request headers\n        data: Request body\n        max_retries: Maximum number of retries\n        timeout: Request timeout in seconds\n\n    Returns:\n        API response data\n\n    Raises:\n        AuthenticationError: If authentication fails\n        InsufficientCreditsError: If team is out of credits\n        RateLimitError: If rate limit exceeded after retries\n        ServerError: If server error after retries\n        APIError: For other API errors\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(\n                url,\n                headers=headers,\n                json=data,\n                timeout=timeout\n            )\n            response.raise_for_status()\n            return response.json()\n\n        except requests.exceptions.Timeout:\n            print(f\"Request timed out (attempt {attempt + 1}/{max_retries})\")\n            if attempt == max_retries - 1:\n                raise APIError(\"Request timed out after all retries\")\n            time.sleep(2 ** attempt)\n\n        except requests.exceptions.ConnectionError as e:\n            print(f\"Connection error (attempt {attempt + 1}/{max_retries}): {e}\")\n            if attempt == max_retries - 1:\n                raise APIError(f\"Connection failed after all retries: {e}\")\n            time.sleep(2 ** attempt)\n\n        except requests.exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            detail = e.response.json().get(\"detail\", \"Unknown error\")\n\n            # 401: Authentication error (don't retry)\n            if status_code == 401:\n                raise AuthenticationError(f\"Authentication failed: {detail}\")\n\n            # 403: Forbidden (don't retry)\n            elif status_code == 403:\n                if \"credit\" in detail.lower():\n                    raise InsufficientCreditsError(f\"Insufficient credits: {detail}\")\n                else:\n                    raise APIError(f\"Access denied: {detail}\")\n\n            # 404: Not found (don't retry)\n            elif status_code == 404:\n                raise APIError(f\"Resource not found: {detail}\")\n\n            # 422: Validation error (don't retry)\n            elif status_code == 422:\n                raise APIError(f\"Validation error: {detail}\")\n\n            # 429: Rate limit (retry with backoff)\n            elif status_code == 429:\n                if attempt &lt; max_retries - 1:\n                    wait_time = 2 ** attempt\n                    print(f\"Rate limited. Waiting {wait_time}s (attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(wait_time)\n                else:\n                    raise RateLimitError(f\"Rate limit exceeded after all retries: {detail}\")\n\n            # 500/503: Server errors (retry with backoff)\n            elif status_code in [500, 503]:\n                if attempt &lt; max_retries - 1:\n                    wait_time = 2 ** attempt\n                    print(f\"Server error. Retrying in {wait_time}s (attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(wait_time)\n                else:\n                    raise ServerError(f\"Server error after all retries: {detail}\")\n\n            # Other errors\n            else:\n                raise APIError(f\"HTTP {status_code}: {detail}\")\n\n    raise APIError(\"Request failed after all retries\")\n\n# Usage\ntry:\n    result = make_api_request(\n        url=f\"{API_URL}/jobs/create\",\n        headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n        data={\"team_id\": \"acme-corp\", \"job_type\": \"test\"}\n    )\n    print(f\"Job created: {result['job_id']}\")\n\nexcept AuthenticationError as e:\n    print(f\"Auth error: {e}\")\n    # Refresh virtual key or notify admin\n\nexcept InsufficientCreditsError as e:\n    print(f\"Credits error: {e}\")\n    # Notify user to add credits\n\nexcept RateLimitError as e:\n    print(f\"Rate limit error: {e}\")\n    # Queue request for later\n\nexcept ServerError as e:\n    print(f\"Server error: {e}\")\n    # Log error and notify ops team\n\nexcept APIError as e:\n    print(f\"API error: {e}\")\n    # Generic error handling\n</code></pre>"},{"location":"integration/error-handling/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"integration/error-handling/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\nimport json\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\ndef make_logged_request(url, headers, data):\n    \"\"\"Make request with detailed logging\"\"\"\n    request_id = str(uuid.uuid4())\n\n    logger.info(f\"Request {request_id}: POST {url}\")\n\n    try:\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n\n        logger.info(f\"Request {request_id}: Success (status={response.status_code})\")\n        return response.json()\n\n    except requests.exceptions.HTTPError as e:\n        logger.error(\n            f\"Request {request_id}: HTTP error\",\n            extra={\n                \"status_code\": e.response.status_code,\n                \"detail\": e.response.json().get(\"detail\", \"Unknown\"),\n                \"url\": url\n            }\n        )\n        raise\n\n    except Exception as e:\n        logger.error(\n            f\"Request {request_id}: Failed\",\n            extra={\"error\": str(e), \"url\": url}\n        )\n        raise\n</code></pre>"},{"location":"integration/error-handling/#metrics-tracking","title":"Metrics Tracking","text":"<pre><code>import time\nfrom collections import defaultdict\n\nclass APIMetrics:\n    \"\"\"Track API metrics\"\"\"\n\n    def __init__(self):\n        self.requests = defaultdict(int)\n        self.errors = defaultdict(int)\n        self.latencies = []\n\n    def record_request(self, endpoint: str, latency_ms: float, success: bool):\n        \"\"\"Record a request\"\"\"\n        self.requests[endpoint] += 1\n        self.latencies.append(latency_ms)\n\n        if not success:\n            self.errors[endpoint] += 1\n\n    def get_stats(self):\n        \"\"\"Get aggregated stats\"\"\"\n        total_requests = sum(self.requests.values())\n        total_errors = sum(self.errors.values())\n        avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else 0\n\n        return {\n            \"total_requests\": total_requests,\n            \"total_errors\": total_errors,\n            \"error_rate\": total_errors / total_requests if total_requests &gt; 0 else 0,\n            \"avg_latency_ms\": avg_latency,\n            \"p95_latency_ms\": sorted(self.latencies)[int(len(self.latencies) * 0.95)] if self.latencies else 0\n        }\n\n# Usage\nmetrics = APIMetrics()\n\ndef make_tracked_request(url, headers, data):\n    \"\"\"Make request with metrics tracking\"\"\"\n    start_time = time.time()\n    success = False\n\n    try:\n        response = requests.post(url, headers=headers, json=data, timeout=30)\n        response.raise_for_status()\n        success = True\n        return response.json()\n\n    finally:\n        latency_ms = (time.time() - start_time) * 1000\n        metrics.record_request(url, latency_ms, success)\n\n# Later, check metrics\nstats = metrics.get_stats()\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Error rate: {stats['error_rate']:.2%}\")\nprint(f\"Avg latency: {stats['avg_latency_ms']:.0f}ms\")\n</code></pre>"},{"location":"integration/error-handling/#response-type-definitions","title":"Response Type Definitions","text":"<p>Understanding the structure of API responses helps with proper error handling and data extraction.</p>"},{"location":"integration/error-handling/#success-response-structure","title":"Success Response Structure","text":"<pre><code>// Jobs API - Create Job\n{\n  \"job_id\": string,           // UUID\n  \"status\": \"pending\",\n  \"created_at\": string        // ISO 8601 timestamp\n}\n\n// Jobs API - Complete Job\n{\n  \"job_id\": string,\n  \"status\": \"completed\" | \"failed\",\n  \"completed_at\": string,\n  \"costs\": {\n    \"total_calls\": number,\n    \"successful_calls\": number,\n    \"failed_calls\": number,\n    \"total_tokens\": number,\n    \"total_cost_usd\": number,\n    \"avg_latency_ms\": number,\n    \"credit_applied\": boolean,\n    \"credits_remaining\": number\n  },\n  \"calls\": Array&lt;{\n    \"call_id\": string,\n    \"purpose\": string,\n    \"model_group\": string,\n    \"tokens\": number,\n    \"latency_ms\": number,\n    \"error\": string | null\n  }&gt;\n}\n\n// Jobs API - Single-Call Job\n{\n  \"job_id\": string,\n  \"status\": \"completed\",\n  \"response\": {\n    \"content\": string,\n    \"finish_reason\": \"stop\" | \"length\" | \"content_filter\"\n  },\n  \"metadata\": {\n    \"tokens_used\": number,\n    \"latency_ms\": number,\n    \"model\": string\n  },\n  \"costs\": { /* same as Complete Job */ },\n  \"completed_at\": string\n}\n\n// LLM Calls API\n{\n  \"call_id\": string,\n  \"response\": {\n    \"content\": string,\n    \"finish_reason\": string\n  },\n  \"metadata\": {\n    \"tokens_used\": number,\n    \"latency_ms\": number,\n    \"model_group\": string\n  }\n}\n</code></pre>"},{"location":"integration/error-handling/#error-response-structure","title":"Error Response Structure","text":"<p>All error responses follow this format:</p> <pre><code>{\n  \"detail\": string | Array&lt;ValidationError&gt;\n}\n\n// Validation errors (422 status)\n{\n  \"detail\": [\n    {\n      \"loc\": Array&lt;string | number&gt;,  // Field path\n      \"msg\": string,                   // Error message\n      \"type\": string                   // Error type\n    }\n  ]\n}\n</code></pre> <p>Examples:</p> <pre><code>// 401 Unauthorized\n{\n  \"detail\": \"Invalid or missing API key\"\n}\n\n// 403 Forbidden\n{\n  \"detail\": \"Team suspended or insufficient credits\"\n}\n\n// 404 Not Found\n{\n  \"detail\": \"Job not found\"\n}\n\n// 422 Validation Error\n{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"messages\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n\n// 429 Rate Limit\n{\n  \"detail\": \"Rate limit exceeded\"\n}\n\n// 500 Server Error\n{\n  \"detail\": \"Internal server error\"\n}\n</code></pre>"},{"location":"integration/error-handling/#retry-best-practices","title":"Retry Best Practices","text":""},{"location":"integration/error-handling/#when-to-retry","title":"When to Retry","text":"<p>Retry these errors: - \u2705 429 (Rate Limit) - Always retry with exponential backoff - \u2705 500 (Internal Server Error) - Retry up to 3 times - \u2705 503 (Service Unavailable) - Retry with longer delays - \u2705 Network timeouts - Retry with increased timeout - \u2705 Connection errors - Retry with exponential backoff</p> <p>Do NOT retry these errors: - \u274c 400 (Bad Request) - Fix the request format - \u274c 401 (Unauthorized) - Check authentication - \u274c 403 (Forbidden) - Check permissions/credits - \u274c 404 (Not Found) - Resource doesn't exist - \u274c 422 (Validation Error) - Fix validation issues</p>"},{"location":"integration/error-handling/#exponential-backoff-strategy","title":"Exponential Backoff Strategy","text":"<p>Recommended Configuration:</p> Attempt Wait Time Total Time Elapsed 1 0s (immediate) 0s 2 1s 1s 3 2s 3s 4 4s 7s 5 8s 15s <p>Implementation:</p> <pre><code>import time\nimport random\n\ndef exponential_backoff_with_jitter(attempt: int, base_delay: float = 1.0, max_delay: float = 32.0) -&gt; float:\n    \"\"\"\n    Calculate exponential backoff with jitter.\n\n    Args:\n        attempt: Current retry attempt (0-indexed)\n        base_delay: Base delay in seconds\n        max_delay: Maximum delay in seconds\n\n    Returns:\n        Delay in seconds with random jitter\n    \"\"\"\n    # Exponential: base_delay * 2^attempt\n    delay = min(base_delay * (2 ** attempt), max_delay)\n\n    # Add jitter (\u00b125% random variation)\n    jitter = delay * 0.25 * (2 * random.random() - 1)\n\n    return delay + jitter\n\ndef make_request_with_smart_retry(\n    url: str,\n    headers: dict,\n    data: dict,\n    max_retries: int = 3,\n    timeout: int = 30\n) -&gt; dict:\n    \"\"\"\n    Make API request with intelligent retry logic.\n\n    Retries:\n    - 429 (Rate Limit): Up to max_retries with exponential backoff\n    - 500/503 (Server errors): Up to max_retries with exponential backoff\n    - Timeouts/Connection errors: Up to max_retries with exponential backoff\n\n    Does NOT retry:\n    - 400, 401, 403, 404, 422: Client errors that won't be fixed by retrying\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(url, headers=headers, json=data, timeout=timeout)\n            response.raise_for_status()\n            return response.json()\n\n        except requests.exceptions.Timeout:\n            if attempt &lt; max_retries - 1:\n                delay = exponential_backoff_with_jitter(attempt)\n                print(f\"Timeout. Retrying in {delay:.1f}s (attempt {attempt + 1}/{max_retries})\")\n                time.sleep(delay)\n            else:\n                raise APIError(\"Request timed out after all retries\")\n\n        except requests.exceptions.ConnectionError:\n            if attempt &lt; max_retries - 1:\n                delay = exponential_backoff_with_jitter(attempt)\n                print(f\"Connection error. Retrying in {delay:.1f}s (attempt {attempt + 1}/{max_retries})\")\n                time.sleep(delay)\n            else:\n                raise APIError(\"Connection failed after all retries\")\n\n        except requests.exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            detail = e.response.json().get(\"detail\", \"Unknown error\")\n\n            # Don't retry client errors (4xx except 429)\n            if 400 &lt;= status_code &lt; 500 and status_code != 429:\n                if status_code == 401:\n                    raise AuthenticationError(f\"Authentication failed: {detail}\")\n                elif status_code == 403:\n                    raise APIError(f\"Forbidden: {detail}\")\n                elif status_code == 404:\n                    raise APIError(f\"Not found: {detail}\")\n                elif status_code == 422:\n                    raise APIError(f\"Validation error: {detail}\")\n                else:\n                    raise APIError(f\"Client error {status_code}: {detail}\")\n\n            # Retry 429 and 5xx errors\n            if attempt &lt; max_retries - 1:\n                delay = exponential_backoff_with_jitter(attempt)\n                print(f\"HTTP {status_code}. Retrying in {delay:.1f}s (attempt {attempt + 1}/{max_retries})\")\n                time.sleep(delay)\n            else:\n                if status_code == 429:\n                    raise RateLimitError(f\"Rate limit exceeded: {detail}\")\n                else:\n                    raise ServerError(f\"Server error {status_code}: {detail}\")\n\n    raise APIError(\"Request failed after all retries\")\n</code></pre>"},{"location":"integration/error-handling/#timeout-configuration","title":"Timeout Configuration","text":"<p>Recommended Timeouts by Endpoint:</p> Endpoint Recommended Timeout Notes <code>/api/jobs/create</code> 10s Fast operation <code>/api/jobs/{id}</code> (GET) 10s Fast operation <code>/api/jobs/{id}/llm-call</code> 60s LLM calls can be slow <code>/api/jobs/{id}/complete</code> 30s Aggregation + DB writes <code>/api/jobs/create-and-call</code> 60s Includes LLM call Streaming endpoints 120s Long-running operations <p>Example with endpoint-specific timeouts:</p> <pre><code>ENDPOINT_TIMEOUTS = {\n    \"/api/jobs/create\": 10,\n    \"/api/jobs/\": 10,  # GET job\n    \"/api/jobs/.*llm-call\": 60,\n    \"/api/jobs/.*complete\": 30,\n    \"/api/jobs/create-and-call\": 60,\n}\n\ndef get_timeout_for_endpoint(url: str) -&gt; int:\n    \"\"\"Get recommended timeout for endpoint\"\"\"\n    for pattern, timeout in ENDPOINT_TIMEOUTS.items():\n        if pattern in url:\n            return timeout\n    return 30  # Default\n\n# Usage\ntimeout = get_timeout_for_endpoint(url)\nresponse = requests.post(url, json=data, timeout=timeout)\n</code></pre>"},{"location":"integration/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"integration/error-handling/#1-always-use-timeouts","title":"1. Always Use Timeouts","text":"<pre><code># \u2705 Good - With appropriate timeout\nresponse = requests.post(url, json=data, timeout=30)\n\n# \u2705 Better - With endpoint-specific timeout\ntimeout = get_timeout_for_endpoint(url)\nresponse = requests.post(url, json=data, timeout=timeout)\n\n# \u274c Bad - Can hang forever\nresponse = requests.post(url, json=data)\n</code></pre>"},{"location":"integration/error-handling/#2-implement-smart-retry-logic","title":"2. Implement Smart Retry Logic","text":"<pre><code># \u2705 Good - Exponential backoff with jitter\nfor attempt in range(3):\n    try:\n        response = requests.post(url, json=data, timeout=30)\n        response.raise_for_status()\n        break\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code in [429, 500, 503] and attempt &lt; 2:\n            delay = (2 ** attempt) + random.uniform(0, 0.5)\n            time.sleep(delay)\n        else:\n            raise\n\n# \u274c Bad - Fixed delay, retries everything\nfor attempt in range(3):\n    try:\n        response = requests.post(url, json=data, timeout=30)\n        response.raise_for_status()\n        break\n    except:\n        time.sleep(1)  # Always 1 second, no distinction between errors\n</code></pre>"},{"location":"integration/error-handling/#3-handle-specific-errors","title":"3. Handle Specific Errors","text":"<pre><code># \u2705 Good - Handle specific errors differently\ntry:\n    response = make_request()\nexcept AuthenticationError:\n    # Refresh key\n    pass\nexcept InsufficientCreditsError:\n    # Notify user\n    pass\nexcept RateLimitError:\n    # Queue for later\n    pass\n\n# \u274c Bad - Catch-all error handling\ntry:\n    response = make_request()\nexcept Exception:\n    # What went wrong?\n    pass\n</code></pre>"},{"location":"integration/error-handling/#4-log-errors-with-context","title":"4. Log Errors with Context","text":"<pre><code>try:\n    response = make_request()\nexcept Exception as e:\n    logger.error(\n        \"API request failed\",\n        extra={\n            \"error\": str(e),\n            \"error_type\": type(e).__name__,\n            \"team_id\": team_id,\n            \"job_id\": job_id,\n            \"url\": url,\n            \"attempt\": attempt,\n            \"status_code\": getattr(e.response, 'status_code', None)\n        }\n    )\n</code></pre>"},{"location":"integration/error-handling/#5-complete-jobs-on-failure","title":"5. Complete Jobs on Failure","text":"<pre><code>try:\n    # Make LLM call\n    response = llm_call(job_id, messages)\n    # Complete successfully\n    complete_job(job_id, \"completed\")\nexcept Exception as e:\n    # Mark job as failed (important for credit tracking!)\n    complete_job(job_id, \"failed\", error_message=str(e))\n    raise\n</code></pre>"},{"location":"integration/error-handling/#6-use-circuit-breaker-pattern","title":"6. Use Circuit Breaker Pattern","text":"<p>For high-volume applications, implement circuit breaker to prevent cascading failures:</p> <pre><code>from datetime import datetime, timedelta\n\nclass CircuitBreaker:\n    \"\"\"Simple circuit breaker implementation\"\"\"\n\n    def __init__(self, failure_threshold: int = 5, timeout: int = 60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failures = 0\n        self.last_failure_time = None\n        self.state = \"closed\"  # closed, open, half_open\n\n    def call(self, func, *args, **kwargs):\n        \"\"\"Call function with circuit breaker\"\"\"\n\n        # If circuit is open, check if timeout expired\n        if self.state == \"open\":\n            if datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.timeout):\n                self.state = \"half_open\"\n                self.failures = 0\n            else:\n                raise APIError(\"Circuit breaker is open\")\n\n        try:\n            result = func(*args, **kwargs)\n\n            # Success - reset failures\n            if self.state == \"half_open\":\n                self.state = \"closed\"\n            self.failures = 0\n\n            return result\n\n        except (ServerError, RateLimitError) as e:\n            # Track failures\n            self.failures += 1\n            self.last_failure_time = datetime.now()\n\n            # Open circuit if threshold exceeded\n            if self.failures &gt;= self.failure_threshold:\n                self.state = \"open\"\n\n            raise\n\n# Usage\ncircuit_breaker = CircuitBreaker(failure_threshold=5, timeout=60)\n\ntry:\n    result = circuit_breaker.call(\n        make_api_request,\n        url=url,\n        headers=headers,\n        data=data\n    )\nexcept APIError as e:\n    print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"integration/error-handling/#next-steps","title":"Next Steps","text":"<p>Now that you understand error handling:</p> <ol> <li>Learn Best Practices - Optimization and performance tips</li> <li>See Examples - Working code with error handling</li> <li>Review API Reference - All error codes documented</li> <li>Troubleshooting Guide - Common issues and solutions</li> </ol>"},{"location":"integration/job-workflow/","title":"Job Workflow","text":"<p>Learn about the job-based workflow that powers SaaS LiteLLM's cost tracking and billing system.</p>"},{"location":"integration/job-workflow/#what-is-a-job","title":"What is a Job?","text":"<p>A Job is a logical grouping of related LLM calls that represent a single business operation or workflow. Instead of tracking and billing individual LLM calls, the platform groups them into jobs for simplified cost management.</p>"},{"location":"integration/job-workflow/#why-jobs","title":"Why Jobs?","text":"<p>Traditional LLM APIs charge per API call. This creates complexity when a single workflow makes multiple calls:</p> Traditional Approach Job-Based Approach \u274c Charge per API call \u2705 Charge per completed job \u274c Hard to track workflow costs \u2705 Aggregate costs per workflow \u274c Complex billing \u2705 Simple billing (1 credit = 1 job) \u274c API-centric \u2705 Business-centric"},{"location":"integration/job-workflow/#real-world-example-resume-analysis","title":"Real-World Example: Resume Analysis","text":"<p>Your resume analysis tool performs multiple LLM operations:</p> <pre><code>Job: \"resume_analysis\"\n\u251c\u2500\u2500 LLM Call 1: Parse resume text\n\u251c\u2500\u2500 LLM Call 2: Compare requirements with candidate qualifications\n\u2514\u2500\u2500 LLM Call 3: Generate executive summary\n\nResult: 1 Job = 1 Credit charged (not 3 credits!)\n</code></pre> <p>Key Benefits: - Simplified Billing - Pay per job completion, not per LLM call - Cost Aggregation - Track total cost across multiple calls - Usage Analytics - Understand which workflows are expensive - Failure Handling - Failed jobs don't consume credits</p>"},{"location":"integration/job-workflow/#job-lifecycle","title":"Job Lifecycle","text":"<p>A job goes through several states during its lifetime:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; pending: Create Job (POST /api/jobs/create)\n    pending --&gt; in_progress: First LLM Call\n    in_progress --&gt; in_progress: Additional LLM Calls\n    in_progress --&gt; completed: Complete Job (status: completed)\n    in_progress --&gt; failed: Complete Job (status: failed)\n    completed --&gt; [*]: 1 credit deducted \u2713\n    failed --&gt; [*]: No credit deducted\n\n    note right of completed\n        Credit deducted ONLY for completed jobs\n        with all successful LLM calls\n    end note\n\n    note right of failed\n        Failed jobs don't consume credits,\n        allowing you to retry without cost\n    end note</code></pre>"},{"location":"integration/job-workflow/#job-states","title":"Job States","text":"State Description Credit Impact <code>pending</code> Job created, no LLM calls yet None <code>in_progress</code> At least one LLM call made None (until completion) <code>completed</code> Job successfully completed 1 credit deducted <code>failed</code> Job failed or cancelled No credit deducted"},{"location":"integration/job-workflow/#workflow-options","title":"Workflow Options","text":"<p>SaaS LiteLLM supports two workflow patterns:</p>"},{"location":"integration/job-workflow/#1-single-call-workflow-recommended-for-simple-use-cases","title":"1. Single-Call Workflow (Recommended for Simple Use Cases)","text":"<p>Best for chat applications, single-turn responses, and simple tasks requiring only one LLM call.</p> <p>Advantages: - ~66% latency reduction (1 API call vs 3) - Simpler code - Automatic error handling - Built-in credit deduction</p> <p>When to use: - Chat applications - Single-turn text generation - Simple classification tasks - Any workflow with just one LLM call</p> <p>Jump to: Single-Call Workflow \u2192</p>"},{"location":"integration/job-workflow/#2-multi-step-workflow-for-complex-operations","title":"2. Multi-Step Workflow (For Complex Operations)","text":"<p>Best for complex workflows requiring multiple LLM calls, agentic workflows, or batch processing.</p> <p>Advantages: - Granular control over each step - Support for multiple LLM calls per job - Retry individual calls without starting over - Track intermediate results</p> <p>When to use: - Multi-step document analysis - Agentic workflows - Batch processing - Complex chains requiring multiple calls</p> <p>Jump to: Multi-Step Workflow \u2192</p>"},{"location":"integration/job-workflow/#single-call-workflow","title":"Single-Call Workflow","text":"<p>For simple workflows that only need a single LLM call, use the combined create-and-call endpoint:</p>"},{"location":"integration/job-workflow/#endpoint","title":"Endpoint","text":"<pre><code>POST /api/jobs/create-and-call\n</code></pre>"},{"location":"integration/job-workflow/#example-chat-response","title":"Example: Chat Response","text":"PythonJavaScriptcURL <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\nheaders = {\n    \"Authorization\": \"Bearer sk-your-virtual-key\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Single API call - creates job, calls LLM, and completes job\nresponse = requests.post(\n    f\"{API}/jobs/create-and-call\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"chat_response\",\n        \"model\": \"gpt-4\",  # Model alias or group\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ],\n        \"temperature\": 0.7,\n        \"max_tokens\": 500\n    }\n)\n\nresult = response.json()\nprint(f\"Response: {result['response']['content']}\")\nprint(f\"Tokens used: {result['metadata']['tokens_used']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre> <pre><code>const API = \"http://localhost:8003/api\";\nconst headers = {\n  'Authorization': 'Bearer sk-your-virtual-key',\n  'Content-Type': 'application/json'\n};\n\n// Single API call - creates job, calls LLM, and completes job\nconst response = await fetch(`${API}/jobs/create-and-call`, {\n  method: 'POST',\n  headers: headers,\n  body: JSON.stringify({\n    team_id: 'acme-corp',\n    job_type: 'chat_response',\n    model: 'gpt-4',\n    messages: [\n      {role: 'user', content: 'What is Python?'}\n    ],\n    temperature: 0.7,\n    max_tokens: 500\n  })\n});\n\nconst result = await response.json();\nconsole.log(`Response: ${result.response.content}`);\nconsole.log(`Tokens used: ${result.metadata.tokens_used}`);\nconsole.log(`Credits remaining: ${result.costs.credits_remaining}`);\n</code></pre> <pre><code>curl -X POST http://localhost:8003/api/jobs/create-and-call \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"chat_response\",\n    \"model\": \"gpt-4\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is Python?\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"response\": {\n    \"content\": \"Python is a high-level programming language...\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 256,\n    \"latency_ms\": 1340,\n    \"model\": \"gpt-4\"\n  },\n  \"costs\": {\n    \"total_calls\": 1,\n    \"successful_calls\": 1,\n    \"failed_calls\": 0,\n    \"total_tokens\": 256,\n    \"total_cost_usd\": 0.0128,\n    \"avg_latency_ms\": 1340,\n    \"credit_applied\": true,\n    \"credits_remaining\": 999\n  },\n  \"completed_at\": \"2024-10-14T12:00:05.340Z\"\n}\n</code></pre></p>"},{"location":"integration/job-workflow/#error-handling","title":"Error Handling","text":"<p>The single-call endpoint automatically handles errors:</p> <pre><code>try:\n    response = requests.post(\n        f\"{API}/jobs/create-and-call\",\n        headers=headers,\n        json={\n            \"team_id\": \"acme-corp\",\n            \"job_type\": \"chat\",\n            \"model\": \"gpt-4\",\n            \"messages\": messages\n        },\n        timeout=30\n    )\n    response.raise_for_status()\n    result = response.json()\n\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 403:\n        print(f\"Access error: {e.response.json()['detail']}\")\n    elif e.response.status_code == 500:\n        print(f\"LLM call failed: {e.response.json()['detail']}\")\n    else:\n        print(f\"Request failed: {e}\")\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\n</code></pre>"},{"location":"integration/job-workflow/#performance-comparison","title":"Performance Comparison","text":"Workflow Type API Calls Latency Best For Single-Call 1 ~1.5s Chat apps, simple tasks Multi-Step 3+ ~4.5s+ Complex workflows, multiple calls <p>Latency breakdown: <pre><code>Single-Call:     [Create+Call+Complete] = ~1.5s\n                           \u2193\nMulti-Step:      [Create] \u2192 [Call] \u2192 [Complete] = ~4.5s\n                   ~0.1s     ~1.4s      ~0.1s\n</code></pre></p>"},{"location":"integration/job-workflow/#multi-step-workflow","title":"Multi-Step Workflow","text":"<p>For complex workflows requiring multiple LLM calls, use the full job lifecycle:</p>"},{"location":"integration/job-workflow/#1-create-job","title":"1. Create Job","text":"<p>Create a job to start tracking LLM calls:</p> PythonJavaScriptcURL <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\nheaders = {\n    \"Authorization\": \"Bearer sk-your-virtual-key\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Create job\nresponse = requests.post(\n    f\"{API}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"user_id\": \"john@acme.com\",\n        \"job_type\": \"document_analysis\",\n        \"metadata\": {\n            \"document_id\": \"doc_123\",\n            \"document_name\": \"report.pdf\",\n            \"pages\": 5\n        }\n    }\n)\n\njob = response.json()\njob_id = job[\"job_id\"]\nprint(f\"Created job: {job_id}\")\n</code></pre> <pre><code>const API = \"http://localhost:8003/api\";\nconst headers = {\n  'Authorization': 'Bearer sk-your-virtual-key',\n  'Content-Type': 'application/json'\n};\n\n// Create job\nconst response = await fetch(`${API}/jobs/create`, {\n  method: 'POST',\n  headers: headers,\n  body: JSON.stringify({\n    team_id: 'acme-corp',\n    user_id: 'john@acme.com',\n    job_type: 'document_analysis',\n    metadata: {\n      document_id: 'doc_123',\n      document_name: 'report.pdf',\n      pages: 5\n    }\n  })\n});\n\nconst job = await response.json();\nconst jobId = job.job_id;\nconsole.log(`Created job: ${jobId}`);\n</code></pre> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-virtual-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"team_id\": \"acme-corp\",\n    \"user_id\": \"john@acme.com\",\n    \"job_type\": \"document_analysis\",\n    \"metadata\": {\n      \"document_id\": \"doc_123\",\n      \"document_name\": \"report.pdf\",\n      \"pages\": 5\n    }\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2024-10-14T12:00:00.000Z\"\n}\n</code></pre></p>"},{"location":"integration/job-workflow/#2-make-llm-calls","title":"2. Make LLM Calls","text":"<p>Make one or more LLM calls associated with the job:</p> <pre><code># Make multiple LLM calls for the same job\nfor step in [\"parse\", \"analyze\", \"summarize\"]:\n    response = requests.post(\n        f\"{API}/jobs/{job_id}/llm-call\",\n        headers=headers,\n        json={\n            \"messages\": [\n                {\"role\": \"user\", \"content\": f\"{step} this document...\"}\n            ],\n            \"purpose\": step  # Optional: for tracking\n        }\n    )\n\n    result = response.json()\n    print(f\"{step}: {result['response']['content'][:50]}...\")\n</code></pre> <p>Response: <pre><code>{\n  \"call_id\": \"call-uuid-1\",\n  \"response\": {\n    \"content\": \"Here is the analysis...\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 450,\n    \"latency_ms\": 1250\n  }\n}\n</code></pre></p> <p>Job Status Changes</p> <p>After the first LLM call, the job status automatically changes from <code>pending</code> to <code>in_progress</code>.</p>"},{"location":"integration/job-workflow/#3-complete-job","title":"3. Complete Job","text":"<p>Complete the job to trigger credit deduction and get cost summary:</p> <pre><code># Complete job successfully\nresponse = requests.post(\n    f\"{API}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\n        \"status\": \"completed\",\n        \"metadata\": {\n            \"result\": \"success\",\n            \"output_file\": \"analysis_123.json\"\n        }\n    }\n)\n\nresult = response.json()\n\nprint(f\"Job completed!\")\nprint(f\"Total calls: {result['costs']['total_calls']}\")\nprint(f\"Total cost: ${result['costs']['total_cost_usd']:.4f}\")\nprint(f\"Credit deducted: {result['costs']['credit_applied']}\")\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"completed_at\": \"2024-10-14T12:05:23.000Z\",\n  \"costs\": {\n    \"total_calls\": 3,\n    \"successful_calls\": 3,\n    \"failed_calls\": 0,\n    \"total_tokens\": 1350,\n    \"total_cost_usd\": 0.0045,\n    \"avg_latency_ms\": 1200,\n    \"credit_applied\": true,\n    \"credits_remaining\": 999\n  },\n  \"calls\": [\n    {\n      \"call_id\": \"call-uuid-1\",\n      \"purpose\": \"parse\",\n      \"tokens\": 450,\n      \"latency_ms\": 1250,\n      \"error\": null\n    },\n    {\n      \"call_id\": \"call-uuid-2\",\n      \"purpose\": \"analyze\",\n      \"tokens\": 480,\n      \"latency_ms\": 1180,\n      \"error\": null\n    },\n    {\n      \"call_id\": \"call-uuid-3\",\n      \"purpose\": \"summarize\",\n      \"tokens\": 420,\n      \"latency_ms\": 1170,\n      \"error\": null\n    }\n  ]\n}\n</code></pre></p>"},{"location":"integration/job-workflow/#credit-deduction-rules","title":"Credit Deduction Rules","text":"<p>Credits are ONLY deducted when ALL of these conditions are met:</p> <ol> <li>\u2705 Job status is <code>\"completed\"</code> (not <code>\"failed\"</code>)</li> <li>\u2705 All LLM calls succeeded (no failed calls)</li> <li>\u2705 Credit hasn't already been applied</li> </ol> <p>Key Points: - 1 Job = 1 Credit regardless of:   - Number of LLM calls (could be 1 or 100)   - Actual USD cost (tracked separately for analytics)   - Models used (different models in same job)   - Time duration (seconds or hours)</p>"},{"location":"integration/job-workflow/#handling-failures","title":"Handling Failures","text":"<p>If something goes wrong, complete the job as <code>failed</code> to avoid credit deduction:</p> <pre><code>try:\n    # Make LLM calls\n    for step in workflow_steps:\n        make_llm_call(job_id, step)\n\n    # Complete successfully\n    complete_job(job_id, \"completed\")\n\nexcept Exception as e:\n    # Complete as failed (no credit deducted)\n    requests.post(\n        f\"{API}/jobs/{job_id}/complete\",\n        headers=headers,\n        json={\n            \"status\": \"failed\",\n            \"error_message\": str(e),\n            \"metadata\": {\"error_type\": type(e).__name__}\n        }\n    )\n    print(f\"Job failed: {e} (no credit charged)\")\n</code></pre> <p>Always Complete Jobs</p> <p>Always complete jobs (even as failed) to prevent \"zombie jobs\" that stay in <code>in_progress</code> state forever. This keeps your analytics clean.</p>"},{"location":"integration/job-workflow/#job-metadata","title":"Job Metadata","text":"<p>Use metadata to store custom information about your jobs:</p>"},{"location":"integration/job-workflow/#job-creation-metadata","title":"Job Creation Metadata","text":"<pre><code>job = requests.post(f\"{API}/jobs/create\", headers=headers, json={\n    \"team_id\": \"acme-corp\",\n    \"user_id\": \"john@acme.com\",\n    \"job_type\": \"document_analysis\",\n    \"metadata\": {\n        # Your app's context\n        \"task_id\": \"task_123\",\n        \"user_email\": \"john@acme.com\",\n        \"document_name\": \"report.pdf\",\n        \"document_size_mb\": 2.5,\n        \"priority\": \"high\",\n        \"source\": \"upload\",\n        \"workflow_version\": \"v2.1\"\n    }\n}).json()\n</code></pre>"},{"location":"integration/job-workflow/#completion-metadata","title":"Completion Metadata","text":"<pre><code>result = requests.post(\n    f\"{API}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\n        \"status\": \"completed\",\n        \"metadata\": {\n            # Your app's results\n            \"output_file\": \"analysis_123.json\",\n            \"confidence_score\": 0.95,\n            \"warnings\": [\"Low quality image on page 3\"],\n            \"processing_time_seconds\": 12.5,\n            \"pages_processed\": 5\n        }\n    }\n).json()\n</code></pre>"},{"location":"integration/job-workflow/#complete-example-multi-step-workflow","title":"Complete Example: Multi-Step Workflow","text":"<p>Here's a complete example showing a multi-step document analysis workflow:</p> <pre><code>import requests\nfrom typing import List, Dict\n\nclass DocumentAnalyzer:\n    def __init__(self, base_url: str, virtual_key: str):\n        self.base_url = base_url\n        self.headers = {\n            \"Authorization\": f\"Bearer {virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def analyze_document(\n        self,\n        team_id: str,\n        document_text: str,\n        document_name: str\n    ) -&gt; Dict:\n        \"\"\"\n        Complete document analysis workflow:\n        1. Extract key information\n        2. Classify document type\n        3. Generate summary\n\n        3 LLM calls = 1 Job = 1 Credit\n        \"\"\"\n        # Step 1: Create job\n        job = requests.post(\n            f\"{self.base_url}/api/jobs/create\",\n            headers=self.headers,\n            json={\n                \"team_id\": team_id,\n                \"job_type\": \"document_analysis\",\n                \"metadata\": {\n                    \"document_name\": document_name,\n                    \"document_length\": len(document_text)\n                }\n            }\n        ).json()\n\n        job_id = job[\"job_id\"]\n        print(f\"\u2713 Created job: {job_id}\")\n\n        try:\n            # Step 2: Extract key information\n            extraction = requests.post(\n                f\"{self.base_url}/api/jobs/{job_id}/llm-call\",\n                headers=self.headers,\n                json={\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"Extract key information from documents.\"\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": f\"Extract key info from:\\n\\n{document_text}\"\n                        }\n                    ],\n                    \"purpose\": \"extraction\"\n                }\n            ).json()\n\n            extracted_info = extraction[\"response\"][\"content\"]\n            print(f\"\u2713 Extracted information\")\n\n            # Step 3: Classify document type\n            classification = requests.post(\n                f\"{self.base_url}/api/jobs/{job_id}/llm-call\",\n                headers=self.headers,\n                json={\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"Classify document types.\"\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": f\"Classify: {extracted_info}\"\n                        }\n                    ],\n                    \"purpose\": \"classification\"\n                }\n            ).json()\n\n            doc_type = classification[\"response\"][\"content\"]\n            print(f\"\u2713 Classified as: {doc_type}\")\n\n            # Step 4: Generate summary\n            summary = requests.post(\n                f\"{self.base_url}/api/jobs/{job_id}/llm-call\",\n                headers=self.headers,\n                json={\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"Generate concise summaries.\"\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": f\"Summarize: {extracted_info}\"\n                        }\n                    ],\n                    \"purpose\": \"summarization\"\n                }\n            ).json()\n\n            doc_summary = summary[\"response\"][\"content\"]\n            print(f\"\u2713 Generated summary\")\n\n            # Step 5: Complete job successfully\n            completion = requests.post(\n                f\"{self.base_url}/api/jobs/{job_id}/complete\",\n                headers=self.headers,\n                json={\n                    \"status\": \"completed\",\n                    \"metadata\": {\n                        \"result\": \"success\",\n                        \"document_type\": doc_type,\n                        \"summary_length\": len(doc_summary)\n                    }\n                }\n            ).json()\n\n            print(f\"\\n\u2705 Job completed!\")\n            print(f\"   Total calls: {completion['costs']['total_calls']}\")\n            print(f\"   Total cost: ${completion['costs']['total_cost_usd']:.4f}\")\n            print(f\"   Credits remaining: {completion['costs']['credits_remaining']}\")\n\n            return {\n                \"extracted_info\": extracted_info,\n                \"document_type\": doc_type,\n                \"summary\": doc_summary,\n                \"costs\": completion[\"costs\"]\n            }\n\n        except Exception as e:\n            # Complete as failed (no credit charge)\n            requests.post(\n                f\"{self.base_url}/api/jobs/{job_id}/complete\",\n                headers=self.headers,\n                json={\n                    \"status\": \"failed\",\n                    \"error_message\": str(e)\n                }\n            )\n            print(f\"\u274c Job failed: {e} (no credit charged)\")\n            raise\n\n# Usage\nanalyzer = DocumentAnalyzer(\n    base_url=\"http://localhost:8003\",\n    virtual_key=\"sk-your-virtual-key\"\n)\n\nresult = analyzer.analyze_document(\n    team_id=\"acme-corp\",\n    document_text=\"Your document text here...\",\n    document_name=\"report.pdf\"\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANALYSIS RESULTS\")\nprint(\"=\"*50)\nprint(f\"Type: {result['document_type']}\")\nprint(f\"Summary: {result['summary']}\")\n</code></pre> <p>Output: <pre><code>\u2713 Created job: 7f3d9a8b-4c21-4e89-b5d3-2a1c8f6e9b0d\n\u2713 Extracted information\n\u2713 Classified as: Financial Report\n\u2713 Generated summary\n\n\u2705 Job completed!\n   Total calls: 3\n   Total cost: $0.0045\n   Credits remaining: 999\n\n==================================================\nANALYSIS RESULTS\n==================================================\nType: Financial Report\nSummary: This quarterly financial report shows...\n</code></pre></p>"},{"location":"integration/job-workflow/#best-practices","title":"Best Practices","text":""},{"location":"integration/job-workflow/#1-always-complete-jobs","title":"1. Always Complete Jobs","text":"<pre><code># \u2705 GOOD: Always complete jobs\ntry:\n    make_llm_calls(job_id)\n    complete_job(job_id, \"completed\")\nexcept Exception as e:\n    complete_job(job_id, \"failed\", error_message=str(e))\n\n# \u274c BAD: Leaving jobs incomplete\ntry:\n    make_llm_calls(job_id)\n    complete_job(job_id, \"completed\")\nexcept Exception as e:\n    pass  # Job stays in in_progress forever!\n</code></pre>"},{"location":"integration/job-workflow/#2-use-meaningful-job-types","title":"2. Use Meaningful Job Types","text":"<pre><code># \u2705 GOOD: Descriptive job types\njob_types = [\n    \"document_analysis\",\n    \"resume_parsing\",\n    \"chat_session\",\n    \"data_extraction\",\n    \"content_generation\"\n]\n\n# \u274c BAD: Generic job types\njob_types = [\"job1\", \"job2\", \"task\", \"process\"]\n</code></pre>"},{"location":"integration/job-workflow/#3-add-contextual-metadata","title":"3. Add Contextual Metadata","text":"<pre><code># \u2705 GOOD: Rich metadata\nmetadata = {\n    \"task_id\": \"task_123\",\n    \"user_email\": \"john@acme.com\",\n    \"document_name\": \"report.pdf\",\n    \"priority\": \"high\",\n    \"workflow_version\": \"v2.1\"\n}\n\n# \u274c BAD: Empty or minimal metadata\nmetadata = {}\n</code></pre>"},{"location":"integration/job-workflow/#4-use-purpose-for-call-tracking","title":"4. Use Purpose for Call Tracking","text":"<pre><code># \u2705 GOOD: Label each LLM call\npurposes = [\"parsing\", \"classification\", \"summarization\"]\n\n# \u274c BAD: No purpose tracking\npurpose = None\n</code></pre>"},{"location":"integration/job-workflow/#viewing-job-details","title":"Viewing Job Details","text":"<p>You can retrieve job details at any time:</p> <pre><code>curl http://localhost:8003/api/jobs/{job_id} \\\n  -H \"Authorization: Bearer sk-your-virtual-key\"\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"team_id\": \"acme-corp\",\n  \"user_id\": \"john@acme.com\",\n  \"job_type\": \"document_analysis\",\n  \"status\": \"completed\",\n  \"created_at\": \"2024-10-14T12:00:00.000Z\",\n  \"completed_at\": \"2024-10-14T12:05:23.000Z\",\n  \"metadata\": {\n    \"document_id\": \"doc_123\",\n    \"result\": \"success\"\n  },\n  \"costs\": {\n    \"total_calls\": 3,\n    \"total_cost_usd\": 0.0045,\n    \"credit_applied\": true\n  },\n  \"calls\": [...]\n}\n</code></pre></p>"},{"location":"integration/job-workflow/#next-steps","title":"Next Steps","text":"<ul> <li>Non-Streaming Calls - Learn about standard LLM calls</li> <li>Streaming Calls - Implement real-time streaming</li> <li>Error Handling - Handle errors gracefully</li> <li>Examples - See working code examples</li> </ul>"},{"location":"integration/non-streaming/","title":"Non-Streaming LLM Calls","text":"<p>Learn how to make standard (buffered) LLM calls where the complete response is returned at once.</p>"},{"location":"integration/non-streaming/#overview","title":"Overview","text":"<p>Non-streaming calls are the simplest way to interact with LLMs. The client sends a request and waits for the complete response to be returned at once.</p> <p>Best for: - Batch processing workflows - Simple requests with short responses - Structured outputs (JSON, Pydantic models) - When progressive display isn't needed - Background tasks</p> <p>Characteristics: - \u2705 Simpler to implement - \u2705 Complete response at once - \u2705 Easier error handling - \u2705 Perfect for structured outputs - \u274c Higher perceived latency (wait for full response) - \u274c No progressive display</p>"},{"location":"integration/non-streaming/#basic-non-streaming-call","title":"Basic Non-Streaming Call","text":""},{"location":"integration/non-streaming/#complete-workflow","title":"Complete Workflow","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create job\njob_response = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"document_analysis\"\n    }\n).json()\n\njob_id = job_response[\"job_id\"]\nprint(f\"Created job: {job_id}\")\n\n# 2. Make LLM call\nllm_response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Analyze this document and summarize the key points\"}\n        ]\n    }\n).json()\n\nprint(f\"Response: {llm_response['response']['content']}\")\n\n# 3. Complete job\ncomplete_response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\"status\": \"completed\"}\n).json()\n\nprint(f\"Job completed. Credits remaining: {complete_response['costs']['credits_remaining']}\")\n</code></pre>"},{"location":"integration/non-streaming/#request-format","title":"Request Format","text":"<p>Endpoint: <code>POST /api/jobs/{job_id}/llm-call</code></p> <p>Headers: <pre><code>{\n  \"Authorization\": \"Bearer sk-your-virtual-key\",\n  \"Content-Type\": \"application/json\"\n}\n</code></pre></p> <p>Body: <pre><code>{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 500\n}\n</code></pre></p> <p>Optional Parameters: - <code>temperature</code> (float, 0.0-2.0) - Controls randomness - <code>max_tokens</code> (int) - Maximum response length - <code>top_p</code> (float, 0.0-1.0) - Nucleus sampling - <code>frequency_penalty</code> (float, -2.0 to 2.0) - Reduce repetition - <code>presence_penalty</code> (float, -2.0 to 2.0) - Encourage new topics - <code>stop</code> (string or array) - Stop sequences</p>"},{"location":"integration/non-streaming/#response-format","title":"Response Format","text":"<p>Success Response: <pre><code>{\n  \"call_id\": \"call_abc123\",\n  \"response\": {\n    \"content\": \"The capital of France is Paris.\",\n    \"role\": \"assistant\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 125,\n    \"latency_ms\": 850\n  }\n}\n</code></pre></p> <p>Fields: - <code>call_id</code> - Unique identifier for this LLM call - <code>response.content</code> - The LLM's response text - <code>response.role</code> - Always \"assistant\" - <code>response.finish_reason</code> - Why the response ended (\"stop\", \"length\", \"content_filter\") - <code>metadata.tokens_used</code> - Total tokens consumed (prompt + completion) - <code>metadata.latency_ms</code> - Response time in milliseconds</p>"},{"location":"integration/non-streaming/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<p>For chat-like interactions, include conversation history in the messages:</p> <pre><code># First message\nresponse1 = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ]\n    }\n).json()\n\n# Second message - include history\nresponse2 = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is Python?\"},\n            {\"role\": \"assistant\", \"content\": response1[\"response\"][\"content\"]},\n            {\"role\": \"user\", \"content\": \"What are its main uses?\"}\n        ]\n    }\n).json()\n\nprint(response2[\"response\"][\"content\"])\n</code></pre> <p>Managing Context</p> <p>Each LLM call is independent. To maintain conversation context, you must include all previous messages in each request.</p>"},{"location":"integration/non-streaming/#system-prompts","title":"System Prompts","text":"<p>Use system messages to set the assistant's behavior:</p> <pre><code>response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a professional Python tutor. Explain concepts clearly and provide code examples.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How do I read a CSV file in Python?\"\n            }\n        ]\n    }\n).json()\n</code></pre>"},{"location":"integration/non-streaming/#temperature-and-creativity","title":"Temperature and Creativity","text":"<p>Control the randomness of responses with temperature:</p> <pre><code># Creative response (temperature = 1.5)\ncreative = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Write a creative tagline for a coffee shop\"}\n        ],\n        \"temperature\": 1.5\n    }\n).json()\n\n# Factual response (temperature = 0.2)\nfactual = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n        ],\n        \"temperature\": 0.2\n    }\n).json()\n</code></pre> <p>Temperature Guidelines: - 0.0-0.3 - Deterministic, factual (code generation, data extraction) - 0.4-0.7 - Balanced (general assistant, Q&amp;A) - 0.8-1.2 - Creative (content writing, brainstorming) - 1.3-2.0 - Very creative (poetry, experimental)</p>"},{"location":"integration/non-streaming/#limiting-response-length","title":"Limiting Response Length","text":"<p>Use <code>max_tokens</code> to control response length:</p> <pre><code># Short summary\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Summarize the theory of relativity\"}\n        ],\n        \"max_tokens\": 100  # ~75 words\n    }\n).json()\n</code></pre> <p>Token Limits</p> <ul> <li>Tokens include both input (prompt) and output (response)</li> <li>If <code>max_tokens</code> is too low, responses may be cut off</li> <li>Different models have different context limits</li> </ul>"},{"location":"integration/non-streaming/#using-stop-sequences","title":"Using Stop Sequences","text":"<p>Stop generation when certain text is encountered:</p> <pre><code>response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"List the first 5 planets:\\n1.\"}\n        ],\n        \"stop\": [\"\\n6.\", \"END\"]  # Stop at 6th planet or \"END\"\n    }\n).json()\n</code></pre>"},{"location":"integration/non-streaming/#structured-outputs","title":"Structured Outputs","text":"<p>For JSON responses, prompt the model explicitly:</p> <pre><code>response = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction API. Always respond with valid JSON.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract person info: John Smith, age 35, email john@example.com. Return as JSON with keys: name, age, email\"\n            }\n        ],\n        \"temperature\": 0.1  # Low temperature for consistency\n    }\n).json()\n\nimport json\ndata = json.loads(response[\"response\"][\"content\"])\nprint(data[\"name\"])  # \"John Smith\"\n</code></pre> <p>For type-safe structured outputs, use the typed client with Pydantic models:</p> <p> Learn more about structured outputs</p>"},{"location":"integration/non-streaming/#error-handling","title":"Error Handling","text":""},{"location":"integration/non-streaming/#http-error-handling","title":"HTTP Error Handling","text":"<pre><code>import requests\n\ndef make_llm_call(job_id, messages):\n    try:\n        response = requests.post(\n            f\"{API_URL}/jobs/{job_id}/llm-call\",\n            headers=headers,\n            json={\"messages\": messages},\n            timeout=30  # 30 second timeout\n        )\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.Timeout:\n        print(\"Request timed out\")\n        # Retry logic here\n        raise\n\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 400:\n            print(f\"Bad request: {e.response.json()}\")\n        elif e.response.status_code == 401:\n            print(\"Authentication failed\")\n        elif e.response.status_code == 403:\n            print(\"Insufficient credits or access denied\")\n        elif e.response.status_code == 429:\n            print(\"Rate limit exceeded\")\n        else:\n            print(f\"HTTP error: {e}\")\n        raise\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        raise\n\n# Usage\nresult = make_llm_call(job_id, [\n    {\"role\": \"user\", \"content\": \"Hello\"}\n])\n</code></pre>"},{"location":"integration/non-streaming/#common-errors","title":"Common Errors","text":"Status Code Error Solution 400 Invalid request format Check message structure and parameters 401 Invalid virtual key Verify your virtual key is correct 403 Insufficient credits Add credits to your team 404 Job not found Verify job_id exists 429 Rate limit exceeded Wait and retry with exponential backoff 500 Internal server error Retry with exponential backoff 503 Service unavailable Wait and retry <p> See comprehensive error handling guide</p>"},{"location":"integration/non-streaming/#best-practices","title":"Best Practices","text":""},{"location":"integration/non-streaming/#1-reuse-jobs-for-related-calls","title":"1. Reuse Jobs for Related Calls","text":"<p>Group related LLM calls into a single job for cost tracking:</p> <pre><code># \u2705 Good - One job for related calls\njob_id = create_job(\"document_analysis\")\nextract_text(job_id)\nclassify_content(job_id)\ngenerate_summary(job_id)\ncomplete_job(job_id)\n\n# \u274c Bad - Separate jobs for related calls\njob1 = create_job(\"extract_text\")\nextract_text(job1)\ncomplete_job(job1)\n\njob2 = create_job(\"classify_content\")\nclassify_content(job2)\ncomplete_job(job2)\n</code></pre>"},{"location":"integration/non-streaming/#2-set-reasonable-timeouts","title":"2. Set Reasonable Timeouts","text":"<pre><code># Set appropriate timeout based on expected response time\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\"messages\": messages},\n    timeout=30  # 30 seconds\n)\n</code></pre>"},{"location":"integration/non-streaming/#3-validate-responses","title":"3. Validate Responses","text":"<pre><code>response = make_llm_call(job_id, messages)\n\n# Check finish_reason\nif response[\"response\"][\"finish_reason\"] != \"stop\":\n    print(f\"Warning: Response ended with {response['response']['finish_reason']}\")\n    if response[\"response\"][\"finish_reason\"] == \"length\":\n        print(\"Response was truncated. Consider increasing max_tokens.\")\n\n# Validate content\ncontent = response[\"response\"][\"content\"]\nif not content or len(content) == 0:\n    print(\"Warning: Empty response received\")\n</code></pre>"},{"location":"integration/non-streaming/#4-cache-common-requests","title":"4. Cache Common Requests","text":"<p>For frequently repeated requests, consider caching responses:</p> <pre><code>import hashlib\nimport json\n\n# Simple in-memory cache\nresponse_cache = {}\n\ndef make_cached_llm_call(job_id, messages):\n    # Create cache key from messages\n    cache_key = hashlib.md5(\n        json.dumps(messages, sort_keys=True).encode()\n    ).hexdigest()\n\n    # Check cache\n    if cache_key in response_cache:\n        print(\"Cache hit!\")\n        return response_cache[cache_key]\n\n    # Make actual call\n    response = make_llm_call(job_id, messages)\n\n    # Store in cache\n    response_cache[cache_key] = response\n    return response\n</code></pre> <p>Redis Caching</p> <p>SaaS LiteLLM automatically caches responses in Redis. This is an additional application-level cache.</p>"},{"location":"integration/non-streaming/#5-monitor-performance","title":"5. Monitor Performance","text":"<p>Track token usage and latency:</p> <pre><code>def make_llm_call_with_metrics(job_id, messages):\n    response = make_llm_call(job_id, messages)\n\n    # Log metrics\n    print(f\"Tokens used: {response['metadata']['tokens_used']}\")\n    print(f\"Latency: {response['metadata']['latency_ms']}ms\")\n\n    # Alert on high usage\n    if response['metadata']['tokens_used'] &gt; 2000:\n        print(\"Warning: High token usage\")\n\n    if response['metadata']['latency_ms'] &gt; 5000:\n        print(\"Warning: High latency\")\n\n    return response\n</code></pre>"},{"location":"integration/non-streaming/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of a document analysis workflow:</p> <pre><code>import requests\nimport json\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef analyze_document(team_id, document_text):\n    \"\"\"Analyze a document and return structured insights\"\"\"\n\n    # 1. Create job\n    job = requests.post(\n        f\"{API_URL}/jobs/create\",\n        headers=headers,\n        json={\n            \"team_id\": team_id,\n            \"job_type\": \"document_analysis\",\n            \"metadata\": {\"document_length\": len(document_text)}\n        }\n    ).json()\n    job_id = job[\"job_id\"]\n\n    try:\n        # 2. Extract key points\n        extraction = requests.post(\n            f\"{API_URL}/jobs/{job_id}/llm-call\",\n            headers=headers,\n            json={\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Extract key points from documents as bullet points.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Extract key points from:\\n\\n{document_text}\"\n                    }\n                ],\n                \"temperature\": 0.3,\n                \"max_tokens\": 500\n            }\n        ).json()\n\n        key_points = extraction[\"response\"][\"content\"]\n\n        # 3. Generate summary\n        summary = requests.post(\n            f\"{API_URL}/jobs/{job_id}/llm-call\",\n            headers=headers,\n            json={\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Create concise summaries of documents.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Summarize this in 2-3 sentences:\\n\\n{document_text}\"\n                    }\n                ],\n                \"temperature\": 0.5,\n                \"max_tokens\": 200\n            }\n        ).json()\n\n        summary_text = summary[\"response\"][\"content\"]\n\n        # 4. Complete job\n        result = requests.post(\n            f\"{API_URL}/jobs/{job_id}/complete\",\n            headers=headers,\n            json={\n                \"status\": \"completed\",\n                \"metadata\": {\"success\": True}\n            }\n        ).json()\n\n        return {\n            \"job_id\": job_id,\n            \"key_points\": key_points,\n            \"summary\": summary_text,\n            \"credits_remaining\": result[\"costs\"][\"credits_remaining\"]\n        }\n\n    except Exception as e:\n        # Mark job as failed\n        requests.post(\n            f\"{API_URL}/jobs/{job_id}/complete\",\n            headers=headers,\n            json={\n                \"status\": \"failed\",\n                \"metadata\": {\"error\": str(e)}\n            }\n        )\n        raise\n\n# Usage\ndocument = \"\"\"\nArtificial intelligence (AI) is transforming industries...\n[long document text]\n\"\"\"\n\nresult = analyze_document(\"acme-corp\", document)\nprint(f\"Summary: {result['summary']}\")\nprint(f\"Key Points:\\n{result['key_points']}\")\nprint(f\"Credits remaining: {result['credits_remaining']}\")\n</code></pre>"},{"location":"integration/non-streaming/#next-steps","title":"Next Steps","text":"<p>Now that you understand non-streaming calls:</p> <ol> <li>Learn Streaming Calls - Real-time responses</li> <li>Try Structured Outputs - Type-safe Pydantic models</li> <li>See Full Examples - Working code</li> <li>Error Handling - Comprehensive error handling</li> </ol>"},{"location":"integration/non-streaming/#additional-resources","title":"Additional Resources","text":"<ul> <li>Job Workflow Guide - Understanding jobs</li> <li>Best Practices - Optimization tips</li> <li>API Reference - Complete API docs</li> </ul>"},{"location":"integration/overview/","title":"Integration Overview","text":"<p>Learn how to integrate the SaaS LiteLLM API into your application. This guide covers integration patterns, authentication, and the job-based workflow.</p> <p>Built on LiteLLM</p> <p>SaaS LiteLLM is built on top of LiteLLM, which provides unified access to 100+ LLM providers (OpenAI, Anthropic, Google, Azure, etc.). The SaaS API layer adds job-based cost tracking and multi-tenancy on top of LiteLLM's routing capabilities.</p>"},{"location":"integration/overview/#integration-patterns","title":"Integration Patterns","text":"<p>There are two ways to integrate with the SaaS LiteLLM API:</p>"},{"location":"integration/overview/#pattern-a-job-based-api-recommended","title":"Pattern A: Job-Based API (Recommended) \ud83c\udf1f","text":"<p>Best for: Multi-step workflows, cost tracking, simplified billing</p> <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\nheaders = {\"Authorization\": \"Bearer sk-your-virtual-key\"}\n\n# 1. Create job\njob = requests.post(f\"{API}/jobs/create\", headers=headers, json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"document_analysis\"\n}).json()\n\n# 2. Make LLM calls\nresponse = requests.post(f\"{API}/jobs/{job['job_id']}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this...\"}]\n    }\n).json()\n\n# 3. Complete job\nresult = requests.post(f\"{API}/jobs/{job['job_id']}/complete\",\n    headers=headers,\n    json={\"status\": \"completed\"}\n).json()\n\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre> <p>Key Features: - \u2705 Job-based tracking - Group related LLM calls - \u2705 Simple credit model - 1 credit per completed job - \u2705 Cost aggregation - Track total costs per workflow - \u2705 Model abstraction - Use semantic names (e.g., \"ChatAgent\") - \u2705 Automatic fallbacks - Handled by the API - \u2705 Usage analytics - Track which workflows are expensive</p> <p> Learn more about the job workflow</p>"},{"location":"integration/overview/#pattern-b-streaming-with-sse","title":"Pattern B: Streaming with SSE","text":"<p>Best for: Real-time responses, chat applications, progressive output</p> <pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\nheaders = {\"Authorization\": \"Bearer sk-your-virtual-key\"}\n\n# Create job and make streaming call\nresponse = requests.post(\n    f\"{API}/jobs/{job_id}/llm-call-stream\",\n    headers=headers,\n    json={\n        \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n    },\n    stream=True\n)\n\n# Process Server-Sent Events\nfor line in response.iter_lines():\n    if line.startswith(b'data: '):\n        chunk = json.loads(line[6:])\n        if chunk.get(\"content\"):\n            print(chunk[\"content\"], end=\"\", flush=True)\n</code></pre> <p>Key Features: - \u2705 Server-Sent Events (SSE) - Standard streaming protocol - \u2705 Zero buffering - Immediate chunk forwarding - \u2705 Full job tracking - Same credit model as non-streaming - \u2705 Type-safe - Compatible with structured outputs</p> <p> Learn more about streaming</p>"},{"location":"integration/overview/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[Your Application] --&gt;|HTTP/JSON| B[SaaS API :8003]\n    B --&gt;|Virtual Key| C[LiteLLM Proxy :8002]\n    C --&gt; D[OpenAI]\n    C --&gt; E[Anthropic]\n    C --&gt; F[Other Providers]\n\n    B -.Job Tracking.-&gt; G[PostgreSQL]\n    B -.Credits.-&gt; G\n\n    style B fill:#4CAF50\n    style C fill:#2196F3\n    style G fill:#FF9800</code></pre>"},{"location":"integration/overview/#component-breakdown","title":"Component Breakdown","text":"<ul> <li>Your Application - Makes API calls with virtual keys</li> <li>SaaS API (Port 8003) - Job-based wrapper, handles tracking and billing</li> <li>LiteLLM Proxy (Port 8002) - Routes to providers, handles caching and rate limiting</li> <li>PostgreSQL - Stores jobs, calls, teams, and usage data</li> </ul>"},{"location":"integration/overview/#authentication","title":"Authentication","text":"<p>All API endpoints require authentication using your team's virtual API key.</p>"},{"location":"integration/overview/#getting-your-virtual-key","title":"Getting Your Virtual Key","text":"<p>When you create a team (via the admin dashboard or API), you receive a virtual key:</p> <pre><code>curl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-corp\",\n    \"team_alias\": \"ACME Corp\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"team_id\": \"acme-corp\",\n  \"virtual_key\": \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"credits_allocated\": 1000,\n  \"credits_remaining\": 1000\n}\n</code></pre></p> <p>Keep your virtual key secure</p> <ul> <li>Store securely (environment variables, secrets manager)</li> <li>Never commit to version control</li> <li>Rotate keys periodically</li> <li>Use different keys for different environments</li> </ul>"},{"location":"integration/overview/#using-the-virtual-key","title":"Using the Virtual Key","text":"<p>Include the virtual key in the <code>Authorization</code> header:</p> PythonJavaScriptcURL <pre><code>headers = {\n    \"Authorization\": \"Bearer sk-your-virtual-key-here\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    \"http://localhost:8003/api/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"analysis\"}\n)\n</code></pre> <pre><code>const headers = {\n  'Authorization': 'Bearer sk-your-virtual-key-here',\n  'Content-Type': 'application/json'\n};\n\nconst response = await fetch('http://localhost:8003/api/jobs/create', {\n  method: 'POST',\n  headers: headers,\n  body: JSON.stringify({\n    team_id: 'acme-corp',\n    job_type: 'analysis'\n  })\n});\n</code></pre> <pre><code>curl -X POST http://localhost:8003/api/jobs/create \\\n  -H \"Authorization: Bearer sk-your-virtual-key-here\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"team_id\":\"acme-corp\",\"job_type\":\"analysis\"}'\n</code></pre> <p> Learn more about authentication</p>"},{"location":"integration/overview/#job-based-workflow","title":"Job-Based Workflow","text":"<p>The job-based workflow groups multiple LLM calls into a single business operation:</p>"},{"location":"integration/overview/#why-jobs","title":"Why Jobs?","text":"<p>Traditional LLM APIs charge per API call. With jobs:</p> <ul> <li>Simplified Billing - 1 credit per completed job (not per call)</li> <li>Cost Aggregation - Track total cost across multiple calls</li> <li>Business Context - Track operations, not just API calls</li> <li>Failure Handling - Failed jobs don't consume credits</li> </ul>"},{"location":"integration/overview/#job-lifecycle","title":"Job Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Pending: Create Job\n    Pending --&gt; InProgress: First LLM Call\n    InProgress --&gt; InProgress: Additional Calls\n    InProgress --&gt; Completed: Complete (success)\n    InProgress --&gt; Failed: Complete (failure)\n    Completed --&gt; [*]: 1 credit deducted\n    Failed --&gt; [*]: No credit deducted</code></pre>"},{"location":"integration/overview/#real-world-example-document-analysis","title":"Real-World Example: Document Analysis","text":"<pre><code># 1 Job = Multiple LLM Calls = 1 Credit\njob_id = create_job(\"document_analysis\")\n\n# Make multiple LLM calls\nextract_text(job_id)      # Call 1\nclassify_content(job_id)  # Call 2\ngenerate_summary(job_id)  # Call 3\n\n# Complete job\ncomplete_job(job_id, \"completed\")\n# Result: 1 credit deducted (not 3!)\n</code></pre> <p> Learn more about the job workflow</p>"},{"location":"integration/overview/#streaming-vs-non-streaming","title":"Streaming vs Non-Streaming","text":""},{"location":"integration/overview/#non-streaming-buffered","title":"Non-Streaming (Buffered)","text":"<p>Best for: Batch processing, simple requests, structured outputs</p> <pre><code># Get complete response at once\nresponse = requests.post(\n    f\"{API}/jobs/{job_id}/llm-call\",\n    headers=headers,\n    json={\"messages\": messages}\n).json()\n\ncontent = response[\"response\"][\"content\"]\n</code></pre> <p>Characteristics: - \u2705 Simpler to implement - \u2705 Complete response at once - \u2705 Easier error handling - \u274c Higher latency (wait for full response) - \u274c No progressive display</p> <p> Learn more about non-streaming</p>"},{"location":"integration/overview/#streaming-server-sent-events","title":"Streaming (Server-Sent Events)","text":"<p>Best for: Chat applications, real-time responses, long outputs</p> <pre><code># Get response progressively\nresponse = requests.post(\n    f\"{API}/jobs/{job_id}/llm-call-stream\",\n    headers=headers,\n    json={\"messages\": messages},\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line.startswith(b'data: '):\n        chunk = json.loads(line[6:])\n        print(chunk.get(\"content\", \"\"), end=\"\")\n</code></pre> <p>Characteristics: - \u2705 Lower perceived latency - \u2705 Progressive display - \u2705 Better user experience - \u2705 Same credit model as non-streaming - \u26a0\ufe0f More complex to implement</p> <p> Learn more about streaming</p>"},{"location":"integration/overview/#client-libraries","title":"Client Libraries","text":""},{"location":"integration/overview/#type-safe-python-client","title":"Type-Safe Python Client","text":"<p>We provide a type-safe Python client with full type hints and Pydantic validation:</p> <pre><code>from saas_litellm_client import SaasLiteLLMClient\n\nasync with SaasLiteLLMClient(\n    base_url=\"http://localhost:8003\",\n    team_id=\"acme-corp\",\n    virtual_key=\"sk-your-key\"\n) as client:\n    # Create job\n    job = await client.create_job(\"document_analysis\")\n\n    # Make LLM call\n    response = await client.llm_call(\n        job_id=job.job_id,\n        messages=[{\"role\": \"user\", \"content\": \"Analyze...\"}]\n    )\n\n    # Complete job\n    result = await client.complete_job(job.job_id, \"completed\")\n    print(f\"Credits remaining: {result.credits_remaining}\")\n</code></pre> <p>Features: - \u2705 Full type hints - \u2705 Pydantic validation - \u2705 Async/await support - \u2705 Automatic error handling - \u2705 Context manager support</p> <p> Learn more about the typed client</p>"},{"location":"integration/overview/#structured-outputs","title":"Structured Outputs","text":"<p>Use Pydantic models for type-safe responses:</p> <pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n\n# Get structured response\nperson = await client.structured_call(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Extract person info...\"}],\n    response_model=Person\n)\n\nprint(f\"Name: {person.name}, Age: {person.age}\")\n</code></pre> <p> Learn more about structured outputs</p>"},{"location":"integration/overview/#quick-start","title":"Quick Start","text":""},{"location":"integration/overview/#step-1-set-up-organization-and-team","title":"Step 1: Set Up Organization and Team","text":"<pre><code># Create organization\ncurl -X POST http://localhost:8003/api/organizations/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"name\": \"ACME Corporation\"\n  }'\n\n# Create model access group\ncurl -X POST http://localhost:8003/api/model-access-groups/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"group_name\": \"gpt-models\",\n    \"description\": \"GPT models\"\n  }'\n\n# Create team\ncurl -X POST http://localhost:8003/api/teams/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"organization_id\": \"org_acme\",\n    \"team_id\": \"acme-corp\",\n    \"team_alias\": \"ACME Corp Team\",\n    \"access_groups\": [\"gpt-models\"],\n    \"credits_allocated\": 1000\n  }'\n</code></pre> <p>Use the Admin Dashboard</p> <p>The easiest way to set up organizations and teams is through the Admin Dashboard at http://localhost:3002</p>"},{"location":"integration/overview/#step-2-make-your-first-api-call","title":"Step 2: Make Your First API Call","text":"<pre><code>import requests\n\nAPI = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Create job\njob = requests.post(f\"{API}/jobs/create\", headers=headers, json={\n    \"team_id\": \"acme-corp\",\n    \"job_type\": \"test\",\n    \"metadata\": {\"test\": True}\n}).json()\n\nprint(f\"Created job: {job['job_id']}\")\n\n# Make LLM call\nresponse = requests.post(\n    f\"{API}/jobs/{job['job_id']}/llm-call\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Say hello!\"}\n        ]\n    }\n).json()\n\nprint(f\"Response: {response['response']['content']}\")\n\n# Complete job\nresult = requests.post(\n    f\"{API}/jobs/{job['job_id']}/complete\",\n    headers=headers,\n    json={\"status\": \"completed\"}\n).json()\n\nprint(f\"Credits remaining: {result['costs']['credits_remaining']}\")\n</code></pre>"},{"location":"integration/overview/#integration-guides","title":"Integration Guides","text":"<p>Ready to integrate? Check out these detailed guides:</p> <ul> <li> <p>Job Workflow</p> <p>Learn the job-based workflow: create, call, complete</p> </li> <li> <p>Authentication</p> <p>Virtual keys, security best practices, error handling</p> </li> <li> <p>Non-Streaming Calls</p> <p>Standard LLM calls with complete responses</p> </li> <li> <p>Streaming Calls</p> <p>Server-Sent Events (SSE) for real-time responses</p> </li> <li> <p>Typed Client</p> <p>Type-safe Python client with async support</p> </li> <li> <p>Structured Outputs</p> <p>Pydantic models for type-safe responses</p> </li> <li> <p>Error Handling</p> <p>Common errors, retry strategies, best practices</p> </li> <li> <p>Best Practices</p> <p>Performance, security, cost optimization</p> </li> </ul>"},{"location":"integration/overview/#code-examples","title":"Code Examples","text":"<p>Want to see working code? Check out these examples:</p> <ul> <li>Basic Usage - Simple job creation and LLM call</li> <li>Streaming Examples - Real-time streaming responses</li> <li>Structured Outputs - Type-safe Pydantic models</li> <li>Full Chain Example - UI \u2192 Client \u2192 SaaS API streaming chain</li> </ul>"},{"location":"integration/overview/#api-reference","title":"API Reference","text":"<p>Need detailed API documentation?</p> <ul> <li>Interactive API Docs (Swagger) - Try the API in your browser</li> <li>ReDoc - Beautiful API documentation</li> <li>API Reference - Complete endpoint documentation</li> </ul>"},{"location":"integration/overview/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the Error Handling Guide for common errors</li> <li>Review the Troubleshooting Guide</li> <li>See Working Examples</li> </ul>"},{"location":"integration/overview/#next-steps","title":"Next Steps","text":"<ol> <li>Understand the Job Workflow - Core concept for cost tracking</li> <li>Set Up Authentication - Get your virtual key and secure it</li> <li>Try the Examples - Run working code</li> <li>Explore Streaming - Add real-time responses to your app</li> </ol>"},{"location":"integration/streaming/","title":"Streaming LLM Calls","text":"<p>Learn how to use Server-Sent Events (SSE) for real-time streaming responses from LLMs.</p>"},{"location":"integration/streaming/#overview","title":"Overview","text":"<p>Streaming allows you to receive LLM responses progressively as they're generated, rather than waiting for the complete response. This creates a better user experience with lower perceived latency.</p> <p>Best for: - Chat applications and conversational interfaces - Real-time content generation - Long-form responses (articles, code, etc.) - Interactive user experiences - Progressive display requirements</p> <p>Characteristics: - \u2705 Lower perceived latency (TTFT ~300-500ms vs ~2000ms) - \u2705 Progressive display of responses - \u2705 Better user experience - \u2705 Same credit model as non-streaming - \u2705 Same job-based tracking - \u26a0\ufe0f More complex implementation - \u26a0\ufe0f Requires SSE handling</p>"},{"location":"integration/streaming/#how-streaming-works","title":"How Streaming Works","text":""},{"location":"integration/streaming/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":"<p>Streaming uses the Server-Sent Events (SSE) protocol, a standard for server-to-client streaming over HTTP:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: text/event-stream\nCache-Control: no-cache\nX-Accel-Buffering: no\n\ndata: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\"!\"}}]}\n\ndata: [DONE]\n</code></pre> <p>Key Points: - Each chunk is prefixed with <code>data:</code> - Chunks arrive as they're generated - Stream ends with <code>data: [DONE]</code> - Zero buffering - minimal latency</p>"},{"location":"integration/streaming/#streaming-architecture","title":"Streaming Architecture","text":"<pre><code>sequenceDiagram\n    participant Client as Your App\n    participant API as SaaS API :8003\n    participant LLM as LiteLLM :8002\n    participant Provider as OpenAI/Anthropic\n    participant DB as PostgreSQL\n\n    Client-&gt;&gt;API: POST /api/jobs/{job_id}/llm-call-stream\n    API-&gt;&gt;DB: Update job status to \"in_progress\"\n    API-&gt;&gt;LLM: Forward request (streaming=true)\n    LLM-&gt;&gt;Provider: Make streaming API call\n\n    loop Each token generated\n        Provider--&gt;&gt;LLM: Send token chunk\n        LLM--&gt;&gt;API: Forward chunk (zero buffering)\n        API--&gt;&gt;Client: Forward chunk (SSE format)\n        Note over API: Accumulate content for DB\n    end\n\n    Provider--&gt;&gt;LLM: Stream complete\n    LLM--&gt;&gt;API: Final chunk + metadata\n    API-&gt;&gt;DB: Record llm_call with full content\n    API--&gt;&gt;Client: data: [DONE]</code></pre> <p>Performance: - Time to First Token (TTFT): ~300-500ms - Per-token latency: ~50ms - Overhead per hop: ~10-50ms (minimal!)</p>"},{"location":"integration/streaming/#basic-streaming-call","title":"Basic Streaming Call","text":""},{"location":"integration/streaming/#using-raw-http","title":"Using Raw HTTP","text":"<pre><code>import requests\nimport json\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 1. Create job\njob = requests.post(\n    f\"{API_URL}/jobs/create\",\n    headers=headers,\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"chat\"}\n).json()\n\njob_id = job[\"job_id\"]\n\n# 2. Make streaming call\nresponse = requests.post(\n    f\"{API_URL}/jobs/{job_id}/llm-call-stream\",\n    headers=headers,\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Tell me a story\"}\n        ]\n    },\n    stream=True  # Important: enable streaming\n)\n\n# 3. Process Server-Sent Events\naccumulated = \"\"\nfor line in response.iter_lines():\n    if line:\n        line = line.decode('utf-8')\n        if line.startswith('data: '):\n            data_str = line[6:]  # Remove 'data: ' prefix\n\n            if data_str == '[DONE]':\n                print(\"\\n\\nStream complete!\")\n                break\n\n            try:\n                chunk = json.loads(data_str)\n                if chunk.get(\"choices\"):\n                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    if content:\n                        accumulated += content\n                        print(content, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                continue\n\n# 4. Complete job\nresult = requests.post(\n    f\"{API_URL}/jobs/{job_id}/complete\",\n    headers=headers,\n    json={\"status\": \"completed\"}\n).json()\n\nprint(f\"\\nCredits remaining: {result['costs']['credits_remaining']}\")\n</code></pre>"},{"location":"integration/streaming/#single-call-streaming-easiest-option","title":"Single-Call Streaming (Easiest Option)","text":"<p>For applications that only need to make one LLM call per job (like simple chat messages), use the single-call streaming endpoint that combines all three steps into one request:</p> <p><code>POST /api/jobs/create-and-call-stream</code></p> <p>This endpoint: - Creates the job - Streams the LLM response via SSE - Automatically completes the job and deducts credits</p> <p>Perfect for chat applications that need real-time streaming without the complexity of managing job lifecycle.</p>"},{"location":"integration/streaming/#using-single-call-streaming","title":"Using Single-Call Streaming","text":"<pre><code>import requests\nimport json\n\nAPI_URL = \"http://localhost:8003/api\"\nVIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Single request streams the entire response\nresponse = requests.post(\n    f\"{API_URL}/jobs/create-and-call-stream\",\n    headers=headers,\n    json={\n        \"team_id\": \"acme-corp\",\n        \"job_type\": \"chat\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Tell me a short story\"}\n        ]\n    },\n    stream=True  # Important: enable streaming\n)\n\n# Process Server-Sent Events\naccumulated = \"\"\nfor line in response.iter_lines():\n    if line:\n        line = line.decode('utf-8')\n        if line.startswith('data: '):\n            data_str = line[6:]  # Remove 'data: ' prefix\n\n            if data_str == '[DONE]':\n                print(\"\\n\\nStream complete!\")\n                break\n\n            try:\n                chunk = json.loads(data_str)\n                if chunk.get(\"choices\"):\n                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    if content:\n                        accumulated += content\n                        print(content, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                continue\n\nprint(f\"\\nFull response: {accumulated}\")\n</code></pre>"},{"location":"integration/streaming/#single-call-vs-multi-step-streaming","title":"Single-Call vs Multi-Step Streaming","text":"Feature Single-Call Streaming Multi-Step Streaming Endpoints 1 request 3 requests (create \u2192 stream \u2192 complete) Use Case Simple chat, one LLM call per job Complex agents, multiple LLM calls Latency Lowest (~300ms TTFT) Slightly higher (~350ms TTFT) Credit Deduction Automatic after stream Manual via complete endpoint Job Control Automatic Full control Best For Chat apps, simple queries Multi-step workflows, agents <p>When to use Single-Call Streaming: - \u2705 Chat applications with one message per job - \u2705 Simple Q&amp;A interfaces - \u2705 You want the simplest possible integration - \u2705 Each user message = one job</p> <p>When to use Multi-Step Streaming: - \u2705 Agent workflows with multiple LLM calls - \u2705 Complex multi-step processes - \u2705 You need full control over job lifecycle - \u2705 Multiple LLM calls per job</p>"},{"location":"integration/streaming/#single-call-with-async","title":"Single-Call with Async","text":"<pre><code>import httpx\nimport asyncio\nimport json\n\nasync def streaming_chat():\n    API_URL = \"http://localhost:8003/api\"\n    VIRTUAL_KEY = \"sk-your-virtual-key-here\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {VIRTUAL_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\n            \"POST\",\n            f\"{API_URL}/jobs/create-and-call-stream\",\n            headers=headers,\n            json={\n                \"team_id\": \"acme-corp\",\n                \"job_type\": \"chat\",\n                \"model\": \"gpt-4\",\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Write a haiku about Python\"}\n                ],\n                \"temperature\": 0.7\n            },\n            timeout=30.0\n        ) as response:\n            accumulated = \"\"\n            async for line in response.aiter_lines():\n                if line.startswith(\"data: \"):\n                    data_str = line[6:]\n\n                    if data_str == \"[DONE]\":\n                        break\n\n                    try:\n                        chunk = json.loads(data_str)\n                        if chunk.get(\"choices\"):\n                            content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\", \"\")\n                            if content:\n                                accumulated += content\n                                print(content, end=\"\", flush=True)\n                    except json.JSONDecodeError:\n                        continue\n\n            print(f\"\\n\\nComplete response: {accumulated}\")\n\nasyncio.run(streaming_chat())\n</code></pre>"},{"location":"integration/streaming/#sse-chunk-format","title":"SSE Chunk Format","text":"<p>Each streaming chunk follows this format:</p> <pre><code>{\n  \"id\": \"chatcmpl-xyz\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1697896000,\n  \"model\": \"gpt-4\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello\"\n      },\n      \"finish_reason\": null\n    }\n  ]\n}\n</code></pre> <p>Fields: - <code>delta.role</code> - Present in first chunk only (\"assistant\") - <code>delta.content</code> - The text chunk being generated - <code>finish_reason</code> - <code>null</code> during streaming, \"stop\"/\"length\" at end</p>"},{"location":"integration/streaming/#using-the-type-safe-client","title":"Using the Type-Safe Client","text":"<p>The Python typed client provides a much easier interface:</p> <pre><code>from examples.typed_client import SaaSLLMClient\n\nasync def streaming_example():\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key\"\n    ) as client:\n        # Create job\n        job_id = await client.create_job(\"chat\")\n\n        # Stream response\n        accumulated = \"\"\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"Write a poem about Python\"}\n            ]\n        ):\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n                content = delta.get(\"content\", \"\")\n                if content:\n                    accumulated += content\n                    print(content, end=\"\", flush=True)\n\n        print(f\"\\n\\nFull response: {accumulated}\")\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"Credits remaining: {result.credits_remaining}\")\n\n# Run\nimport asyncio\nasyncio.run(streaming_example())\n</code></pre> <p> Learn more about the typed client</p>"},{"location":"integration/streaming/#multi-turn-streaming-conversations","title":"Multi-Turn Streaming Conversations","text":"<p>Maintain conversation context across streaming calls:</p> <pre><code>async def streaming_conversation():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"chat_session\")\n\n        messages = []\n\n        # Turn 1\n        messages.append({\"role\": \"user\", \"content\": \"What is Python?\"})\n        response1 = \"\"\n        async for chunk in client.chat_stream(job_id, messages):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                response1 += content\n                print(content, end=\"\", flush=True)\n\n        # Add assistant response to history\n        messages.append({\"role\": \"assistant\", \"content\": response1})\n\n        # Turn 2\n        print(\"\\n\\n--- Turn 2 ---\")\n        messages.append({\"role\": \"user\", \"content\": \"What are its main uses?\"})\n        response2 = \"\"\n        async for chunk in client.chat_stream(job_id, messages):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                response2 += content\n                print(content, end=\"\", flush=True)\n\n        # Complete job\n        await client.complete_job(job_id, \"completed\")\n\nasyncio.run(streaming_conversation())\n</code></pre>"},{"location":"integration/streaming/#streaming-with-system-prompts","title":"Streaming with System Prompts","text":"<p>Set the assistant's behavior with system messages:</p> <pre><code>async def streaming_with_system_prompt():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"tutoring\")\n\n        async for chunk in client.chat_stream(\n            job_id=job_id,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a patient Python tutor. Explain concepts simply and provide code examples.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"How do I read a CSV file?\"\n                }\n            ],\n            temperature=0.7\n        ):\n            if chunk.choices:\n                content = chunk.choices[0].delta.get(\"content\", \"\")\n                print(content, end=\"\", flush=True)\n\n        await client.complete_job(job_id, \"completed\")\n\nasyncio.run(streaming_with_system_prompt())\n</code></pre>"},{"location":"integration/streaming/#controlling-stream-parameters","title":"Controlling Stream Parameters","text":""},{"location":"integration/streaming/#temperature","title":"Temperature","text":"<pre><code># Creative streaming (higher temperature)\nasync for chunk in client.chat_stream(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Write a creative story\"}],\n    temperature=1.2  # More creative\n):\n    # Process chunks...\n\n# Factual streaming (lower temperature)\nasync for chunk in client.chat_stream(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Explain photosynthesis\"}],\n    temperature=0.3  # More deterministic\n):\n    # Process chunks...\n</code></pre>"},{"location":"integration/streaming/#max-tokens","title":"Max Tokens","text":"<pre><code># Limit response length\nasync for chunk in client.chat_stream(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Summarize Python in one paragraph\"}],\n    max_tokens=150  # Short response\n):\n    # Process chunks...\n</code></pre>"},{"location":"integration/streaming/#stop-sequences","title":"Stop Sequences","text":"<pre><code># Stop at specific text\nasync for chunk in client.chat_stream(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"List 5 programming languages:\\n1.\"}],\n    stop=[\"\\n6.\", \"END\"]  # Stop at 6th item or \"END\"\n):\n    # Process chunks...\n</code></pre>"},{"location":"integration/streaming/#error-handling","title":"Error Handling","text":""},{"location":"integration/streaming/#handling-stream-interruptions","title":"Handling Stream Interruptions","text":"<pre><code>async def robust_streaming():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"chat\")\n\n        try:\n            accumulated = \"\"\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n            ):\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    accumulated += content\n                    print(content, end=\"\", flush=True)\n\n            # Stream completed successfully\n            await client.complete_job(job_id, \"completed\")\n\n        except Exception as e:\n            print(f\"\\nStream error: {e}\")\n            # Mark job as failed\n            await client.complete_job(job_id, \"failed\")\n            raise\n\nasyncio.run(robust_streaming())\n</code></pre>"},{"location":"integration/streaming/#timeout-handling","title":"Timeout Handling","text":"<pre><code>import asyncio\n\nasync def streaming_with_timeout():\n    async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n        job_id = await client.create_job(\"chat\")\n\n        try:\n            # Set a timeout for the entire stream\n            async with asyncio.timeout(30):  # 30 second timeout\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n                ):\n                    # Process chunks...\n                    pass\n\n        except asyncio.TimeoutError:\n            print(\"Stream timed out\")\n            await client.complete_job(job_id, \"failed\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n            await client.complete_job(job_id, \"failed\")\n\nasyncio.run(streaming_with_timeout())\n</code></pre>"},{"location":"integration/streaming/#retry-logic","title":"Retry Logic","text":"<pre><code>async def streaming_with_retry(max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n                job_id = await client.create_job(\"chat\")\n\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n                ):\n                    # Process chunks...\n                    pass\n\n                await client.complete_job(job_id, \"completed\")\n                return  # Success\n\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise  # Final attempt failed\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\nasyncio.run(streaming_with_retry())\n</code></pre>"},{"location":"integration/streaming/#full-streaming-chain-example","title":"Full Streaming Chain Example","text":"<p>Build a complete streaming chain from browser \u2192 your API \u2192 SaaS API \u2192 LLM:</p>"},{"location":"integration/streaming/#your-client-api-fastapi","title":"Your Client API (FastAPI)","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport httpx\n\napp = FastAPI()\n\n@app.post(\"/chat\")\nasync def chat_endpoint(request: dict):\n    \"\"\"Forward streaming from SaaS API to browser\"\"\"\n\n    # Create job in SaaS API\n    async with httpx.AsyncClient() as client:\n        job_response = await client.post(\n            \"http://localhost:8003/api/jobs/create\",\n            headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n            json={\"team_id\": \"acme-corp\", \"job_type\": \"chat\"}\n        )\n        job_id = job_response.json()[\"job_id\"]\n\n    async def stream_generator():\n        \"\"\"Forward chunks from SaaS API\"\"\"\n        async with httpx.AsyncClient(timeout=30.0) as client:\n            async with client.stream(\n                \"POST\",\n                f\"http://localhost:8003/api/jobs/{job_id}/llm-call-stream\",\n                headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n                json=request\n            ) as response:\n                async for line in response.aiter_lines():\n                    if line.startswith(\"data: \"):\n                        yield f\"{line}\\n\\n\"\n\n    return StreamingResponse(\n        stream_generator(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n</code></pre>"},{"location":"integration/streaming/#browser-client-javascript","title":"Browser Client (JavaScript)","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Streaming Chat&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div id=\"output\"&gt;&lt;/div&gt;\n    &lt;input id=\"input\" type=\"text\" placeholder=\"Type a message...\"&gt;\n    &lt;button onclick=\"sendMessage()\"&gt;Send&lt;/button&gt;\n\n    &lt;script&gt;\n        async function sendMessage() {\n            const input = document.getElementById('input');\n            const output = document.getElementById('output');\n            const message = input.value;\n            input.value = '';\n\n            // Create message div\n            const messageDiv = document.createElement('div');\n            output.appendChild(messageDiv);\n\n            // Stream response\n            const response = await fetch('http://localhost:8001/chat', {\n                method: 'POST',\n                headers: {'Content-Type': 'application/json'},\n                body: JSON.stringify({\n                    messages: [{role: 'user', content: message}]\n                })\n            });\n\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n\n            while (true) {\n                const {done, value} = await reader.read();\n                if (done) break;\n\n                const text = decoder.decode(value);\n                const lines = text.split('\\n');\n\n                for (const line of lines) {\n                    if (line.startsWith('data: ')) {\n                        const data = line.substring(6);\n                        if (data === '[DONE]') continue;\n\n                        try {\n                            const chunk = JSON.parse(data);\n                            const content = chunk.choices?.[0]?.delta?.content || '';\n                            messageDiv.textContent += content;\n                        } catch (e) {\n                            // Ignore parse errors\n                        }\n                    }\n                }\n            }\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p> See complete streaming chain example</p>"},{"location":"integration/streaming/#best-practices","title":"Best Practices","text":""},{"location":"integration/streaming/#1-always-accumulate-content","title":"1. Always Accumulate Content","text":"<p>Keep track of the full response as you stream:</p> <pre><code>accumulated = \"\"\nasync for chunk in client.chat_stream(job_id, messages):\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        accumulated += content\n        print(content, end=\"\", flush=True)\n\n# Use accumulated for further processing\nprint(f\"\\n\\nFull response length: {len(accumulated)}\")\n</code></pre>"},{"location":"integration/streaming/#2-flush-output-for-real-time-display","title":"2. Flush Output for Real-Time Display","text":"<pre><code># \u2705 Good - Real-time display\nprint(content, end=\"\", flush=True)\n\n# \u274c Bad - Buffered output\nprint(content, end=\"\")\n</code></pre>"},{"location":"integration/streaming/#3-handle-empty-chunks","title":"3. Handle Empty Chunks","text":"<p>Not all chunks contain content:</p> <pre><code>async for chunk in client.chat_stream(job_id, messages):\n    if chunk.choices:\n        delta = chunk.choices[0].delta\n        # Check if content exists\n        if \"content\" in delta and delta[\"content\"]:\n            print(delta[\"content\"], end=\"\", flush=True)\n</code></pre>"},{"location":"integration/streaming/#4-monitor-finish-reason","title":"4. Monitor Finish Reason","text":"<pre><code>async for chunk in client.chat_stream(job_id, messages):\n    if chunk.choices:\n        choice = chunk.choices[0]\n\n        # Process content\n        if \"content\" in choice.delta:\n            print(choice.delta[\"content\"], end=\"\", flush=True)\n\n        # Check if stream is ending\n        if choice.finish_reason:\n            print(f\"\\n\\nFinished: {choice.finish_reason}\")\n            if choice.finish_reason == \"length\":\n                print(\"Warning: Response was truncated due to max_tokens\")\n</code></pre>"},{"location":"integration/streaming/#5-use-async-for-better-performance","title":"5. Use Async for Better Performance","text":"<pre><code># \u2705 Good - Async allows concurrent operations\nasync with SaaSLLMClient(...) as client:\n    async for chunk in client.chat_stream(...):\n        # Non-blocking\n\n# \u274c Avoid - Sync blocks the entire thread\nimport requests\nresponse = requests.post(..., stream=True)\nfor line in response.iter_lines():\n    # Blocks thread\n</code></pre>"},{"location":"integration/streaming/#6-set-appropriate-headers","title":"6. Set Appropriate Headers","text":"<p>If you're building your own streaming proxy:</p> <pre><code>headers = {\n    \"Content-Type\": \"text/event-stream\",\n    \"Cache-Control\": \"no-cache\",\n    \"X-Accel-Buffering\": \"no\",  # Prevent nginx buffering\n    \"Connection\": \"keep-alive\"\n}\n</code></pre>"},{"location":"integration/streaming/#streaming-vs-non-streaming-comparison","title":"Streaming vs Non-Streaming Comparison","text":"Feature Non-Streaming Streaming Latency (perceived) High (~2000ms TTFT) Low (~300-500ms TTFT) User Experience Wait for full response Progressive display Implementation Simpler More complex Use Case Batch processing Interactive apps Buffering Full response buffered Zero buffering Error Handling Simpler More complex Credits 1 per completed job 1 per completed job Cost Tracking Same Same"},{"location":"integration/streaming/#complete-example","title":"Complete Example","text":"<p>Here's a complete streaming chat application:</p> <pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def chat_session():\n    \"\"\"Interactive streaming chat session\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key\"\n    ) as client:\n\n        # Create job for this chat session\n        job_id = await client.create_job(\"chat_session\")\n        print(f\"Started chat session: {job_id}\")\n\n        messages = []\n\n        while True:\n            # Get user input\n            user_input = input(\"\\nYou: \")\n            if user_input.lower() in ['quit', 'exit', 'bye']:\n                break\n\n            # Add to messages\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            # Stream response\n            print(\"Assistant: \", end=\"\", flush=True)\n            assistant_response = \"\"\n\n            try:\n                async for chunk in client.chat_stream(\n                    job_id=job_id,\n                    messages=messages,\n                    temperature=0.7\n                ):\n                    if chunk.choices:\n                        content = chunk.choices[0].delta.get(\"content\", \"\")\n                        if content:\n                            assistant_response += content\n                            print(content, end=\"\", flush=True)\n\n                # Add assistant response to messages\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"\\nError: {e}\")\n                await client.complete_job(job_id, \"failed\")\n                return\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\n\\nSession ended. Credits remaining: {result.credits_remaining}\")\n\n# Run the chat\nasyncio.run(chat_session())\n</code></pre>"},{"location":"integration/streaming/#next-steps","title":"Next Steps","text":"<p>Now that you understand streaming:</p> <ol> <li>Try Structured Outputs - Type-safe responses with Pydantic</li> <li>See Full Examples - Working streaming examples</li> <li>Learn Error Handling - Handle streaming errors robustly</li> <li>Best Practices - Optimization and performance tips</li> </ol>"},{"location":"integration/streaming/#additional-resources","title":"Additional Resources","text":"<ul> <li>Typed Client Guide - Easier streaming with the Python client</li> <li>Job Workflow Guide - Understanding the job lifecycle</li> <li>API Reference - Complete streaming API docs</li> <li>Streaming Architecture - Deep dive into implementation</li> </ul>"},{"location":"integration/structured-outputs/","title":"Structured Outputs","text":"<p>Get type-safe, validated responses from LLMs using Pydantic models instead of raw text.</p>"},{"location":"integration/structured-outputs/#why-structured-outputs","title":"Why Structured Outputs?","text":"<p>Traditional Approach (Unreliable): <pre><code># Ask LLM to return JSON\nresponse = await client.chat(job_id, [{\n    \"role\": \"user\",\n    \"content\": \"Extract name and email from: John Doe, john@example.com. Return as JSON.\"\n}])\n\n# Hope the response is valid JSON\ntext = response.choices[0].message[\"content\"]\ndata = json.loads(text)  # \u274c Might fail if LLM returns invalid JSON\nname = data[\"name\"]       # \u274c Might fail if field missing\n</code></pre></p> <p>Structured Outputs (Reliable): <pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    email: str\n\n# Get validated Pydantic model\nperson = await client.structured_output(\n    job_id=job_id,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract: John Doe, john@example.com\"\n    }],\n    response_model=Person\n)\n\nprint(person.name)   # \u2705 Guaranteed to exist\nprint(person.email)  # \u2705 Guaranteed to be a string\n</code></pre></p> <p>Benefits: - \u2705 Type Safety: IDE autocomplete and type checking - \u2705 Validation: Automatic data validation via Pydantic - \u2705 Reliability: LLM is forced to match your schema - \u2705 No Parsing: No need to parse JSON manually - \u2705 Error Handling: Clear validation errors if data is malformed</p>"},{"location":"integration/structured-outputs/#how-it-works","title":"How It Works","text":"<pre><code>1. You define Pydantic model\n   \u2193\n2. Client converts model to JSON schema\n   \u2193\n3. Schema sent to LLM as response_format\n   \u2193\n4. LLM generates JSON matching schema\n   \u2193\n5. Client validates and returns Pydantic instance\n</code></pre> <p>Built on LiteLLM</p> <p>Structured outputs leverage LiteLLM's function calling capabilities, which work across OpenAI, Anthropic, Google, and other providers that support structured generation.</p>"},{"location":"integration/structured-outputs/#basic-example","title":"Basic Example","text":""},{"location":"integration/structured-outputs/#define-your-model","title":"Define Your Model","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass MovieReview(BaseModel):\n    title: str = Field(description=\"Movie title\")\n    rating: int = Field(ge=1, le=5, description=\"Rating from 1-5 stars\")\n    summary: str = Field(description=\"Brief review summary\")\n    recommended: bool = Field(description=\"Whether you recommend this movie\")\n</code></pre>"},{"location":"integration/structured-outputs/#extract-structured-data","title":"Extract Structured Data","text":"<pre><code>import asyncio\nfrom examples.typed_client import SaaSLLMClient\n\nasync def extract_review():\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"review_extraction\")\n\n        review_text = \"\"\"\n        I watched Inception last night. Amazing movie!\n        The plot was complex but engaging. Christopher Nolan\n        is a genius. I'd give it 5 stars and recommend it\n        to everyone who likes mind-bending thrillers.\n        \"\"\"\n\n        review = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract structured review from: {review_text}\"\n            }],\n            response_model=MovieReview\n        )\n\n        print(f\"Title: {review.title}\")              # Inception\n        print(f\"Rating: {review.rating}/5\")           # 5/5\n        print(f\"Summary: {review.summary}\")           # Complex but engaging...\n        print(f\"Recommended: {review.recommended}\")   # True\n\n        await client.complete_job(job_id, \"completed\")\n\nasyncio.run(extract_review())\n</code></pre>"},{"location":"integration/structured-outputs/#pydantic-model-features","title":"Pydantic Model Features","text":""},{"location":"integration/structured-outputs/#basic-types","title":"Basic Types","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str                    # String\n    age: int                     # Integer\n    height: float                # Float\n    is_active: bool              # Boolean\n    tags: list[str]              # List of strings\n    metadata: dict[str, str]     # Dictionary\n</code></pre>"},{"location":"integration/structured-outputs/#optional-fields","title":"Optional Fields","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Optional\n\nclass User(BaseModel):\n    username: str                      # Required\n    email: str                         # Required\n    phone: Optional[str] = None        # Optional, defaults to None\n    middle_name: str | None = None     # Alternative syntax (Python 3.10+)\n</code></pre>"},{"location":"integration/structured-outputs/#field-validation","title":"Field Validation","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str = Field(min_length=1, max_length=100)\n    price: float = Field(gt=0, description=\"Price must be positive\")\n    quantity: int = Field(ge=0, le=10000)\n    sku: str = Field(pattern=r\"^[A-Z]{3}-\\d{4}$\")  # Regex validation\n</code></pre>"},{"location":"integration/structured-outputs/#default-values","title":"Default Values","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass Settings(BaseModel):\n    theme: str = \"dark\"                              # Simple default\n    notifications: bool = Field(default=True)        # Field with default\n    max_retries: int = 3\n</code></pre>"},{"location":"integration/structured-outputs/#field-descriptions","title":"Field Descriptions","text":"<p>Descriptions help the LLM understand what to extract:</p> <pre><code>class Address(BaseModel):\n    street: str = Field(description=\"Street address including number\")\n    city: str = Field(description=\"City name\")\n    state: str = Field(description=\"2-letter state code (e.g., CA, NY)\")\n    zip_code: str = Field(description=\"5-digit ZIP code\")\n    country: str = Field(default=\"USA\", description=\"Country name\")\n</code></pre>"},{"location":"integration/structured-outputs/#nested-models","title":"Nested Models","text":"<pre><code>class Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Company(BaseModel):\n    name: str\n    industry: str\n\nclass Employee(BaseModel):\n    name: str\n    email: str\n    address: Address           # Nested model\n    employer: Company          # Nested model\n    skills: list[str]\n</code></pre>"},{"location":"integration/structured-outputs/#enums","title":"Enums","text":"<pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    URGENT = \"urgent\"\n\nclass Task(BaseModel):\n    title: str\n    priority: Priority         # Must be one of the enum values\n    assignee: str\n</code></pre>"},{"location":"integration/structured-outputs/#lists-and-complex-types","title":"Lists and Complex Types","text":"<pre><code>from pydantic import BaseModel\n\nclass Tag(BaseModel):\n    name: str\n    color: str\n\nclass BlogPost(BaseModel):\n    title: str\n    author: str\n    tags: list[Tag]                    # List of nested models\n    word_count: int\n    published: bool\n</code></pre>"},{"location":"integration/structured-outputs/#common-use-cases","title":"Common Use Cases","text":""},{"location":"integration/structured-outputs/#use-case-1-contact-information-extraction","title":"Use Case 1: Contact Information Extraction","text":"<pre><code>from pydantic import BaseModel, EmailStr, Field\n\nclass Contact(BaseModel):\n    name: str\n    email: EmailStr                     # Validates email format\n    phone: str = Field(pattern=r\"^\\+?1?\\d{10,15}$\")\n    company: str\n    job_title: str\n\nasync def extract_contact(text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"contact_extraction\")\n\n        contact = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract contact information: {text}\"\n            }],\n            response_model=Contact\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return contact\n\n# Usage\ntext = \"John Smith, CTO at TechCorp, john.smith@techcorp.com, +1-555-123-4567\"\ncontact = await extract_contact(text)\n</code></pre>"},{"location":"integration/structured-outputs/#use-case-2-resume-parsing","title":"Use Case 2: Resume Parsing","text":"<pre><code>from pydantic import BaseModel\n\nclass Education(BaseModel):\n    degree: str\n    institution: str\n    graduation_year: int\n\nclass WorkExperience(BaseModel):\n    title: str\n    company: str\n    start_date: str\n    end_date: str\n    responsibilities: list[str]\n\nclass Resume(BaseModel):\n    name: str\n    email: str\n    phone: str\n    summary: str\n    education: list[Education]\n    experience: list[WorkExperience]\n    skills: list[str]\n\nasync def parse_resume(resume_text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"resume_parsing\")\n\n        resume = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Parse this resume:\\n\\n{resume_text}\"\n            }],\n            response_model=Resume\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return resume\n</code></pre>"},{"location":"integration/structured-outputs/#use-case-3-sentiment-analysis","title":"Use Case 3: Sentiment Analysis","text":"<pre><code>from enum import Enum\nfrom pydantic import BaseModel, Field\n\nclass Sentiment(str, Enum):\n    VERY_NEGATIVE = \"very_negative\"\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n    VERY_POSITIVE = \"very_positive\"\n\nclass SentimentAnalysis(BaseModel):\n    sentiment: Sentiment\n    confidence: float = Field(ge=0.0, le=1.0)\n    key_phrases: list[str] = Field(description=\"Key phrases that influenced sentiment\")\n    summary: str = Field(description=\"Brief explanation of the sentiment\")\n\nasync def analyze_sentiment(text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"sentiment_analysis\")\n\n        analysis = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Analyze sentiment: {text}\"\n            }],\n            response_model=SentimentAnalysis\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return analysis\n</code></pre>"},{"location":"integration/structured-outputs/#use-case-4-data-classification","title":"Use Case 4: Data Classification","text":"<pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Category(str, Enum):\n    SPAM = \"spam\"\n    SUPPORT = \"support\"\n    SALES = \"sales\"\n    BILLING = \"billing\"\n    FEEDBACK = \"feedback\"\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\nclass EmailClassification(BaseModel):\n    category: Category\n    priority: Priority\n    requires_response: bool\n    suggested_department: str\n    key_topics: list[str]\n\nasync def classify_email(email_text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"email_classification\")\n\n        classification = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Classify this email:\\n\\n{email_text}\"\n            }],\n            response_model=EmailClassification\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return classification\n</code></pre>"},{"location":"integration/structured-outputs/#use-case-5-invoice-extraction","title":"Use Case 5: Invoice Extraction","text":"<pre><code>from pydantic import BaseModel, Field\nfrom datetime import date\n\nclass LineItem(BaseModel):\n    description: str\n    quantity: int = Field(ge=1)\n    unit_price: float = Field(gt=0)\n    total: float = Field(gt=0)\n\nclass Invoice(BaseModel):\n    invoice_number: str\n    invoice_date: str\n    due_date: str\n    vendor_name: str\n    vendor_address: str\n    customer_name: str\n    customer_address: str\n    line_items: list[LineItem]\n    subtotal: float\n    tax: float\n    total: float\n\nasync def extract_invoice(invoice_text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"invoice_extraction\")\n\n        invoice = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract invoice data:\\n\\n{invoice_text}\"\n            }],\n            response_model=Invoice\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return invoice\n</code></pre>"},{"location":"integration/structured-outputs/#error-handling","title":"Error Handling","text":""},{"location":"integration/structured-outputs/#validation-errors","title":"Validation Errors","text":"<pre><code>from pydantic import ValidationError\n\nasync def safe_extraction():\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"extraction\")\n\n        try:\n            person = await client.structured_output(\n                job_id=job_id,\n                messages=[{\"role\": \"user\", \"content\": \"...\"}],\n                response_model=Person\n            )\n\n            await client.complete_job(job_id, \"completed\")\n            return person\n\n        except ValidationError as e:\n            # Pydantic validation failed\n            print(f\"Validation error: {e}\")\n            await client.complete_job(job_id, \"failed\")\n            raise\n\n        except Exception as e:\n            # Other errors (API, network, etc.)\n            print(f\"Error: {e}\")\n            await client.complete_job(job_id, \"failed\")\n            raise\n</code></pre>"},{"location":"integration/structured-outputs/#handling-missing-data","title":"Handling Missing Data","text":"<p>Use optional fields for data that might not exist:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Optional\n\nclass PersonWithOptionals(BaseModel):\n    name: str                           # Required\n    email: str                          # Required\n    phone: Optional[str] = None         # Optional\n    address: Optional[str] = None       # Optional\n    company: Optional[str] = None       # Optional\n</code></pre>"},{"location":"integration/structured-outputs/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"integration/structured-outputs/#retry-on-validation-failure","title":"Retry on Validation Failure","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10)\n)\nasync def extract_with_retry(text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"extraction_with_retry\")\n\n        try:\n            result = await client.structured_output(\n                job_id=job_id,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": f\"Extract data: {text}\"\n                }],\n                response_model=Person\n            )\n\n            await client.complete_job(job_id, \"completed\")\n            return result\n\n        except Exception as e:\n            await client.complete_job(job_id, \"failed\")\n            raise\n</code></pre>"},{"location":"integration/structured-outputs/#batch-processing","title":"Batch Processing","text":"<pre><code>async def process_batch(documents: list[str]):\n    \"\"\"Process multiple documents concurrently\"\"\"\n\n    async with SaaSLLMClient(...) as client:\n\n        async def process_one(doc: str):\n            job_id = await client.create_job(\"batch_extraction\")\n\n            try:\n                result = await client.structured_output(\n                    job_id=job_id,\n                    messages=[{\"role\": \"user\", \"content\": f\"Extract: {doc}\"}],\n                    response_model=Person\n                )\n                await client.complete_job(job_id, \"completed\")\n                return result\n\n            except Exception as e:\n                await client.complete_job(job_id, \"failed\")\n                raise\n\n        # Process all documents concurrently\n        tasks = [process_one(doc) for doc in documents]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Separate successes and failures\n        successes = [r for r in results if not isinstance(r, Exception)]\n        failures = [r for r in results if isinstance(r, Exception)]\n\n        return successes, failures\n</code></pre>"},{"location":"integration/structured-outputs/#multiple-extraction-passes","title":"Multiple Extraction Passes","text":"<pre><code>class InitialExtraction(BaseModel):\n    raw_text: str\n    detected_entities: list[str]\n\nclass DetailedExtraction(BaseModel):\n    name: str\n    email: str\n    phone: str\n    company: str\n\nasync def two_pass_extraction(text: str):\n    async with SaaSLLMClient(...) as client:\n        job_id = await client.create_job(\"two_pass_extraction\")\n\n        # Pass 1: Identify what's in the text\n        initial = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Identify entities in: {text}\"\n            }],\n            response_model=InitialExtraction\n        )\n\n        # Pass 2: Extract detailed information\n        detailed = await client.structured_output(\n            job_id=job_id,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Extract contact details from: {text}\"\n            }],\n            response_model=DetailedExtraction\n        )\n\n        await client.complete_job(job_id, \"completed\")\n        return detailed\n</code></pre>"},{"location":"integration/structured-outputs/#model-compatibility","title":"Model Compatibility","text":"<p>Structured outputs work with models that support function calling:</p> <p>\u2705 Supported: - OpenAI: GPT-4, GPT-4-turbo, GPT-3.5-turbo - Anthropic: Claude 3 Opus, Sonnet, Haiku - Google: Gemini Pro, Gemini 1.5 Pro - Azure OpenAI: All GPT-4 and GPT-3.5-turbo variants</p> <p>\u274c Not Supported: - Legacy models (GPT-3, older models) - Some open-source models without function calling</p>"},{"location":"integration/structured-outputs/#best-practices","title":"Best Practices","text":""},{"location":"integration/structured-outputs/#1-use-descriptive-field-names","title":"1. Use Descriptive Field Names","text":"<pre><code># \u274c Bad: Unclear field names\nclass Data(BaseModel):\n    f1: str\n    f2: int\n    f3: bool\n\n# \u2705 Good: Clear, descriptive names\nclass UserProfile(BaseModel):\n    full_name: str\n    age_years: int\n    is_verified: bool\n</code></pre>"},{"location":"integration/structured-outputs/#2-add-field-descriptions","title":"2. Add Field Descriptions","text":"<pre><code># \u2705 Good: Descriptions help the LLM understand\nclass Product(BaseModel):\n    name: str = Field(description=\"Product name as it appears on packaging\")\n    price: float = Field(description=\"Price in USD, without currency symbol\")\n    sku: str = Field(description=\"Stock keeping unit, format: ABC-1234\")\n</code></pre>"},{"location":"integration/structured-outputs/#3-use-validation","title":"3. Use Validation","text":"<pre><code># \u2705 Good: Validate data types and ranges\nclass Rating(BaseModel):\n    score: int = Field(ge=1, le=5, description=\"Rating from 1-5\")\n    reviewer: str = Field(min_length=1, max_length=100)\n    verified: bool\n</code></pre>"},{"location":"integration/structured-outputs/#4-make-fields-optional-when-appropriate","title":"4. Make Fields Optional When Appropriate","text":"<pre><code># \u2705 Good: Optional for data that might not exist\nclass Article(BaseModel):\n    title: str                           # Always required\n    author: str                          # Always required\n    subtitle: Optional[str] = None       # Might not exist\n    published_date: Optional[str] = None # Might not be found\n</code></pre>"},{"location":"integration/structured-outputs/#5-use-enums-for-fixed-options","title":"5. Use Enums for Fixed Options","text":"<pre><code># \u2705 Good: Constrain to specific values\nclass Status(str, Enum):\n    DRAFT = \"draft\"\n    PUBLISHED = \"published\"\n    ARCHIVED = \"archived\"\n\nclass Article(BaseModel):\n    title: str\n    status: Status  # Can only be one of the enum values\n</code></pre>"},{"location":"integration/structured-outputs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integration/structured-outputs/#llm-returns-invalid-data","title":"LLM Returns Invalid Data","text":"<p>Problem: Validation errors even with clear schema</p> <p>Solutions: 1. Add more detailed field descriptions 2. Provide an example in the prompt 3. Use a more capable model (GPT-4 vs GPT-3.5) 4. Make fields optional if data might not exist</p> <pre><code># Better prompt with example\nmessages=[{\n    \"role\": \"user\",\n    \"content\": f\"\"\"\n    Extract person data from: {text}\n\n    Example output format:\n    {{\n        \"name\": \"John Doe\",\n        \"email\": \"john@example.com\",\n        \"phone\": \"+1-555-1234\"\n    }}\n    \"\"\"\n}]\n</code></pre>"},{"location":"integration/structured-outputs/#model-doesnt-support-structured-outputs","title":"Model Doesn't Support Structured Outputs","text":"<p>Problem: \"Model does not support function calling\"</p> <p>Solution: Use a compatible model (GPT-4, Claude 3, Gemini Pro)</p>"},{"location":"integration/structured-outputs/#performance-issues","title":"Performance Issues","text":"<p>Problem: Structured outputs are slower than regular chat</p> <p>This is expected: - LLM must generate valid JSON matching schema - More processing overhead than free-form text - Trade-off for reliability and type safety</p> <p>Optimize: - Use faster models (GPT-3.5-turbo vs GPT-4) for simple extractions - Process documents in batches concurrently - Cache results when possible</p>"},{"location":"integration/structured-outputs/#next-steps","title":"Next Steps","text":"<p>Now that you understand structured outputs:</p> <ol> <li>See Examples - Working code examples</li> <li>Learn Streaming - Real-time responses</li> <li>Error Handling - Handle failures gracefully</li> <li>Best Practices - Production patterns</li> </ol>"},{"location":"integration/structured-outputs/#quick-reference","title":"Quick Reference","text":""},{"location":"integration/structured-outputs/#basic-structured-output","title":"Basic Structured Output","text":"<pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    email: str\n\nperson = await client.structured_output(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Extract: John, john@example.com\"}],\n    response_model=Person\n)\n</code></pre>"},{"location":"integration/structured-outputs/#with-validation","title":"With Validation","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str = Field(min_length=1)\n    price: float = Field(gt=0)\n    quantity: int = Field(ge=0)\n\nproduct = await client.structured_output(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"...\"}],\n    response_model=Product\n)\n</code></pre>"},{"location":"integration/structured-outputs/#with-optional-fields","title":"With Optional Fields","text":"<pre><code>from typing import Optional\n\nclass User(BaseModel):\n    username: str\n    email: str\n    phone: Optional[str] = None\n\nuser = await client.structured_output(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"...\"}],\n    response_model=User\n)\n</code></pre>"},{"location":"integration/typed-client/","title":"Type-Safe Python Client","text":"<p>The easiest way to integrate with SaaS LiteLLM is using our type-safe Python client with full async support and Pydantic validation.</p> <p>Download the Client</p> <p> Get the Typed Client</p> <p>Complete source code, installation guide, and usage examples.</p>"},{"location":"integration/typed-client/#why-use-the-typed-client","title":"Why Use the Typed Client?","text":"<p>Raw API: <pre><code># Complex, verbose, error-prone\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:8003/api/jobs/create\",\n    headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n    json={\"team_id\": \"acme-corp\", \"job_type\": \"analysis\"}\n)\njob = response.json()\n\nllm_response = requests.post(\n    f\"http://localhost:8003/api/jobs/{job['job_id']}/llm-call\",\n    headers={\"Authorization\": f\"Bearer {VIRTUAL_KEY}\"},\n    json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}\n)\n# ...\n</code></pre></p> <p>Typed Client: <pre><code># Clean, typed, easy\nfrom examples.typed_client import SaaSLLMClient\n\nasync with SaaSLLMClient(\n    base_url=\"http://localhost:8003\",\n    team_id=\"acme-corp\",\n    virtual_key=\"sk-your-key\"\n) as client:\n    job_id = await client.create_job(\"analysis\")\n    response = await client.chat(job_id, [\n        {\"role\": \"user\", \"content\": \"Hello\"}\n    ])\n    await client.complete_job(job_id, \"completed\")\n</code></pre></p> <p>\u2705 Type hints and autocomplete \u2705 Automatic error handling \u2705 Context manager support \u2705 Pydantic validation \u2705 Async/await support \u2705 Cleaner code</p>"},{"location":"integration/typed-client/#installation","title":"Installation","text":""},{"location":"integration/typed-client/#copy-the-client","title":"Copy the Client","text":"<p>The typed client is in <code>examples/typed_client.py</code>:</p> <pre><code># Copy to your project\ncp examples/typed_client.py your_project/saas_litellm_client.py\n</code></pre>"},{"location":"integration/typed-client/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install httpx pydantic\n</code></pre>"},{"location":"integration/typed-client/#quick-start","title":"Quick Start","text":""},{"location":"integration/typed-client/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom saas_litellm_client import SaaSLLMClient\n\nasync def main():\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-virtual-key-here\"\n    ) as client:\n\n        # Create job\n        job_id = await client.create_job(\"my_first_job\")\n        print(f\"Created job: {job_id}\")\n\n        # Make LLM call\n        response = await client.chat(\n            job_id=job_id,\n            messages=[\n                {\"role\": \"user\", \"content\": \"What is Python?\"}\n            ]\n        )\n\n        print(f\"Response: {response.choices[0].message['content']}\")\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"Credits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"integration/typed-client/#client-methods","title":"Client Methods","text":""},{"location":"integration/typed-client/#create_job","title":"create_job()","text":"<p>Create a new job for tracking:</p> <pre><code>job_id = await client.create_job(\n    job_type=\"document_analysis\",\n    metadata={\"document_id\": \"doc_123\", \"user\": \"john\"}\n)\n</code></pre> <p>Parameters: - <code>job_type</code> (str): Type of job (e.g., \"analysis\", \"chat\", \"extraction\") - <code>metadata</code> (dict, optional): Custom data</p> <p>Returns: <code>str</code> - Job ID (UUID)</p>"},{"location":"integration/typed-client/#chat","title":"chat()","text":"<p>Make a non-streaming LLM call:</p> <pre><code>response = await client.chat(\n    job_id=job_id,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    temperature=0.7,\n    max_tokens=500\n)\n\ncontent = response.choices[0].message[\"content\"]\n</code></pre> <p>Parameters: - <code>job_id</code> (str): Job ID from create_job() - <code>messages</code> (list): Chat messages - <code>temperature</code> (float, optional): 0.0-2.0, default 0.7 - <code>max_tokens</code> (int, optional): Max response length - <code>top_p</code> (float, optional): Nucleus sampling - <code>frequency_penalty</code> (float, optional): Reduce repetition - <code>presence_penalty</code> (float, optional): Encourage new topics - <code>stop</code> (str|list, optional): Stop sequences</p> <p>Returns: <code>ChatCompletionResponse</code> - Pydantic model with response</p>"},{"location":"integration/typed-client/#chat_stream","title":"chat_stream()","text":"<p>Make a streaming LLM call:</p> <pre><code>async for chunk in client.chat_stream(\n    job_id=job_id,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n):\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n</code></pre> <p>Parameters: Same as <code>chat()</code></p> <p>Yields: <code>ChatCompletionChunk</code> - Streaming chunks</p>"},{"location":"integration/typed-client/#structured_output","title":"structured_output()","text":"<p>Get type-safe structured responses with Pydantic models:</p> <pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n\nperson = await client.structured_output(\n    job_id=job_id,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract: John Smith, 35, john@example.com\"\n    }],\n    response_model=Person\n)\n\nprint(f\"Name: {person.name}, Age: {person.age}\")\n</code></pre> <p>Parameters: - <code>job_id</code> (str): Job ID - <code>messages</code> (list): Chat messages - <code>response_model</code> (Type[BaseModel]): Pydantic model class - Other chat parameters</p> <p>Returns: Instance of your Pydantic model</p>"},{"location":"integration/typed-client/#complete_job","title":"complete_job()","text":"<p>Mark job as completed:</p> <pre><code>result = await client.complete_job(\n    job_id=job_id,\n    status=\"completed\",\n    metadata={\"result\": \"success\", \"output_file\": \"result.json\"}\n)\n\nprint(f\"Credits remaining: {result.credits_remaining}\")\nprint(f\"Total calls: {result.total_calls}\")\n</code></pre> <p>Parameters: - <code>job_id</code> (str): Job ID - <code>status</code> (str): \"completed\" or \"failed\" - <code>metadata</code> (dict, optional): Additional data</p> <p>Returns: <code>JobCompletionResult</code> - Pydantic model with results</p>"},{"location":"integration/typed-client/#complete-example","title":"Complete Example","text":""},{"location":"integration/typed-client/#document-analysis","title":"Document Analysis","text":"<pre><code>import asyncio\nfrom saas_litellm_client import SaaSLLMClient\n\nasync def analyze_document(document_text: str):\n    \"\"\"Analyze a document: extract key points and summarize\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        # Create job\n        job_id = await client.create_job(\n            job_type=\"document_analysis\",\n            metadata={\"document_length\": len(document_text)}\n        )\n\n        try:\n            # Extract key points\n            key_points_response = await client.chat(\n                job_id=job_id,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Extract key points as bullet points\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Extract key points from:\\n\\n{document_text}\"\n                    }\n                ],\n                temperature=0.3,\n                max_tokens=500\n            )\n\n            key_points = key_points_response.choices[0].message[\"content\"]\n            print(f\"Key Points:\\n{key_points}\\n\")\n\n            # Generate summary\n            summary_response = await client.chat(\n                job_id=job_id,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Create concise summaries\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Summarize in 2-3 sentences:\\n\\n{document_text}\"\n                    }\n                ],\n                temperature=0.5,\n                max_tokens=200\n            )\n\n            summary = summary_response.choices[0].message[\"content\"]\n            print(f\"Summary:\\n{summary}\\n\")\n\n            # Complete job\n            result = await client.complete_job(job_id, \"completed\")\n            print(f\"Analysis complete! Credits remaining: {result.credits_remaining}\")\n\n            return {\n                \"key_points\": key_points,\n                \"summary\": summary\n            }\n\n        except Exception as e:\n            # Mark job as failed\n            await client.complete_job(job_id, \"failed\")\n            raise\n\nif __name__ == \"__main__\":\n    document = \"\"\"\n    Artificial intelligence (AI) is transforming industries worldwide.\n    Machine learning algorithms can process vast amounts of data and\n    identify patterns. This technology is being applied in healthcare,\n    finance, and transportation for various innovative solutions.\n    \"\"\"\n\n    result = asyncio.run(analyze_document(document))\n</code></pre>"},{"location":"integration/typed-client/#streaming-chat","title":"Streaming Chat","text":"<pre><code>import asyncio\nfrom saas_litellm_client import SaaSLLMClient\n\nasync def interactive_chat():\n    \"\"\"Interactive streaming chat session\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        # Create job for chat session\n        job_id = await client.create_job(\"chat_session\")\n        messages = []\n\n        while True:\n            # Get user input\n            user_input = input(\"\\nYou: \")\n            if user_input.lower() in ['quit', 'exit', 'bye']:\n                break\n\n            # Add to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            # Stream response\n            print(\"Assistant: \", end=\"\", flush=True)\n            assistant_response = \"\"\n\n            async for chunk in client.chat_stream(\n                job_id=job_id,\n                messages=messages,\n                temperature=0.7\n            ):\n                if chunk.choices:\n                    content = chunk.choices[0].delta.get(\"content\", \"\")\n                    assistant_response += content\n                    print(content, end=\"\", flush=True)\n\n            # Add assistant response to conversation\n            messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n        # Complete job\n        result = await client.complete_job(job_id, \"completed\")\n        print(f\"\\n\\nChat ended. Credits remaining: {result.credits_remaining}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(interactive_chat())\n</code></pre>"},{"location":"integration/typed-client/#structured-data-extraction","title":"Structured Data Extraction","text":"<pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom saas_litellm_client import SaaSLLMClient\n\nclass Resume(BaseModel):\n    name: str\n    email: str\n    phone: str\n    years_experience: int\n    skills: list[str]\n    education: str\n\nasync def parse_resume(resume_text: str):\n    \"\"\"Extract structured data from resume\"\"\"\n\n    async with SaaSLLMClient(\n        base_url=\"http://localhost:8003\",\n        team_id=\"acme-corp\",\n        virtual_key=\"sk-your-key\"\n    ) as client:\n\n        job_id = await client.create_job(\"resume_parsing\")\n\n        try:\n            # Get structured output\n            resume = await client.structured_output(\n                job_id=job_id,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": f\"Extract structured data from this resume:\\n\\n{resume_text}\"\n                }],\n                response_model=Resume\n            )\n\n            # resume is now a fully typed Resume object!\n            print(f\"Name: {resume.name}\")\n            print(f\"Email: {resume.email}\")\n            print(f\"Experience: {resume.years_experience} years\")\n            print(f\"Skills: {', '.join(resume.skills)}\")\n\n            await client.complete_job(job_id, \"completed\")\n            return resume\n\n        except Exception as e:\n            await client.complete_job(job_id, \"failed\")\n            raise\n\nif __name__ == \"__main__\":\n    resume_text = \"\"\"\n    John Doe\n    Email: john@example.com\n    Phone: (555) 123-4567\n\n    EXPERIENCE: 5 years as a software engineer\n\n    SKILLS: Python, JavaScript, React, Docker, Kubernetes\n\n    EDUCATION: BS in Computer Science, MIT\n    \"\"\"\n\n    result = asyncio.run(parse_resume(resume_text))\n</code></pre>"},{"location":"integration/typed-client/#error-handling","title":"Error Handling","text":""},{"location":"integration/typed-client/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>from httpx import HTTPStatusError\n\nasync with SaaSLLMClient(...) as client:\n    try:\n        job_id = await client.create_job(\"test\")\n        response = await client.chat(job_id, messages)\n        await client.complete_job(job_id, \"completed\")\n\n    except HTTPStatusError as e:\n        if e.response.status_code == 401:\n            print(\"Authentication failed - check your virtual key\")\n        elif e.response.status_code == 403:\n            print(\"Access denied - check credits or team status\")\n        elif e.response.status_code == 429:\n            print(\"Rate limited - wait and retry\")\n        else:\n            print(f\"HTTP error: {e}\")\n        raise\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"integration/typed-client/#retry-logic","title":"Retry Logic","text":"<pre><code>import asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10)\n)\nasync def make_llm_call_with_retry(client, job_id, messages):\n    \"\"\"Make LLM call with automatic retries\"\"\"\n    return await client.chat(job_id, messages)\n\n# Usage\nasync with SaaSLLMClient(...) as client:\n    job_id = await client.create_job(\"test\")\n\n    try:\n        response = await make_llm_call_with_retry(client, job_id, messages)\n        await client.complete_job(job_id, \"completed\")\n    except Exception as e:\n        await client.complete_job(job_id, \"failed\")\n        raise\n</code></pre>"},{"location":"integration/typed-client/#configuration","title":"Configuration","text":""},{"location":"integration/typed-client/#environment-variables","title":"Environment Variables","text":"<pre><code>import os\nfrom saas_litellm_client import SaaSLLMClient\n\n# Load from environment\nAPI_URL = os.environ.get(\"SAAS_LITELLM_API_URL\", \"http://localhost:8003\")\nTEAM_ID = os.environ[\"SAAS_LITELLM_TEAM_ID\"]\nVIRTUAL_KEY = os.environ[\"SAAS_LITELLM_VIRTUAL_KEY\"]\n\nasync with SaaSLLMClient(\n    base_url=API_URL,\n    team_id=TEAM_ID,\n    virtual_key=VIRTUAL_KEY\n) as client:\n    # Use client\n    pass\n</code></pre> <p><code>.env</code> file: <pre><code>SAAS_LITELLM_API_URL=https://api.yourcompany.com\nSAAS_LITELLM_TEAM_ID=acme-prod\nSAAS_LITELLM_VIRTUAL_KEY=sk-your-actual-key-here\n</code></pre></p>"},{"location":"integration/typed-client/#custom-timeout","title":"Custom Timeout","text":"<pre><code>async with SaaSLLMClient(\n    base_url=\"http://localhost:8003\",\n    team_id=\"acme-corp\",\n    virtual_key=\"sk-your-key\",\n    timeout=60.0  # 60 seconds\n) as client:\n    # Use client\n    pass\n</code></pre>"},{"location":"integration/typed-client/#advanced-usage","title":"Advanced Usage","text":""},{"location":"integration/typed-client/#concurrent-jobs","title":"Concurrent Jobs","text":"<p>Process multiple jobs concurrently:</p> <pre><code>import asyncio\n\nasync def process_document(client, document):\n    \"\"\"Process one document\"\"\"\n    job_id = await client.create_job(\"analysis\")\n    response = await client.chat(job_id, [\n        {\"role\": \"user\", \"content\": f\"Analyze: {document}\"}\n    ])\n    await client.complete_job(job_id, \"completed\")\n    return response\n\nasync def process_batch(documents):\n    \"\"\"Process multiple documents concurrently\"\"\"\n    async with SaaSLLMClient(...) as client:\n        tasks = [process_document(client, doc) for doc in documents]\n        results = await asyncio.gather(*tasks)\n        return results\n\n# Process 10 documents at once\ndocuments = [\"doc1\", \"doc2\", ..., \"doc10\"]\nresults = asyncio.run(process_batch(documents))\n</code></pre>"},{"location":"integration/typed-client/#context-manager-options","title":"Context Manager Options","text":"<pre><code># Option 1: Context manager (recommended)\nasync with SaaSLLMClient(...) as client:\n    # Automatic cleanup\n    pass\n\n# Option 2: Manual lifecycle\nclient = SaaSLLMClient(...)\ntry:\n    job_id = await client.create_job(\"test\")\n    # ...\nfinally:\n    await client.close()\n</code></pre>"},{"location":"integration/typed-client/#best-practices","title":"Best Practices","text":"<ol> <li>Always use context manager (<code>async with</code>) for automatic cleanup</li> <li>Handle errors properly - Always complete jobs, even on failure</li> <li>Use structured outputs for type safety when extracting data</li> <li>Set timeouts - Don't let requests hang forever</li> <li>Monitor credits - Check balance periodically</li> <li>Reuse client - Don't create new client for each request</li> <li>Environment variables - Never hardcode credentials</li> </ol>"},{"location":"integration/typed-client/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integration/typed-client/#import-error","title":"Import Error","text":"<p>Problem: <code>ImportError: No module named 'httpx'</code></p> <p>Solution: <pre><code>pip install httpx pydantic\n</code></pre></p>"},{"location":"integration/typed-client/#authentication-error","title":"Authentication Error","text":"<p>Problem: 401 Unauthorized</p> <p>Solutions: 1. Check virtual key is correct 2. Verify <code>Authorization: Bearer sk-...</code> format 3. Check team is active (not suspended)</p>"},{"location":"integration/typed-client/#timeout-error","title":"Timeout Error","text":"<p>Problem: Request times out</p> <p>Solutions: 1. Increase timeout: <code>SaaSLLMClient(..., timeout=120)</code> 2. Check API is running and accessible 3. For long responses, use streaming instead</p>"},{"location":"integration/typed-client/#next-steps","title":"Next Steps","text":"<p>Now that you understand the typed client:</p> <ol> <li>See More Examples - Additional code examples</li> <li>Learn About Streaming - Real-time responses</li> <li>Structured Outputs - Type-safe data extraction</li> <li>Error Handling - Comprehensive error handling</li> <li>Best Practices - Production-ready patterns</li> </ol>"},{"location":"reference/credit-system/","title":"Credit System Reference","text":"<p>Complete technical reference for the credit-based billing system in SaaS LiteLLM. This system implements job-based billing where 1 credit = 1 successfully completed job, independent of actual LLM costs.</p>"},{"location":"reference/credit-system/#overview","title":"Overview","text":"<p>The credit system provides:</p> <ul> <li>Job-based billing: Charge customers per business operation, not per token</li> <li>Budget control: Fixed and unlimited budget modes with optional auto-refill</li> <li>Audit trail: Complete transaction history for compliance</li> <li>Credit isolation: Per-team credit accounts with organization hierarchy</li> <li>Flexible allocation: Manual and automatic credit management</li> </ul>"},{"location":"reference/credit-system/#core-concept","title":"Core Concept","text":""},{"location":"reference/credit-system/#1-credit-1-job","title":"1 Credit = 1 Job","text":"<p>Unlike traditional LLM billing (per token), this system charges 1 credit per successfully completed job:</p> <ul> <li>A job can make 1 or 100 LLM calls - still costs 1 credit</li> <li>Failed jobs don't consume credits</li> <li>Cancelled jobs don't consume credits</li> <li>Only jobs with <code>status='completed'</code> and no failed calls are charged</li> </ul> <p>This simplifies pricing for your customers and makes costs predictable.</p>"},{"location":"reference/credit-system/#credit-flow","title":"Credit Flow","text":""},{"location":"reference/credit-system/#job-lifecycle-and-credit-deduction","title":"Job Lifecycle and Credit Deduction","text":"<pre><code>graph TD\n    A[Create Job] --&gt;|status: pending| B[Job Created]\n    B --&gt;|First LLM call| C[status: in_progress]\n    C --&gt;|More LLM calls| C\n    C --&gt;|Complete job| D{Check Status}\n    D --&gt;|status: completed| E{Check Errors}\n    D --&gt;|status: failed| G[No Credit Deducted]\n    D --&gt;|status: cancelled| G\n    E --&gt;|No failed calls| F{Check Credits}\n    E --&gt;|Has failed calls| G\n    F --&gt;|Credits available| H[Deduct 1 Credit]\n    F --&gt;|Insufficient| I[Error: Insufficient Credits]\n    H --&gt; J[Mark credit_applied=true]\n    J --&gt; K[Record Transaction]\n    K --&gt; L[Return Summary]</code></pre>"},{"location":"reference/credit-system/#credit-deduction-rules","title":"Credit Deduction Rules","text":"<p>Credits are deducted only when ALL conditions are met:</p> <ol> <li>Job Status: <code>status = 'completed'</code> (not <code>'failed'</code> or <code>'cancelled'</code>)</li> <li>No Errors: All associated LLM calls succeeded (no <code>error</code> field set)</li> <li>Not Already Applied: <code>credit_applied = FALSE</code> (prevents double-charging)</li> </ol> <p>Implementation (from <code>src/saas_api.py</code>):</p> <pre><code>if (request.status == \"completed\" and\n    costs[\"failed_calls\"] == 0 and\n    not job.credit_applied):\n\n    credit_manager.deduct_credit(\n        team_id=job.team_id,\n        job_id=job.job_id,\n        credits_amount=1,\n        reason=f\"Job {job.job_type} completed successfully\"\n    )\n    job.credit_applied = True\n</code></pre>"},{"location":"reference/credit-system/#database-schema","title":"Database Schema","text":""},{"location":"reference/credit-system/#team_credits-table","title":"team_credits Table","text":"<p>Primary table for credit balance tracking.</p> <pre><code>CREATE TABLE team_credits (\n    team_id VARCHAR(255) PRIMARY KEY,\n    organization_id VARCHAR(255) REFERENCES organizations(organization_id),\n    credits_allocated INTEGER DEFAULT 0,\n    credits_used INTEGER DEFAULT 0,\n    credits_remaining INTEGER GENERATED ALWAYS AS (credits_allocated - credits_used) STORED,\n    credit_limit INTEGER,\n    auto_refill BOOLEAN DEFAULT FALSE,\n    refill_amount INTEGER,\n    refill_period VARCHAR(50),\n    last_refill_at TIMESTAMP,\n    virtual_key VARCHAR(500),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n</code></pre> <p>Key Fields:</p> <ul> <li><code>credits_allocated</code>: Total credits given to team</li> <li><code>credits_used</code>: Credits consumed by completed jobs</li> <li><code>credits_remaining</code>: Auto-calculated (allocated - used)</li> <li><code>credit_limit</code>: Hard limit (NULL = unlimited)</li> <li><code>virtual_key</code>: LiteLLM virtual API key for this team</li> </ul>"},{"location":"reference/credit-system/#credit_transactions-table","title":"credit_transactions Table","text":"<p>Immutable audit log of all credit operations.</p> <pre><code>CREATE TABLE credit_transactions (\n    transaction_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    team_id VARCHAR(255) NOT NULL,\n    organization_id VARCHAR(255),\n    job_id UUID REFERENCES jobs(job_id),\n    transaction_type VARCHAR(50) NOT NULL,\n    credits_amount INTEGER NOT NULL,\n    credits_before INTEGER NOT NULL,\n    credits_after INTEGER NOT NULL,\n    reason VARCHAR(500),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n</code></pre> <p>Transaction Types:</p> <ul> <li><code>deduction</code>: Credit removed (job completed)</li> <li><code>allocation</code>: Credit added (purchase, grant)</li> <li><code>refund</code>: Credit returned (job failed after deduction, error correction)</li> <li><code>adjustment</code>: Manual correction by admin</li> </ul>"},{"location":"reference/credit-system/#budget-modes","title":"Budget Modes","text":"<p>The credit system supports three budget modes that determine how credits are calculated and deducted:</p>"},{"location":"reference/credit-system/#mode-1-job-based-default","title":"Mode 1: Job-Based (Default)","text":"<p>1 credit = 1 completed job regardless of actual costs.</p> <p>Configuration: <pre><code>UPDATE team_credits\nSET budget_mode = 'job_based'\nWHERE team_id = 'team-alpha';\n</code></pre></p> <p>Behavior: - Simple, predictable pricing - Every completed job costs exactly 1 credit - Actual USD costs and token usage are tracked separately but don't affect credit deduction - Perfect for fixed-price offerings</p> <p>Use Cases: - Subscription plans with fixed credit allocations - Predictable pricing models - Freemium tiers</p>"},{"location":"reference/credit-system/#mode-2-consumption-usd","title":"Mode 2: Consumption-USD","text":"<p>Credits are deducted based on actual USD cost from LLM providers.</p> <p>Configuration: <pre><code>UPDATE team_credits\nSET budget_mode = 'consumption_usd',\n    credits_per_dollar = 10.0  -- 1 credit = $0.10\nWHERE team_id = 'team-alpha';\n</code></pre></p> <p>Behavior: - Credit deduction: <code>credits = total_cost_usd * credits_per_dollar</code> - Minimum: 1 credit per successful job - Teams can have custom <code>credits_per_dollar</code> rates - Falls back to <code>DEFAULT_CREDITS_PER_DOLLAR</code> (10.0) if not set</p> <p>Per-Team Rate Configuration:</p> <p>Admins can set team-specific rates: <pre><code># Set custom rate for premium customers\nPATCH /api/credits/teams/{team_id}/conversion-rates\nAuthorization: Bearer {admin_token}\n\n{\n  \"credits_per_dollar\": 5.0  # 1 credit = $0.20 (better rate)\n}\n</code></pre></p> <p>Example Calculation: <pre><code>Job costs $0.034 in LLM API calls\ncredits_per_dollar = 10.0\nCredits deducted = 0.034 * 10.0 = 0.34 \u2192 1 credit (minimum)\n\nJob costs $0.152 in LLM API calls\ncredits_per_dollar = 10.0\nCredits deducted = 0.152 * 10.0 = 1.52 \u2192 2 credits\n</code></pre></p> <p>Use Cases: - Pay-as-you-go billing - Enterprise customers with volume discounts - Teams with different pricing tiers</p>"},{"location":"reference/credit-system/#mode-3-consumption-tokens","title":"Mode 3: Consumption-Tokens","text":"<p>Credits are deducted based on total tokens used.</p> <p>Configuration: <pre><code>UPDATE team_credits\nSET budget_mode = 'consumption_tokens',\n    tokens_per_credit = 10000  -- 10,000 tokens = 1 credit\nWHERE team_id = 'team-alpha';\n</code></pre></p> <p>Behavior: - Credit deduction: <code>credits = total_tokens / tokens_per_credit</code> - Minimum: 1 credit per successful job - Teams can have custom <code>tokens_per_credit</code> rates - Falls back to <code>DEFAULT_TOKENS_PER_CREDIT</code> (10000) if not set</p> <p>Per-Team Rate Configuration:</p> <p>Admins can set team-specific rates: <pre><code># Set custom rate for high-volume customers\nPATCH /api/credits/teams/{team_id}/conversion-rates\nAuthorization: Bearer {admin_token}\n\n{\n  \"tokens_per_credit\": 20000  # 20,000 tokens = 1 credit (better rate)\n}\n</code></pre></p> <p>Example Calculation: <pre><code>Job uses 8,500 tokens\ntokens_per_credit = 10000\nCredits deducted = 8500 / 10000 = 0.85 \u2192 1 credit (minimum)\n\nJob uses 45,000 tokens\ntokens_per_credit = 10000\nCredits deducted = 45000 / 10000 = 4.5 \u2192 5 credits\n</code></pre></p> <p>Use Cases: - Token-based pricing models - Different rates for different customer tiers - Cost pass-through with markup</p>"},{"location":"reference/credit-system/#budget-mode-comparison","title":"Budget Mode Comparison","text":"Mode Calculation Predictability Flexibility Best For job_based 1 credit/job High Low Fixed pricing, subscriptions consumption_usd USD * rate Medium High Enterprise, volume pricing consumption_tokens Tokens / rate Medium High Token-based SaaS, developer tools"},{"location":"reference/credit-system/#managing-conversion-rates-admin-only","title":"Managing Conversion Rates (Admin Only)","text":"<p>Administrators can configure per-team conversion rates via the API:</p>"},{"location":"reference/credit-system/#get-current-rates","title":"Get Current Rates","text":"<pre><code>GET /api/credits/teams/{team_id}/conversion-rates\nAuthorization: Bearer {admin_token}\n\nResponse:\n{\n  \"team_id\": \"team-alpha\",\n  \"tokens_per_credit\": 10000,\n  \"credits_per_dollar\": 10.0,\n  \"budget_mode\": \"consumption_tokens\",\n  \"using_defaults\": {\n    \"tokens_per_credit\": false,\n    \"credits_per_dollar\": true\n  }\n}\n</code></pre>"},{"location":"reference/credit-system/#update-rates","title":"Update Rates","text":"<pre><code>PATCH /api/credits/teams/{team_id}/conversion-rates\nAuthorization: Bearer {admin_token}\nContent-Type: application/json\n\n{\n  \"tokens_per_credit\": 20000,      # Optional\n  \"credits_per_dollar\": 5.0         # Optional\n}\n\nResponse:\n{\n  \"team_id\": \"team-alpha\",\n  \"tokens_per_credit\": 20000,\n  \"credits_per_dollar\": 5.0,\n  \"message\": \"Conversion rates updated successfully\"\n}\n</code></pre> <p>Validation: - Both values must be positive numbers - <code>tokens_per_credit</code> must be an integer - <code>credits_per_dollar</code> can be a decimal - Setting to <code>null</code> will use system defaults</p>"},{"location":"reference/credit-system/#default-values","title":"Default Values","text":"<p>If team-specific rates are not configured, the system uses these defaults:</p> <pre><code>DEFAULT_TOKENS_PER_CREDIT = 10000      # 10,000 tokens = 1 credit\nDEFAULT_CREDITS_PER_DOLLAR = 10.0      # 1 credit = $0.10\nMINIMUM_CREDITS_PER_JOB = 1            # Always deduct at least 1 credit\n</code></pre> <p>These are defined in <code>src/api/constants.py</code>.</p>"},{"location":"reference/credit-system/#fixed-budget-implementation","title":"Fixed Budget Implementation","text":""},{"location":"reference/credit-system/#1-fixed-budget","title":"1. Fixed Budget","text":"<p>Team has a fixed allocation. No automatic refills.</p> <p>Configuration:</p> <pre><code>UPDATE team_credits\nSET credits_allocated = 1000,\n    credit_limit = 1000,\n    auto_refill = FALSE\nWHERE team_id = 'team-alpha';\n</code></pre> <p>Behavior:</p> <ul> <li>Can use up to <code>credits_allocated</code> credits</li> <li>When <code>credits_remaining = 0</code>, all new job completions fail with <code>InsufficientCreditsError</code></li> <li>Must manually allocate more credits to continue</li> </ul> <p>Use Cases:</p> <ul> <li>Free tier / trial users</li> <li>One-time credit packages</li> <li>Prepaid credits</li> </ul>"},{"location":"reference/credit-system/#2-unlimited-budget","title":"2. Unlimited Budget","text":"<p>No hard limit on usage. Team can use as many credits as needed.</p> <p>Configuration:</p> <pre><code>UPDATE team_credits\nSET credits_allocated = 0,\n    credit_limit = NULL,\n    auto_refill = FALSE\nWHERE team_id = 'team-enterprise';\n</code></pre> <p>Behavior:</p> <ul> <li>No credit checks performed</li> <li>Team can always complete jobs</li> <li>Track usage for billing/reporting purposes</li> <li><code>credits_used</code> increments, but <code>credits_remaining</code> can go negative</li> </ul> <p>Use Cases:</p> <ul> <li>Enterprise customers</li> <li>Pay-as-you-go billing</li> <li>Internal teams</li> </ul>"},{"location":"reference/credit-system/#3-recurring-budget-with-auto-refill","title":"3. Recurring Budget with Auto-Refill","text":"<p>Credits automatically refill on a schedule (daily, weekly, monthly).</p> <p>Configuration:</p> <pre><code>UPDATE team_credits\nSET credits_allocated = 1000,\n    auto_refill = TRUE,\n    refill_amount = 1000,\n    refill_period = 'monthly',\n    last_refill_at = NOW()\nWHERE team_id = 'team-subscription';\n</code></pre> <p>Behavior:</p> <ul> <li>Every <code>refill_period</code>, add <code>refill_amount</code> to <code>credits_allocated</code></li> <li>Useful for subscription-based pricing</li> <li>Requires background job to check <code>last_refill_at</code> and apply refills</li> </ul> <p>Refill Logic (pseudocode):</p> <pre><code>def check_refill(team_id):\n    team = get_team_credits(team_id)\n\n    if not team.auto_refill:\n        return\n\n    now = datetime.utcnow()\n    last_refill = team.last_refill_at\n\n    if should_refill(last_refill, team.refill_period, now):\n        allocate_credits(\n            team_id=team_id,\n            amount=team.refill_amount,\n            reason=f\"Auto-refill: {team.refill_period}\"\n        )\n        team.last_refill_at = now\n</code></pre> <p>Use Cases:</p> <ul> <li>Monthly subscription plans</li> <li>Weekly credit allowances</li> <li>Trial period extensions</li> </ul>"},{"location":"reference/credit-system/#credit-manager-service","title":"Credit Manager Service","text":"<p>The <code>CreditManager</code> service handles all credit operations.</p>"},{"location":"reference/credit-system/#location","title":"Location","text":"<p><code>src/services/credit_manager.py</code></p>"},{"location":"reference/credit-system/#class-definition","title":"Class Definition","text":"<pre><code>class CreditManager:\n    def __init__(self, db: Session):\n        self.db = db\n</code></pre>"},{"location":"reference/credit-system/#key-methods","title":"Key Methods","text":""},{"location":"reference/credit-system/#1-get_team_credits","title":"1. get_team_credits()","text":"<p>Retrieve team credit balance.</p> <pre><code>def get_team_credits(self, team_id: str) -&gt; Optional[TeamCredits]:\n    return self.db.query(TeamCredits).filter(\n        TeamCredits.team_id == team_id\n    ).first()\n</code></pre> <p>Returns: <code>TeamCredits</code> object or <code>None</code> if not found</p>"},{"location":"reference/credit-system/#2-check_credits_available","title":"2. check_credits_available()","text":"<p>Verify team has sufficient credits before starting a job.</p> <pre><code>def check_credits_available(\n    self,\n    team_id: str,\n    credits_needed: int = 1\n) -&gt; bool:\n    credits = self.get_team_credits(team_id)\n\n    if not credits:\n        raise InsufficientCreditsError(\n            f\"No credit account found for team '{team_id}'\"\n        )\n\n    if credits.credits_remaining &lt; credits_needed:\n        raise InsufficientCreditsError(\n            f\"Insufficient credits. Team has {credits.credits_remaining} \"\n            f\"credits remaining, but {credits_needed} required.\"\n        )\n\n    return True\n</code></pre> <p>Raises: <code>InsufficientCreditsError</code> if credits insufficient or account not found</p> <p>Usage:</p> <pre><code># Before creating job\ncredit_manager = get_credit_manager(db)\ntry:\n    credit_manager.check_credits_available(team_id=\"team-alpha\")\n    # Proceed with job creation\nexcept InsufficientCreditsError as e:\n    return error_response(403, str(e))\n</code></pre>"},{"location":"reference/credit-system/#3-deduct_credit","title":"3. deduct_credit()","text":"<p>Deduct credits when job completes successfully.</p> <pre><code>def deduct_credit(\n    self,\n    team_id: str,\n    job_id: Optional[uuid.UUID] = None,\n    credits_amount: int = 1,\n    reason: str = \"Job completed successfully\"\n) -&gt; CreditTransaction:\n</code></pre> <p>Parameters:</p> <ul> <li><code>team_id</code>: Team to deduct from</li> <li><code>job_id</code>: Associated job (for audit trail)</li> <li><code>credits_amount</code>: Number of credits (usually 1)</li> <li><code>reason</code>: Human-readable explanation</li> </ul> <p>Returns: <code>CreditTransaction</code> record</p> <p>Implementation:</p> <ol> <li>Get team credits</li> <li>Check sufficient balance</li> <li>Increment <code>credits_used</code></li> <li>Create transaction record with before/after balances</li> <li>Commit to database</li> </ol> <p>Example:</p> <pre><code>transaction = credit_manager.deduct_credit(\n    team_id=\"team-alpha\",\n    job_id=job_uuid,\n    credits_amount=1,\n    reason=\"Resume analysis completed successfully\"\n)\n\n# Transaction record:\n# - transaction_type: \"deduction\"\n# - credits_amount: 1\n# - credits_before: 1000\n# - credits_after: 999\n</code></pre>"},{"location":"reference/credit-system/#4-allocate_credits","title":"4. allocate_credits()","text":"<p>Add credits to team account (purchase, grant, refill).</p> <pre><code>def allocate_credits(\n    self,\n    team_id: str,\n    credits_amount: int,\n    reason: str = \"Credit allocation\",\n    organization_id: Optional[str] = None\n) -&gt; CreditTransaction:\n</code></pre> <p>Parameters:</p> <ul> <li><code>team_id</code>: Team to allocate to</li> <li><code>credits_amount</code>: Number of credits to add</li> <li><code>reason</code>: Explanation (e.g., \"Monthly subscription refill\")</li> <li><code>organization_id</code>: Parent org (for reporting)</li> </ul> <p>Returns: <code>CreditTransaction</code> record</p> <p>Implementation:</p> <ol> <li>Get or create team credits record</li> <li>Increment <code>credits_allocated</code></li> <li>Create transaction record</li> <li>Commit to database</li> </ol> <p>Example:</p> <pre><code># Initial allocation\ntransaction = credit_manager.allocate_credits(\n    team_id=\"team-alpha\",\n    credits_amount=1000,\n    reason=\"New team signup - starter plan\",\n    organization_id=\"org-acme\"\n)\n\n# Monthly refill\ntransaction = credit_manager.allocate_credits(\n    team_id=\"team-alpha\",\n    credits_amount=1000,\n    reason=\"Monthly subscription refill\"\n)\n</code></pre>"},{"location":"reference/credit-system/#5-refund_credit","title":"5. refund_credit()","text":"<p>Return credits to team (error correction, failed job recovery).</p> <pre><code>def refund_credit(\n    self,\n    team_id: str,\n    job_id: Optional[uuid.UUID] = None,\n    credits_amount: int = 1,\n    reason: str = \"Credit refund\"\n) -&gt; CreditTransaction:\n</code></pre> <p>Parameters:</p> <ul> <li><code>team_id</code>: Team to refund</li> <li><code>job_id</code>: Original job (for audit trail)</li> <li><code>credits_amount</code>: Credits to return</li> <li><code>reason</code>: Explanation</li> </ul> <p>Returns: <code>CreditTransaction</code> record</p> <p>Implementation:</p> <ol> <li>Get team credits</li> <li>Decrement <code>credits_used</code> (not <code>credits_allocated</code>)</li> <li>Create transaction record</li> <li>Commit to database</li> </ol> <p>Example:</p> <pre><code># Refund after discovering job failed\ntransaction = credit_manager.refund_credit(\n    team_id=\"team-alpha\",\n    job_id=job_uuid,\n    credits_amount=1,\n    reason=\"Job marked as failed - refunding credit\"\n)\n</code></pre>"},{"location":"reference/credit-system/#6-get_credit_transactions","title":"6. get_credit_transactions()","text":"<p>Query transaction history with filters.</p> <pre><code>def get_credit_transactions(\n    self,\n    team_id: Optional[str] = None,\n    organization_id: Optional[str] = None,\n    job_id: Optional[uuid.UUID] = None,\n    limit: int = 100\n) -&gt; list[CreditTransaction]:\n</code></pre> <p>Parameters:</p> <ul> <li><code>team_id</code>: Filter by team</li> <li><code>organization_id</code>: Filter by organization</li> <li><code>job_id</code>: Filter by specific job</li> <li><code>limit</code>: Max results</li> </ul> <p>Returns: List of <code>CreditTransaction</code> objects (newest first)</p> <p>Example:</p> <pre><code># Get recent transactions for team\ntransactions = credit_manager.get_credit_transactions(\n    team_id=\"team-alpha\",\n    limit=50\n)\n\nfor txn in transactions:\n    print(f\"{txn.created_at}: {txn.transaction_type} \"\n          f\"{txn.credits_amount} credits - {txn.reason}\")\n</code></pre>"},{"location":"reference/credit-system/#credit-deduction-flow","title":"Credit Deduction Flow","text":""},{"location":"reference/credit-system/#complete-job-endpoint-implementation","title":"Complete Job Endpoint Implementation","text":"<p>From <code>src/saas_api.py</code>:</p> <pre><code>@app.post(\"/api/jobs/{job_id}/complete\")\nasync def complete_job(\n    job_id: str,\n    request: JobCompleteRequest,\n    db: Session = Depends(get_db),\n    authenticated_team_id: str = Depends(verify_virtual_key)\n):\n    job = db.query(Job).filter(Job.job_id == uuid.UUID(job_id)).first()\n\n    # Update job status\n    job.status = JobStatus(request.status)\n    job.completed_at = datetime.utcnow()\n\n    # Calculate costs\n    costs = calculate_job_costs(db, job.job_id)\n\n    # Credit deduction logic\n    credit_manager = get_credit_manager(db)\n\n    if (request.status == \"completed\" and\n        costs[\"failed_calls\"] == 0 and\n        not job.credit_applied):\n\n        try:\n            credit_manager.deduct_credit(\n                team_id=job.team_id,\n                job_id=job.job_id,\n                credits_amount=1,\n                reason=f\"Job {job.job_type} completed successfully\"\n            )\n            job.credit_applied = True\n        except InsufficientCreditsError as e:\n            # Log warning but don't fail job\n            logger.warning(f\"Job {job_id} couldn't deduct credit: {e}\")\n\n    db.commit()\n\n    # Return response with credit balance\n    team_credits = db.query(TeamCredits).filter(\n        TeamCredits.team_id == job.team_id\n    ).first()\n\n    costs[\"credit_applied\"] = job.credit_applied\n    costs[\"credits_remaining\"] = team_credits.credits_remaining\n\n    return JobCompleteResponse(\n        job_id=str(job.job_id),\n        status=job.status.value,\n        completed_at=job.completed_at.isoformat(),\n        costs=costs,\n        calls=calls_data\n    )\n</code></pre>"},{"location":"reference/credit-system/#key-points","title":"Key Points","text":"<ol> <li>Status check: Only <code>completed</code> jobs are charged</li> <li>Error check: Jobs with failed LLM calls aren't charged</li> <li>Idempotency: <code>credit_applied</code> flag prevents double-charging</li> <li>Graceful handling: If credit deduction fails, job still completes (logged as warning)</li> <li>Balance returned: Response includes updated <code>credits_remaining</code></li> </ol>"},{"location":"reference/credit-system/#credit-vs-cost-tracking","title":"Credit vs. Cost Tracking","text":"<p>This system tracks two separate metrics:</p>"},{"location":"reference/credit-system/#credits-internal-billing","title":"Credits (Internal Billing)","text":"<ul> <li>Purpose: Bill your customers</li> <li>Unit: 1 credit = 1 completed job</li> <li>Table: <code>team_credits</code>, <code>credit_transactions</code></li> <li>Predictable: Fixed cost per job</li> </ul>"},{"location":"reference/credit-system/#costs-external-billing","title":"Costs (External Billing)","text":"<ul> <li>Purpose: Track your expenses to LLM providers</li> <li>Unit: USD based on tokens</li> <li>Table: <code>llm_calls.cost_usd</code>, <code>job_cost_summaries.total_cost_usd</code></li> <li>Variable: Depends on model, tokens used</li> </ul> <p>Example:</p> <pre><code>Job: Resume Analysis\n\u251c\u2500 Credit charged to customer: 1 credit\n\u251c\u2500 Your revenue: $0.10 (if 1 credit = $0.10)\n\u251c\u2500 Actual LLM cost: $0.034 (from OpenAI)\n\u2514\u2500 Your margin: $0.066 (66% margin)\n</code></pre> <p>Query both:</p> <pre><code>SELECT\n    j.job_id,\n    j.job_type,\n    j.credit_applied,\n    jcs.total_cost_usd,\n    tc.credits_remaining\nFROM jobs j\nJOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nJOIN team_credits tc ON j.team_id = tc.team_id\nWHERE j.team_id = 'team-alpha'\nAND j.status = 'completed';\n</code></pre>"},{"location":"reference/credit-system/#api-operations","title":"API Operations","text":""},{"location":"reference/credit-system/#check-credit-balance","title":"Check Credit Balance","text":"<pre><code>GET /api/teams/{team_id}/credits\nAuthorization: Bearer {virtual_key}\n\nResponse:\n{\n  \"team_id\": \"team-alpha\",\n  \"credits_allocated\": 1000,\n  \"credits_used\": 234,\n  \"credits_remaining\": 766,\n  \"credit_limit\": 1000,\n  \"auto_refill\": false\n}\n</code></pre>"},{"location":"reference/credit-system/#allocate-credits-admin","title":"Allocate Credits (Admin)","text":"<pre><code>POST /api/teams/{team_id}/credits/allocate\nAuthorization: Bearer {master_key}\nContent-Type: application/json\n\n{\n  \"credits_amount\": 500,\n  \"reason\": \"Credit purchase - 500 credits\"\n}\n\nResponse:\n{\n  \"transaction_id\": \"txn-uuid-123\",\n  \"team_id\": \"team-alpha\",\n  \"transaction_type\": \"allocation\",\n  \"credits_amount\": 500,\n  \"credits_before\": 766,\n  \"credits_after\": 1266,\n  \"reason\": \"Credit purchase - 500 credits\"\n}\n</code></pre>"},{"location":"reference/credit-system/#get-transaction-history","title":"Get Transaction History","text":"<pre><code>GET /api/teams/{team_id}/credits/transactions?limit=50\nAuthorization: Bearer {virtual_key}\n\nResponse:\n{\n  \"team_id\": \"team-alpha\",\n  \"transactions\": [\n    {\n      \"transaction_id\": \"txn-uuid-456\",\n      \"transaction_type\": \"deduction\",\n      \"credits_amount\": 1,\n      \"credits_before\": 767,\n      \"credits_after\": 766,\n      \"reason\": \"Job resume_analysis completed successfully\",\n      \"job_id\": \"job-uuid-789\",\n      \"created_at\": \"2024-10-15T10:30:00Z\"\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"reference/credit-system/#error-handling","title":"Error Handling","text":""},{"location":"reference/credit-system/#insufficientcreditserror","title":"InsufficientCreditsError","text":"<p>Raised when team lacks credits to complete operation.</p> <p>When Raised:</p> <ul> <li><code>check_credits_available()</code> - Before job starts (optional, not currently used)</li> <li><code>deduct_credit()</code> - When completing job (if balance is 0)</li> </ul> <p>Example:</p> <pre><code>try:\n    credit_manager.deduct_credit(team_id=\"team-alpha\", job_id=job_uuid)\nexcept InsufficientCreditsError as e:\n    # Return error to user\n    return {\n        \"error\": \"insufficient_credits\",\n        \"message\": str(e),\n        \"credits_remaining\": 0,\n        \"credits_needed\": 1\n    }\n</code></pre> <p>HTTP Response:</p> <pre><code>{\n  \"detail\": \"Insufficient credits. Team has 0 credits remaining, but 1 required. Allocated: 1000, Used: 1000\"\n}\n</code></pre>"},{"location":"reference/credit-system/#handling-during-job-completion","title":"Handling During Job Completion","text":"<p>Currently, the system logs a warning but doesn't fail job completion:</p> <pre><code>except InsufficientCreditsError as e:\n    logger.warning(f\"Job {job_id} completed but couldn't deduct credit: {e}\")\n    # Job still completes, but credit_applied stays false\n</code></pre> <p>Rationale: Job already executed, work was done. Failing at this point creates inconsistency.</p> <p>Alternative Approach: Check credits before starting job (proactive check):</p> <pre><code>@app.post(\"/api/jobs/{job_id}/llm-call\")\nasync def make_llm_call(...):\n    # Check credits before making expensive LLM call\n    try:\n        credit_manager.check_credits_available(job.team_id, credits_needed=1)\n    except InsufficientCreditsError:\n        raise HTTPException(403, \"Insufficient credits to start job\")\n\n    # Proceed with LLM call...\n</code></pre>"},{"location":"reference/credit-system/#best-practices","title":"Best Practices","text":""},{"location":"reference/credit-system/#1-always-use-transactions","title":"1. Always Use Transactions","text":"<p>Never manually update <code>credits_used</code> or <code>credits_allocated</code>. Always use <code>CreditManager</code> methods to ensure transaction records are created.</p> <pre><code># \u2705 Good - Creates transaction record\ncredit_manager.deduct_credit(team_id, job_id, credits_amount=1)\n\n# \u274c Bad - No audit trail\nteam_credits.credits_used += 1\ndb.commit()\n</code></pre>"},{"location":"reference/credit-system/#2-idempotent-operations","title":"2. Idempotent Operations","text":"<p>Use <code>credit_applied</code> flag to prevent double-charging:</p> <pre><code>if not job.credit_applied:\n    credit_manager.deduct_credit(...)\n    job.credit_applied = True\n</code></pre>"},{"location":"reference/credit-system/#3-refund-on-errors","title":"3. Refund on Errors","text":"<p>If a job was marked completed but later found to have errors, refund the credit:</p> <pre><code>if job.credit_applied and discovered_error:\n    credit_manager.refund_credit(\n        team_id=job.team_id,\n        job_id=job.job_id,\n        reason=\"Job retroactively marked as failed - refunding credit\"\n    )\n    job.credit_applied = False\n</code></pre>"},{"location":"reference/credit-system/#4-monitor-credit-balance","title":"4. Monitor Credit Balance","text":"<p>Set up alerts when teams reach low credit thresholds:</p> <pre><code>def check_low_balance(team_id: str, threshold: int = 10):\n    credits = credit_manager.get_team_credits(team_id)\n    if credits.credits_remaining &lt; threshold:\n        send_alert(\n            team_id=team_id,\n            message=f\"Low balance: {credits.credits_remaining} credits remaining\"\n        )\n</code></pre>"},{"location":"reference/credit-system/#5-implement-credit-expiry-optional","title":"5. Implement Credit Expiry (Optional)","text":"<p>For trial credits or promotional credits, track expiry:</p> <pre><code>ALTER TABLE team_credits ADD COLUMN credits_expire_at TIMESTAMP;\n\n-- Query expired credits\nSELECT team_id, credits_remaining\nFROM team_credits\nWHERE credits_expire_at &lt; NOW()\nAND credits_remaining &gt; 0;\n</code></pre>"},{"location":"reference/credit-system/#analytics-queries","title":"Analytics Queries","text":""},{"location":"reference/credit-system/#team-credit-usage-over-time","title":"Team Credit Usage Over Time","text":"<pre><code>SELECT\n    DATE_TRUNC('day', created_at) AS date,\n    SUM(CASE WHEN transaction_type = 'deduction' THEN credits_amount ELSE 0 END) AS credits_used,\n    SUM(CASE WHEN transaction_type = 'allocation' THEN credits_amount ELSE 0 END) AS credits_added\nFROM credit_transactions\nWHERE team_id = 'team-alpha'\nAND created_at &gt;= NOW() - INTERVAL '30 days'\nGROUP BY date\nORDER BY date;\n</code></pre>"},{"location":"reference/credit-system/#organization-wide-credit-summary","title":"Organization-Wide Credit Summary","text":"<pre><code>SELECT\n    o.organization_id,\n    o.name,\n    COUNT(DISTINCT tc.team_id) AS team_count,\n    SUM(tc.credits_allocated) AS total_allocated,\n    SUM(tc.credits_used) AS total_used,\n    SUM(tc.credits_remaining) AS total_remaining\nFROM organizations o\nLEFT JOIN team_credits tc ON o.organization_id = tc.organization_id\nGROUP BY o.organization_id, o.name\nORDER BY total_used DESC;\n</code></pre>"},{"location":"reference/credit-system/#credits-per-job-type","title":"Credits per Job Type","text":"<pre><code>SELECT\n    j.job_type,\n    COUNT(*) AS job_count,\n    SUM(CASE WHEN j.credit_applied THEN 1 ELSE 0 END) AS credits_charged,\n    AVG(jcs.total_cost_usd) AS avg_cost_usd\nFROM jobs j\nLEFT JOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nWHERE j.team_id = 'team-alpha'\nAND j.status = 'completed'\nGROUP BY j.job_type\nORDER BY credits_charged DESC;\n</code></pre>"},{"location":"reference/credit-system/#see-also","title":"See Also","text":"<ul> <li>Database Schema Reference - Complete schema documentation</li> <li>API Reference: Credits - Credit management endpoints</li> <li>Job Workflow Guide - Understanding job lifecycle</li> </ul>"},{"location":"reference/database-schema/","title":"Database Schema Reference","text":"<p>Complete technical reference for the SaaS LiteLLM database schema. This system uses PostgreSQL with a multi-tenant architecture supporting organizations, teams, jobs, credits, and model group management.</p>"},{"location":"reference/database-schema/#overview","title":"Overview","text":"<p>The database schema is organized into several functional areas:</p> <ul> <li>Multi-tenancy: Organizations and teams hierarchy</li> <li>Job Tracking: Jobs, LLM calls, and cost summaries</li> <li>Credit System: Credit allocation, usage tracking, and transactions</li> <li>Model Groups: Dynamic model routing with access control</li> <li>Analytics: Team usage summaries and webhook registrations</li> </ul>"},{"location":"reference/database-schema/#schema-diagram","title":"Schema Diagram","text":"<pre><code>organizations\n    |\n    +-- team_credits (1:N)\n    |       |\n    |       +-- credit_transactions (1:N)\n    |\n    +-- jobs (1:N)\n            |\n            +-- llm_calls (1:N)\n            +-- job_cost_summaries (1:1)\n            +-- credit_transactions (1:N)\n\nmodel_groups\n    |\n    +-- model_group_models (1:N)\n    +-- team_model_groups (N:M) -- teams\n</code></pre>"},{"location":"reference/database-schema/#core-tables","title":"Core Tables","text":""},{"location":"reference/database-schema/#organizations","title":"organizations","text":"<p>Represents top-level organizational entities in the multi-tenant hierarchy.</p> <p>Columns:</p> Column Type Constraints Description <code>organization_id</code> VARCHAR(255) PRIMARY KEY Unique organization identifier <code>name</code> VARCHAR(500) NOT NULL Organization display name <code>status</code> VARCHAR(50) DEFAULT 'active' Status: active, suspended, deleted <code>metadata</code> JSONB DEFAULT '{}' Custom organization metadata <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Creation timestamp <code>updated_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Last update timestamp (auto-updated) <p>Indexes:</p> <ul> <li><code>idx_org_status</code> on <code>(status)</code></li> <li><code>idx_org_created</code> on <code>(created_at)</code></li> </ul> <p>Triggers:</p> <ul> <li><code>update_organizations_updated_at</code> - Automatically updates <code>updated_at</code> on row modification</li> </ul> <p>Example:</p> <pre><code>INSERT INTO organizations (organization_id, name, metadata)\nVALUES ('org-acme', 'Acme Corporation', '{\"industry\": \"technology\"}');\n</code></pre>"},{"location":"reference/database-schema/#team_credits","title":"team_credits","text":"<p>Tracks credit allocation and usage for teams. Credits are the billing unit where 1 credit = 1 successfully completed job.</p> <p>Columns:</p> Column Type Constraints Description <code>team_id</code> VARCHAR(255) PRIMARY KEY Team identifier <code>organization_id</code> VARCHAR(255) FOREIGN KEY \u2192 organizations Parent organization <code>credits_allocated</code> INTEGER DEFAULT 0 Total credits allocated to team <code>credits_used</code> INTEGER DEFAULT 0 Credits consumed by completed jobs <code>credits_remaining</code> INTEGER COMPUTED STORED Auto-calculated: allocated - used <code>credit_limit</code> INTEGER NULLABLE Optional hard limit (NULL = unlimited) <code>auto_refill</code> BOOLEAN DEFAULT FALSE Enable automatic credit refills <code>refill_amount</code> INTEGER NULLABLE Amount to refill when auto_refill enabled <code>refill_period</code> VARCHAR(50) NULLABLE Refill frequency: 'daily', 'weekly', 'monthly' <code>last_refill_at</code> TIMESTAMP NULLABLE Last automatic refill timestamp <code>virtual_key</code> VARCHAR(500) NULLABLE LiteLLM virtual API key for this team <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Creation timestamp <code>updated_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Last update timestamp <p>Indexes:</p> <ul> <li><code>idx_team_credits_org</code> on <code>(organization_id)</code></li> <li><code>idx_team_credits_remaining</code> on <code>(credits_remaining)</code></li> <li><code>idx_team_credits_virtual_key</code> on <code>(virtual_key)</code></li> </ul> <p>Computed Column:</p> <p>The <code>credits_remaining</code> column is a generated stored column:</p> <pre><code>credits_remaining INTEGER GENERATED ALWAYS AS (credits_allocated - credits_used) STORED\n</code></pre> <p>This ensures consistency and prevents manual manipulation.</p> <p>Triggers:</p> <ul> <li><code>update_team_credits_updated_at</code> - Auto-updates <code>updated_at</code> on modification</li> </ul> <p>Budget Modes:</p> <ol> <li>Fixed Budget: <code>credit_limit</code> set, <code>auto_refill = FALSE</code></li> <li>Team has a fixed allocation</li> <li> <p>Cannot use more than <code>credits_allocated</code></p> </li> <li> <p>Unlimited Budget: <code>credit_limit = NULL</code></p> </li> <li>No hard limit on usage</li> <li> <p>Useful for enterprise customers</p> </li> <li> <p>Recurring Budget: <code>auto_refill = TRUE</code>, <code>refill_amount</code> and <code>refill_period</code> set</p> </li> <li>Automatically refills credits on schedule</li> <li>Useful for subscription-based billing</li> </ol> <p>Example:</p> <pre><code>-- Create team with 1000 credits\nINSERT INTO team_credits (team_id, organization_id, credits_allocated, virtual_key)\nVALUES ('team-alpha', 'org-acme', 1000, 'sk-xxxxxxxxxxxxx');\n\n-- Setup monthly auto-refill\nUPDATE team_credits\nSET auto_refill = TRUE,\n    refill_amount = 1000,\n    refill_period = 'monthly',\n    last_refill_at = NOW()\nWHERE team_id = 'team-alpha';\n</code></pre>"},{"location":"reference/database-schema/#credit_transactions","title":"credit_transactions","text":"<p>Audit log of all credit operations (allocations, deductions, refunds, adjustments).</p> <p>Columns:</p> Column Type Constraints Description <code>transaction_id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique transaction identifier <code>team_id</code> VARCHAR(255) NOT NULL Team the transaction applies to <code>organization_id</code> VARCHAR(255) NULLABLE Organization for reporting <code>job_id</code> UUID FOREIGN KEY \u2192 jobs, NULLABLE Associated job (for deductions) <code>transaction_type</code> VARCHAR(50) NOT NULL Type: 'deduction', 'allocation', 'refund', 'adjustment' <code>credits_amount</code> INTEGER NOT NULL Number of credits (positive for additions, negative for deductions) <code>credits_before</code> INTEGER NOT NULL Balance before transaction <code>credits_after</code> INTEGER NOT NULL Balance after transaction <code>reason</code> VARCHAR(500) NULLABLE Human-readable reason for transaction <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Transaction timestamp <p>Indexes:</p> <ul> <li><code>idx_credit_transactions_team</code> on <code>(team_id, created_at)</code></li> <li><code>idx_credit_transactions_job</code> on <code>(job_id)</code></li> <li><code>idx_credit_transactions_org</code> on <code>(organization_id, created_at)</code></li> <li><code>idx_credit_transactions_type</code> on <code>(transaction_type)</code></li> </ul> <p>Transaction Types:</p> <ul> <li>deduction: Credit removed for successful job completion</li> <li>allocation: New credits added to team</li> <li>refund: Credit returned (e.g., job failed after deduction)</li> <li>adjustment: Manual credit correction by admin</li> </ul> <p>Example:</p> <pre><code>-- Deduction for completed job\nINSERT INTO credit_transactions (\n    team_id, organization_id, job_id, transaction_type,\n    credits_amount, credits_before, credits_after, reason\n) VALUES (\n    'team-alpha', 'org-acme', 'job-uuid-123', 'deduction',\n    1, 1000, 999, 'Job document_analysis completed successfully'\n);\n</code></pre>"},{"location":"reference/database-schema/#jobs","title":"jobs","text":"<p>Represents a business operation that may involve one or more LLM calls. Jobs are the core billing unit.</p> <p>Columns:</p> Column Type Constraints Description <code>job_id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique job identifier <code>team_id</code> VARCHAR(255) NOT NULL, INDEX Team that owns this job <code>user_id</code> VARCHAR(255) NULLABLE, INDEX User who initiated the job <code>job_type</code> VARCHAR(100) NOT NULL, INDEX Job category (e.g., 'chat', 'document_analysis') <code>status</code> job_status NOT NULL, DEFAULT 'pending', INDEX Current status (enum) <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW(), INDEX Job creation time <code>started_at</code> TIMESTAMP NULLABLE When job moved to 'in_progress' <code>completed_at</code> TIMESTAMP NULLABLE When job finished (success or failure) <code>job_metadata</code> JSONB DEFAULT '{}' Custom application-specific data <code>error_message</code> VARCHAR(1000) NULLABLE Error description if failed <code>organization_id</code> VARCHAR(255) FOREIGN KEY \u2192 organizations, INDEX Parent organization <code>external_task_id</code> VARCHAR(255) NULLABLE, INDEX Reference to external system task ID <code>credit_applied</code> BOOLEAN DEFAULT FALSE Whether credit was deducted <code>model_groups_used</code> TEXT[] DEFAULT '{}' Array of model group names used <p>Status Enum:</p> <pre><code>CREATE TYPE job_status AS ENUM (\n    'pending',      -- Job created but not started\n    'in_progress',  -- Job has started processing\n    'completed',    -- Job finished successfully\n    'failed',       -- Job encountered errors\n    'cancelled'     -- Job was cancelled by user\n);\n</code></pre> <p>Indexes:</p> <ul> <li><code>idx_team_created</code> on <code>(team_id, created_at)</code></li> <li><code>idx_team_status</code> on <code>(team_id, status)</code></li> <li><code>idx_job_type_created</code> on <code>(job_type, created_at)</code></li> <li><code>idx_status</code> on <code>(status)</code></li> <li><code>idx_jobs_organization</code> on <code>(organization_id, created_at)</code></li> <li><code>idx_jobs_external_task</code> on <code>(external_task_id)</code></li> <li><code>idx_jobs_credit_applied</code> on <code>(credit_applied)</code></li> </ul> <p>Credit Deduction Rules:</p> <p>Credits are deducted when: 1. <code>status = 'completed'</code> (not 'failed' or 'cancelled') 2. All associated LLM calls succeeded (no errors) 3. <code>credit_applied = FALSE</code> (prevents double-charging)</p> <p>Example:</p> <pre><code>-- Create job\nINSERT INTO jobs (team_id, job_type, organization_id, external_task_id, job_metadata)\nVALUES (\n    'team-alpha',\n    'resume_analysis',\n    'org-acme',\n    'task-789',\n    '{\"resume_id\": \"res-456\", \"priority\": \"high\"}'\n);\n\n-- Update to in_progress\nUPDATE jobs\nSET status = 'in_progress', started_at = NOW()\nWHERE job_id = 'job-uuid-123';\n\n-- Complete job\nUPDATE jobs\nSET status = 'completed',\n    completed_at = NOW(),\n    credit_applied = TRUE\nWHERE job_id = 'job-uuid-123';\n</code></pre>"},{"location":"reference/database-schema/#llm_calls","title":"llm_calls","text":"<p>Individual LLM API calls within a job. Tracks costs, performance, and usage metrics.</p> <p>Columns:</p> Column Type Constraints Description <code>call_id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique call identifier <code>job_id</code> UUID FOREIGN KEY \u2192 jobs ON DELETE CASCADE, INDEX Parent job <code>litellm_request_id</code> VARCHAR(255) UNIQUE, NULLABLE LiteLLM's internal request ID <code>model_used</code> VARCHAR(100) NULLABLE Actual model that handled the request <code>prompt_tokens</code> INTEGER DEFAULT 0 Input tokens consumed <code>completion_tokens</code> INTEGER DEFAULT 0 Output tokens generated <code>total_tokens</code> INTEGER DEFAULT 0 Sum of prompt + completion tokens <code>cost_usd</code> NUMERIC(10,6) DEFAULT 0.0 Actual USD cost from LLM provider <code>latency_ms</code> INTEGER NULLABLE Request latency in milliseconds <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW(), INDEX Call timestamp <code>purpose</code> VARCHAR(200) NULLABLE Description of call purpose <code>request_data</code> JSONB NULLABLE Full request payload (for debugging) <code>response_data</code> JSONB NULLABLE Full response payload (for debugging) <code>error</code> VARCHAR(1000) NULLABLE Error message if call failed <code>model_group_used</code> VARCHAR(100) NULLABLE, INDEX Model group name requested <code>resolved_model</code> VARCHAR(200) NULLABLE, INDEX Primary model resolved from group <p>Indexes:</p> <ul> <li><code>idx_job_id_created</code> on <code>(job_id, created_at)</code></li> <li><code>idx_created_at</code> on <code>(created_at)</code></li> <li><code>idx_llm_calls_model_group</code> on <code>(model_group_used)</code></li> <li><code>idx_llm_calls_resolved_model</code> on <code>(resolved_model)</code></li> </ul> <p>Cost Calculation:</p> <p>The <code>cost_usd</code> field stores the actual cost charged by the LLM provider (via LiteLLM). This is separate from the credit system (1 credit per job), allowing you to track both: - Internal billing (credits) - External costs (actual USD from OpenAI/Anthropic/etc.)</p> <p>Example:</p> <pre><code>INSERT INTO llm_calls (\n    job_id, model_used, model_group_used, resolved_model,\n    prompt_tokens, completion_tokens, total_tokens,\n    cost_usd, latency_ms, purpose\n) VALUES (\n    'job-uuid-123',\n    'gpt-4-turbo-2024-04-09',\n    'ResumeAgent',\n    'gpt-4-turbo',\n    1250,\n    450,\n    1700,\n    0.034000,\n    2340,\n    'Resume skills extraction'\n);\n</code></pre>"},{"location":"reference/database-schema/#job_cost_summaries","title":"job_cost_summaries","text":"<p>Aggregated cost and performance metrics per job. Generated when job completes.</p> <p>Columns:</p> Column Type Constraints Description <code>job_id</code> UUID PRIMARY KEY, FOREIGN KEY \u2192 jobs ON DELETE CASCADE Job identifier <code>total_calls</code> INTEGER DEFAULT 0 Total number of LLM calls <code>successful_calls</code> INTEGER DEFAULT 0 Calls that succeeded <code>failed_calls</code> INTEGER DEFAULT 0 Calls that failed with errors <code>total_prompt_tokens</code> INTEGER DEFAULT 0 Sum of all prompt tokens <code>total_completion_tokens</code> INTEGER DEFAULT 0 Sum of all completion tokens <code>total_tokens</code> INTEGER DEFAULT 0 Sum of all tokens <code>total_cost_usd</code> NUMERIC(12,6) DEFAULT 0.0 Total actual USD cost <code>avg_latency_ms</code> INTEGER NULLABLE Average latency across all calls <code>total_duration_seconds</code> INTEGER NULLABLE Job duration (completed_at - created_at) <code>calculated_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Summary calculation timestamp <p>Example:</p> <pre><code>INSERT INTO job_cost_summaries (\n    job_id, total_calls, successful_calls, failed_calls,\n    total_prompt_tokens, total_completion_tokens, total_tokens,\n    total_cost_usd, avg_latency_ms, total_duration_seconds\n) VALUES (\n    'job-uuid-123',\n    3, 3, 0,\n    3750, 1350, 5100,\n    0.102000,\n    2180,\n    15\n);\n</code></pre>"},{"location":"reference/database-schema/#team_usage_summaries","title":"team_usage_summaries","text":"<p>Aggregated team usage analytics by period (daily or monthly).</p> <p>Columns:</p> Column Type Constraints Description <code>id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Summary identifier <code>team_id</code> VARCHAR(255) NOT NULL, INDEX Team identifier <code>period</code> VARCHAR(50) NOT NULL Period string: \"2024-10\" or \"2024-10-15\" <code>period_type</code> VARCHAR(20) DEFAULT 'monthly' Type: 'daily' or 'monthly' <code>total_jobs</code> INTEGER DEFAULT 0 Total jobs in period <code>successful_jobs</code> INTEGER DEFAULT 0 Completed jobs <code>failed_jobs</code> INTEGER DEFAULT 0 Failed jobs <code>cancelled_jobs</code> INTEGER DEFAULT 0 Cancelled jobs <code>total_cost_usd</code> NUMERIC(12,2) DEFAULT 0.0 Total actual costs <code>total_tokens</code> INTEGER DEFAULT 0 Total tokens consumed <code>job_type_breakdown</code> JSONB DEFAULT '{}' Per-job-type statistics <code>calculated_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Calculation timestamp <p>Indexes:</p> <ul> <li><code>idx_team_period</code> on <code>(team_id, period)</code> UNIQUE</li> </ul> <p>Job Type Breakdown Format:</p> <pre><code>{\n  \"resume_analysis\": {\n    \"count\": 150,\n    \"cost_usd\": 12.45\n  },\n  \"document_parsing\": {\n    \"count\": 320,\n    \"cost_usd\": 8.90\n  }\n}\n</code></pre> <p>Example:</p> <pre><code>INSERT INTO team_usage_summaries (\n    team_id, period, period_type,\n    total_jobs, successful_jobs, failed_jobs,\n    total_cost_usd, total_tokens, job_type_breakdown\n) VALUES (\n    'team-alpha',\n    '2024-10',\n    'monthly',\n    470, 455, 15,\n    21.35,\n    1250000,\n    '{\"resume_analysis\": {\"count\": 150, \"cost_usd\": 12.45}, \"document_parsing\": {\"count\": 320, \"cost_usd\": 8.90}}'\n);\n</code></pre>"},{"location":"reference/database-schema/#model-group-tables","title":"Model Group Tables","text":""},{"location":"reference/database-schema/#model_groups","title":"model_groups","text":"<p>Defines named groups of models with primary and fallback configurations (e.g., \"ResumeAgent\", \"ParsingAgent\").</p> <p>Columns:</p> Column Type Constraints Description <code>model_group_id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique group identifier <code>group_name</code> VARCHAR(100) UNIQUE, NOT NULL Group name (e.g., \"ResumeAgent\") <code>display_name</code> VARCHAR(200) NULLABLE Human-readable name <code>description</code> TEXT NULLABLE Group description <code>status</code> VARCHAR(50) DEFAULT 'active' Status: active, inactive <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Creation timestamp <code>updated_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Last update timestamp <p>Indexes:</p> <ul> <li><code>idx_model_group_name</code> on <code>(group_name)</code></li> <li><code>idx_model_group_status</code> on <code>(status)</code></li> </ul> <p>Triggers:</p> <ul> <li><code>update_model_groups_updated_at</code> - Auto-updates <code>updated_at</code></li> </ul> <p>Example:</p> <pre><code>INSERT INTO model_groups (group_name, display_name, description)\nVALUES (\n    'ResumeAgent',\n    'Resume Analysis Agent',\n    'High-quality model for resume parsing and skills extraction'\n);\n</code></pre>"},{"location":"reference/database-schema/#model_group_models","title":"model_group_models","text":"<p>Maps models to groups with priority for fallback handling.</p> <p>Columns:</p> Column Type Constraints Description <code>id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique mapping identifier <code>model_group_id</code> UUID FOREIGN KEY \u2192 model_groups ON DELETE CASCADE, NOT NULL Parent group <code>model_name</code> VARCHAR(200) NOT NULL LiteLLM model identifier <code>priority</code> INTEGER DEFAULT 0 0 = primary, 1 = first fallback, 2 = second fallback <code>is_active</code> BOOLEAN DEFAULT TRUE Whether this model is currently enabled <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Creation timestamp <p>Indexes:</p> <ul> <li><code>idx_model_group_models_lookup</code> on <code>(model_group_id, priority, is_active)</code></li> <li><code>idx_model_group_models_group</code> on <code>(model_group_id)</code></li> </ul> <p>Priority System:</p> <ul> <li>Priority 0: Primary model (first choice)</li> <li>Priority 1: First fallback (if primary fails or unavailable)</li> <li>Priority 2: Second fallback</li> <li>Priority N: Nth fallback</li> </ul> <p>Example:</p> <pre><code>-- Add primary model\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES ('group-uuid-123', 'gpt-4-turbo', 0);\n\n-- Add fallbacks\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES\n    ('group-uuid-123', 'gpt-4', 1),\n    ('group-uuid-123', 'gpt-3.5-turbo', 2);\n</code></pre>"},{"location":"reference/database-schema/#team_model_groups","title":"team_model_groups","text":"<p>Junction table mapping teams to their assigned model groups (access control).</p> <p>Columns:</p> Column Type Constraints Description <code>id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique assignment identifier <code>team_id</code> VARCHAR(255) NOT NULL Team identifier <code>model_group_id</code> UUID FOREIGN KEY \u2192 model_groups ON DELETE CASCADE, NOT NULL Model group <code>assigned_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Assignment timestamp <p>Indexes:</p> <ul> <li><code>idx_team_model_groups_team</code> on <code>(team_id)</code></li> <li><code>idx_team_model_groups_group</code> on <code>(model_group_id)</code></li> </ul> <p>Constraints:</p> <ul> <li><code>unique_team_model_group</code> UNIQUE on <code>(team_id, model_group_id)</code> - Prevents duplicate assignments</li> </ul> <p>Example:</p> <pre><code>-- Assign model group to team\nINSERT INTO team_model_groups (team_id, model_group_id)\nVALUES ('team-alpha', 'group-uuid-123');\n</code></pre>"},{"location":"reference/database-schema/#additional-tables","title":"Additional Tables","text":""},{"location":"reference/database-schema/#webhook_registrations","title":"webhook_registrations","text":"<p>Webhook endpoints for job event notifications.</p> <p>Columns:</p> Column Type Constraints Description <code>webhook_id</code> UUID PRIMARY KEY, DEFAULT gen_random_uuid() Unique webhook identifier <code>team_id</code> VARCHAR(255) NOT NULL, INDEX Team that owns webhook <code>webhook_url</code> VARCHAR(500) NOT NULL Webhook endpoint URL <code>events</code> JSONB DEFAULT '[]' Array of event types to trigger on <code>is_active</code> INTEGER DEFAULT 1 Active status (1=active, 0=inactive) <code>created_at</code> TIMESTAMP NOT NULL, DEFAULT NOW() Creation timestamp <code>last_triggered_at</code> TIMESTAMP NULLABLE Last successful trigger <code>auth_header</code> VARCHAR(500) NULLABLE Optional authentication header <p>Indexes:</p> <ul> <li><code>idx_webhook_team_id</code> on <code>(team_id)</code></li> <li><code>idx_webhook_active</code> on <code>(is_active)</code></li> </ul> <p>Event Types:</p> <ul> <li><code>job.created</code></li> <li><code>job.started</code></li> <li><code>job.completed</code></li> <li><code>job.failed</code></li> <li><code>job.cancelled</code></li> </ul> <p>Example:</p> <pre><code>INSERT INTO webhook_registrations (team_id, webhook_url, events, auth_header)\nVALUES (\n    'team-alpha',\n    'https://api.acme.com/webhooks/llm-jobs',\n    '[\"job.completed\", \"job.failed\"]',\n    'Bearer secret-webhook-token-xyz'\n);\n</code></pre>"},{"location":"reference/database-schema/#relationships","title":"Relationships","text":""},{"location":"reference/database-schema/#foreign-key-constraints","title":"Foreign Key Constraints","text":"<pre><code>-- Organizations \u2192 Team Credits\nALTER TABLE team_credits\n    ADD CONSTRAINT fk_team_credits_organization\n    FOREIGN KEY (organization_id) REFERENCES organizations(organization_id);\n\n-- Organizations \u2192 Jobs\nALTER TABLE jobs\n    ADD CONSTRAINT fk_jobs_organization\n    FOREIGN KEY (organization_id) REFERENCES organizations(organization_id);\n\n-- Jobs \u2192 LLM Calls (cascade delete)\nALTER TABLE llm_calls\n    ADD CONSTRAINT fk_llm_calls_job\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE;\n\n-- Jobs \u2192 Job Cost Summaries (cascade delete)\nALTER TABLE job_cost_summaries\n    ADD CONSTRAINT fk_job_cost_summaries_job\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE;\n\n-- Jobs \u2192 Credit Transactions\nALTER TABLE credit_transactions\n    ADD CONSTRAINT fk_credit_transactions_job\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id);\n\n-- Model Groups \u2192 Model Group Models (cascade delete)\nALTER TABLE model_group_models\n    ADD CONSTRAINT fk_model_group_models_group\n    FOREIGN KEY (model_group_id) REFERENCES model_groups(model_group_id) ON DELETE CASCADE;\n\n-- Model Groups \u2192 Team Model Groups (cascade delete)\nALTER TABLE team_model_groups\n    ADD CONSTRAINT fk_team_model_groups_group\n    FOREIGN KEY (model_group_id) REFERENCES model_groups(model_group_id) ON DELETE CASCADE;\n</code></pre>"},{"location":"reference/database-schema/#cascade-behavior","title":"Cascade Behavior","text":"<ul> <li>Jobs deleted \u2192 LLM calls and cost summaries auto-deleted (cascade)</li> <li>Model groups deleted \u2192 Model assignments and team mappings auto-deleted (cascade)</li> <li>Organizations deleted \u2192 No cascade (must manually clean up teams first)</li> </ul>"},{"location":"reference/database-schema/#migration-scripts","title":"Migration Scripts","text":"<p>Migrations are located in <code>/scripts/migrations/</code> and should be run in order:</p> <ol> <li><code>001_create_job_tracking_tables.sql</code> - Core job tracking tables</li> <li><code>002_create_organizations.sql</code> - Organization hierarchy</li> <li><code>003_create_model_groups.sql</code> - Model group system</li> <li><code>004_create_team_model_groups.sql</code> - Team-model assignments</li> <li><code>005_create_credits_tables.sql</code> - Credit system</li> <li><code>006_extend_jobs_and_llm_calls.sql</code> - Add organization and model group fields</li> <li><code>007_add_virtual_key_to_team_credits.sql</code> - Add virtual key column</li> </ol> <p>Running Migrations:</p> <pre><code># Run all migrations\npsql -U postgres -d saas_litellm -f scripts/migrations/001_create_job_tracking_tables.sql\npsql -U postgres -d saas_litellm -f scripts/migrations/002_create_organizations.sql\n# ... etc\n</code></pre>"},{"location":"reference/database-schema/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/database-schema/#index-strategy","title":"Index Strategy","text":"<p>All high-traffic query patterns are covered by indexes:</p> <ul> <li>Team-based queries: <code>idx_team_created</code>, <code>idx_team_status</code></li> <li>Time-range queries: <code>idx_created_at</code>, <code>idx_org_created</code></li> <li>Job type analytics: <code>idx_job_type_created</code></li> <li>Credit lookups: <code>idx_team_credits_remaining</code>, <code>idx_team_credits_virtual_key</code></li> <li>Model group resolution: <code>idx_model_group_models_lookup</code></li> </ul>"},{"location":"reference/database-schema/#query-optimization-tips","title":"Query Optimization Tips","text":"<p>Get team jobs with costs:</p> <pre><code>SELECT\n    j.*,\n    jcs.total_cost_usd,\n    jcs.total_tokens\nFROM jobs j\nLEFT JOIN job_cost_summaries jcs ON j.job_id = jcs.job_id\nWHERE j.team_id = 'team-alpha'\nAND j.created_at &gt;= '2024-10-01'\nORDER BY j.created_at DESC\nLIMIT 100;\n</code></pre> <p>Get team credit balance:</p> <pre><code>SELECT\n    credits_allocated,\n    credits_used,\n    credits_remaining,\n    virtual_key\nFROM team_credits\nWHERE team_id = 'team-alpha';\n</code></pre> <p>Resolve model group:</p> <pre><code>SELECT\n    mgm.model_name,\n    mgm.priority\nFROM model_groups mg\nJOIN model_group_models mgm ON mg.model_group_id = mgm.model_group_id\nJOIN team_model_groups tmg ON mg.model_group_id = tmg.model_group_id\nWHERE tmg.team_id = 'team-alpha'\nAND mg.group_name = 'ResumeAgent'\nAND mgm.is_active = TRUE\nORDER BY mgm.priority ASC;\n</code></pre>"},{"location":"reference/database-schema/#backup-and-maintenance","title":"Backup and Maintenance","text":""},{"location":"reference/database-schema/#recommended-backup-strategy","title":"Recommended Backup Strategy","text":"<pre><code># Full database backup\npg_dump -U postgres saas_litellm &gt; backup_$(date +%Y%m%d).sql\n\n# Backup specific tables\npg_dump -U postgres -t credit_transactions -t team_credits saas_litellm &gt; credits_backup.sql\n</code></pre>"},{"location":"reference/database-schema/#data-retention","title":"Data Retention","text":"<p>Consider archiving old records:</p> <ul> <li>llm_calls: Archive calls older than 90 days to separate table</li> <li>credit_transactions: Keep all records for audit compliance</li> <li>jobs: Archive completed jobs older than 1 year</li> <li>team_usage_summaries: Keep all summaries (small table)</li> </ul>"},{"location":"reference/database-schema/#vacuum-and-analyze","title":"Vacuum and Analyze","text":"<pre><code>-- Regular maintenance\nVACUUM ANALYZE jobs;\nVACUUM ANALYZE llm_calls;\nVACUUM ANALYZE credit_transactions;\n</code></pre>"},{"location":"reference/database-schema/#see-also","title":"See Also","text":"<ul> <li>Credit System Reference - Detailed credit allocation and deduction logic</li> <li>Model Resolution Reference - Model group resolution flow</li> <li>API Reference - API endpoints that use these tables</li> </ul>"},{"location":"reference/model-resolution/","title":"Model Resolution Reference","text":"<p>Complete technical reference for the model group resolution system in SaaS LiteLLM. This system allows you to define named model groups (like \"ResumeAgent\", \"ParsingAgent\") with primary models and fallbacks, then control which teams have access to which groups.</p>"},{"location":"reference/model-resolution/#overview","title":"Overview","text":"<p>The model resolution system provides:</p> <ul> <li>Named Model Groups: Abstract model names from implementation (e.g., \"ResumeAgent\" instead of \"gpt-4-turbo\")</li> <li>Primary + Fallback Models: Define fallback chains for reliability</li> <li>Access Control: Per-team permissions for model groups</li> <li>Easy Model Updates: Change underlying models without code changes</li> <li>Model Versioning: Switch between model versions centrally</li> </ul>"},{"location":"reference/model-resolution/#core-concepts","title":"Core Concepts","text":""},{"location":"reference/model-resolution/#model-groups","title":"Model Groups","text":"<p>A model group is a named collection of models with a priority order:</p> <pre><code>ResumeAgent:\n  Priority 0 (Primary): gpt-4-turbo\n  Priority 1 (Fallback): gpt-4\n  Priority 2 (Fallback): gpt-3.5-turbo\n</code></pre> <p>Your application requests <code>model_group=\"ResumeAgent\"</code>, and the system resolves it to the actual model.</p>"},{"location":"reference/model-resolution/#why-model-groups","title":"Why Model Groups?","text":"<p>Problem: Hard-coding model names creates maintenance nightmares:</p> <pre><code># \u274c Bad - Hard-coded model names\nresponse = litellm.completion(\n    model=\"gpt-4-turbo-2024-04-09\",  # What if we want to change this?\n    messages=messages\n)\n</code></pre> <p>Solution: Use model groups:</p> <pre><code># \u2705 Good - Abstract model group\nresponse = saas_api.llm_call(\n    job_id=job_id,\n    model_group=\"ResumeAgent\",  # Resolved to actual model by system\n    messages=messages\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Update models centrally in database (no code changes)</li> <li>Different teams can use different models for same group</li> <li>Easy A/B testing of models</li> <li>Rollback model changes instantly</li> <li>Add fallbacks without code changes</li> </ul>"},{"location":"reference/model-resolution/#database-schema","title":"Database Schema","text":""},{"location":"reference/model-resolution/#model_groups-table","title":"model_groups Table","text":"<p>Defines named model groups.</p> <pre><code>CREATE TABLE model_groups (\n    model_group_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    group_name VARCHAR(100) UNIQUE NOT NULL,\n    display_name VARCHAR(200),\n    description TEXT,\n    status VARCHAR(50) DEFAULT 'active',\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n</code></pre> <p>Columns:</p> <ul> <li><code>model_group_id</code>: Unique identifier</li> <li><code>group_name</code>: Code-friendly name (e.g., \"ResumeAgent\")</li> <li><code>display_name</code>: Human-readable name (e.g., \"Resume Analysis Agent\")</li> <li><code>description</code>: Purpose and usage notes</li> <li><code>status</code>: <code>active</code> or <code>inactive</code> (inactive groups can't be used)</li> </ul> <p>Example:</p> <pre><code>INSERT INTO model_groups (group_name, display_name, description)\nVALUES (\n    'ResumeAgent',\n    'Resume Analysis Agent',\n    'High-quality model for resume parsing, skills extraction, and job matching'\n);\n</code></pre>"},{"location":"reference/model-resolution/#model_group_models-table","title":"model_group_models Table","text":"<p>Maps models to groups with priority for fallback handling.</p> <pre><code>CREATE TABLE model_group_models (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    model_group_id UUID NOT NULL REFERENCES model_groups(model_group_id) ON DELETE CASCADE,\n    model_name VARCHAR(200) NOT NULL,\n    priority INTEGER DEFAULT 0,\n    is_active BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nCREATE INDEX idx_model_group_models_lookup\n    ON model_group_models(model_group_id, priority, is_active);\n</code></pre> <p>Columns:</p> <ul> <li><code>model_group_id</code>: Parent group</li> <li><code>model_name</code>: LiteLLM model identifier (e.g., \"gpt-4-turbo\", \"claude-3-opus\")</li> <li><code>priority</code>: 0 = primary, 1 = first fallback, 2 = second fallback, etc.</li> <li><code>is_active</code>: Whether this model is enabled (allows disabling without deletion)</li> </ul> <p>Priority System:</p> <ul> <li>Priority 0: Primary model (always tried first)</li> <li>Priority 1: First fallback (used if primary fails or unavailable)</li> <li>Priority 2+: Additional fallbacks</li> </ul> <p>Example:</p> <pre><code>-- Get model group ID\nSELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent';\n-- Returns: 'a1b2c3d4-...'\n\n-- Add models with priorities\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES\n    ('a1b2c3d4-...', 'gpt-4-turbo', 0),        -- Primary\n    ('a1b2c3d4-...', 'gpt-4', 1),              -- First fallback\n    ('a1b2c3d4-...', 'gpt-3.5-turbo', 2);      -- Second fallback\n</code></pre>"},{"location":"reference/model-resolution/#team_model_groups-table","title":"team_model_groups Table","text":"<p>Junction table controlling which teams have access to which model groups.</p> <pre><code>CREATE TABLE team_model_groups (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    team_id VARCHAR(255) NOT NULL,\n    model_group_id UUID NOT NULL REFERENCES model_groups(model_group_id) ON DELETE CASCADE,\n    assigned_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    CONSTRAINT unique_team_model_group UNIQUE(team_id, model_group_id)\n);\n\nCREATE INDEX idx_team_model_groups_team ON team_model_groups(team_id);\nCREATE INDEX idx_team_model_groups_group ON team_model_groups(model_group_id);\n</code></pre> <p>Columns:</p> <ul> <li><code>team_id</code>: Team identifier</li> <li><code>model_group_id</code>: Model group this team can access</li> <li><code>assigned_at</code>: When access was granted</li> </ul> <p>Example:</p> <pre><code>-- Grant team access to model group\nINSERT INTO team_model_groups (team_id, model_group_id)\nSELECT 'team-alpha', model_group_id\nFROM model_groups\nWHERE group_name = 'ResumeAgent';\n\n-- Grant access to multiple groups\nINSERT INTO team_model_groups (team_id, model_group_id)\nSELECT 'team-alpha', model_group_id\nFROM model_groups\nWHERE group_name IN ('ResumeAgent', 'ParsingAgent', 'ChatAgent');\n</code></pre>"},{"location":"reference/model-resolution/#model-resolver-service","title":"Model Resolver Service","text":"<p>The <code>ModelResolver</code> service handles model group resolution and access control.</p>"},{"location":"reference/model-resolution/#location","title":"Location","text":"<p><code>src/services/model_resolver.py</code></p>"},{"location":"reference/model-resolution/#class-definition","title":"Class Definition","text":"<pre><code>class ModelResolver:\n    def __init__(self, db: Session):\n        self.db = db\n</code></pre>"},{"location":"reference/model-resolution/#key-methods","title":"Key Methods","text":""},{"location":"reference/model-resolution/#1-verify_team_access","title":"1. verify_team_access()","text":"<p>Check if a team has access to a specific model group.</p> <pre><code>def verify_team_access(self, team_id: str, model_group_name: str) -&gt; bool:\n    # Get model group\n    model_group = self.db.query(ModelGroup).filter(\n        ModelGroup.group_name == model_group_name\n    ).first()\n\n    if not model_group:\n        return False\n\n    # Check if team has this group assigned\n    assignment = self.db.query(TeamModelGroup).filter(\n        and_(\n            TeamModelGroup.team_id == team_id,\n            TeamModelGroup.model_group_id == model_group.model_group_id\n        )\n    ).first()\n\n    return assignment is not None\n</code></pre> <p>Returns: <code>True</code> if team has access, <code>False</code> otherwise</p> <p>Example:</p> <pre><code>resolver = ModelResolver(db)\n\nif resolver.verify_team_access(\"team-alpha\", \"ResumeAgent\"):\n    print(\"Access granted\")\nelse:\n    print(\"Access denied\")\n</code></pre>"},{"location":"reference/model-resolution/#2-resolve_model_group","title":"2. resolve_model_group()","text":"<p>Resolve a model group name to actual model(s).</p> <pre><code>def resolve_model_group(\n    self,\n    team_id: str,\n    model_group_name: str,\n    include_fallbacks: bool = True\n) -&gt; Tuple[str, List[str]]:\n</code></pre> <p>Parameters:</p> <ul> <li><code>team_id</code>: Team requesting the model</li> <li><code>model_group_name</code>: Name of model group (e.g., \"ResumeAgent\")</li> <li><code>include_fallbacks</code>: Whether to return fallback models (default: <code>True</code>)</li> </ul> <p>Returns: Tuple of <code>(primary_model, [fallback_models])</code></p> <p>Raises: <code>ModelResolutionError</code> if: - Team doesn't have access to group - Model group not found or inactive - No active models configured for group</p> <p>Implementation:</p> <pre><code>def resolve_model_group(\n    self,\n    team_id: str,\n    model_group_name: str,\n    include_fallbacks: bool = True\n) -&gt; Tuple[str, List[str]]:\n    # 1. Verify access\n    if not self.verify_team_access(team_id, model_group_name):\n        raise ModelResolutionError(\n            f\"Team '{team_id}' does not have access to model group '{model_group_name}'\"\n        )\n\n    # 2. Get model group\n    model_group = self.db.query(ModelGroup).filter(\n        and_(\n            ModelGroup.group_name == model_group_name,\n            ModelGroup.status == \"active\"\n        )\n    ).first()\n\n    if not model_group:\n        raise ModelResolutionError(\n            f\"Model group '{model_group_name}' not found or inactive\"\n        )\n\n    # 3. Get models sorted by priority\n    models = self.db.query(ModelGroupModel).filter(\n        and_(\n            ModelGroupModel.model_group_id == model_group.model_group_id,\n            ModelGroupModel.is_active == True\n        )\n    ).order_by(ModelGroupModel.priority).all()\n\n    if not models:\n        raise ModelResolutionError(\n            f\"No active models configured for group '{model_group_name}'\"\n        )\n\n    # 4. Return primary + fallbacks\n    primary_model = models[0].model_name\n    fallback_models = [m.model_name for m in models[1:]] if include_fallbacks else []\n\n    return primary_model, fallback_models\n</code></pre> <p>Example:</p> <pre><code>resolver = ModelResolver(db)\n\nprimary, fallbacks = resolver.resolve_model_group(\n    team_id=\"team-alpha\",\n    model_group_name=\"ResumeAgent\"\n)\n\nprint(f\"Primary: {primary}\")\n# Output: Primary: gpt-4-turbo\n\nprint(f\"Fallbacks: {fallbacks}\")\n# Output: Fallbacks: ['gpt-4', 'gpt-3.5-turbo']\n</code></pre>"},{"location":"reference/model-resolution/#3-get_model_group_by_name","title":"3. get_model_group_by_name()","text":"<p>Retrieve model group details.</p> <pre><code>def get_model_group_by_name(self, model_group_name: str) -&gt; Optional[ModelGroup]:\n    return self.db.query(ModelGroup).filter(\n        ModelGroup.group_name == model_group_name\n    ).first()\n</code></pre> <p>Returns: <code>ModelGroup</code> object or <code>None</code></p>"},{"location":"reference/model-resolution/#4-get_team_model_groups","title":"4. get_team_model_groups()","text":"<p>Get all model groups assigned to a team.</p> <pre><code>def get_team_model_groups(self, team_id: str) -&gt; List[ModelGroup]:\n    assignments = self.db.query(TeamModelGroup).filter(\n        TeamModelGroup.team_id == team_id\n    ).all()\n\n    model_group_ids = [a.model_group_id for a in assignments]\n\n    return self.db.query(ModelGroup).filter(\n        ModelGroup.model_group_id.in_(model_group_ids)\n    ).all()\n</code></pre> <p>Returns: List of <code>ModelGroup</code> objects</p> <p>Example:</p> <pre><code>resolver = ModelResolver(db)\n\ngroups = resolver.get_team_model_groups(\"team-alpha\")\n\nfor group in groups:\n    print(f\"- {group.group_name}: {group.display_name}\")\n\n# Output:\n# - ResumeAgent: Resume Analysis Agent\n# - ParsingAgent: Document Parsing Agent\n# - ChatAgent: General Chat Agent\n</code></pre>"},{"location":"reference/model-resolution/#resolution-flow","title":"Resolution Flow","text":""},{"location":"reference/model-resolution/#complete-resolution-process","title":"Complete Resolution Process","text":"<pre><code>sequenceDiagram\n    participant App as Application\n    participant API as SaaS API\n    participant Resolver as ModelResolver\n    participant DB as PostgreSQL\n    participant LLM as LiteLLM\n\n    App-&gt;&gt;API: POST /api/jobs/{job_id}/llm-call&lt;br/&gt;model_group=\"ResumeAgent\"\n    API-&gt;&gt;Resolver: resolve_model_group(team_id, \"ResumeAgent\")\n    Resolver-&gt;&gt;DB: Check team_model_groups\n    DB--&gt;&gt;Resolver: Access granted\n    Resolver-&gt;&gt;DB: Get model_groups WHERE group_name='ResumeAgent'\n    DB--&gt;&gt;Resolver: model_group_id, status='active'\n    Resolver-&gt;&gt;DB: Get model_group_models&lt;br/&gt;ORDER BY priority\n    DB--&gt;&gt;Resolver: [gpt-4-turbo, gpt-4, gpt-3.5-turbo]\n    Resolver--&gt;&gt;API: primary='gpt-4-turbo', fallbacks=['gpt-4', ...]\n    API-&gt;&gt;LLM: Call with model='gpt-4-turbo'\n    LLM--&gt;&gt;API: Response\n    API-&gt;&gt;DB: Record llm_call:&lt;br/&gt;model_group_used='ResumeAgent'&lt;br/&gt;resolved_model='gpt-4-turbo'\n    API--&gt;&gt;App: Response + metadata</code></pre>"},{"location":"reference/model-resolution/#step-by-step","title":"Step-by-Step","text":"<ol> <li>Application makes request with <code>model_group</code> parameter</li> <li>API receives request, extracts <code>team_id</code> from auth token</li> <li>Resolver checks access via <code>team_model_groups</code> table</li> <li>Access granted \u2192 Resolver fetches model group</li> <li>Get models ordered by priority (ascending)</li> <li>Return primary (priority 0) and fallbacks (priority 1+)</li> <li>API calls LiteLLM with resolved primary model</li> <li>Record call in database with both <code>model_group_used</code> and <code>resolved_model</code></li> <li>Return response to application</li> </ol>"},{"location":"reference/model-resolution/#api-integration","title":"API Integration","text":""},{"location":"reference/model-resolution/#make-llm-call-with-model-group","title":"Make LLM Call with Model Group","text":"<p>From <code>src/saas_api.py</code>:</p> <pre><code>@app.post(\"/api/jobs/{job_id}/llm-call\")\nasync def make_llm_call(\n    job_id: str,\n    request: LLMCallRequest,  # Contains model_group field\n    db: Session = Depends(get_db),\n    authenticated_team_id: str = Depends(verify_virtual_key)\n):\n    # Get job\n    job = db.query(Job).filter(Job.job_id == uuid.UUID(job_id)).first()\n\n    # Verify job belongs to team\n    if job.team_id != authenticated_team_id:\n        raise HTTPException(403, \"Job does not belong to your team\")\n\n    # Get team's virtual key\n    team_credits = db.query(TeamCredits).filter(\n        TeamCredits.team_id == job.team_id\n    ).first()\n\n    if not team_credits.virtual_key:\n        raise HTTPException(500, \"Team has no virtual key configured\")\n\n    # Resolve model group to actual model\n    model_resolver = ModelResolver(db)\n\n    try:\n        primary_model, fallback_models = model_resolver.resolve_model_group(\n            team_id=job.team_id,\n            model_group_name=request.model_group\n        )\n    except ModelResolutionError as e:\n        raise HTTPException(403, str(e))\n\n    # Track model group usage in job\n    if not job.model_groups_used:\n        job.model_groups_used = []\n    if request.model_group not in job.model_groups_used:\n        job.model_groups_used.append(request.model_group)\n        db.commit()\n\n    # Call LiteLLM with resolved model\n    litellm_response = await call_litellm(\n        model=primary_model,  # Use resolved primary model\n        messages=request.messages,\n        virtual_key=team_credits.virtual_key,\n        team_id=job.team_id,\n        temperature=request.temperature,\n        max_tokens=request.max_tokens\n    )\n\n    # Store LLM call record\n    llm_call = LLMCall(\n        job_id=job.job_id,\n        model_used=litellm_response.get(\"model\", primary_model),\n        model_group_used=request.model_group,  # Track requested group\n        resolved_model=primary_model,          # Track resolved model\n        prompt_tokens=usage.get(\"prompt_tokens\", 0),\n        completion_tokens=usage.get(\"completion_tokens\", 0),\n        total_tokens=usage.get(\"total_tokens\", 0),\n        cost_usd=cost_usd,\n        latency_ms=latency_ms\n    )\n\n    db.add(llm_call)\n    db.commit()\n\n    return response\n\n## Model Tracking Fields\n\n### Understanding the Three Model Fields\n\nThe `llm_calls` table tracks three different model identifiers to provide complete visibility into model resolution:\n\n| Field | Type | Description | Example Value |\n|-------|------|-------------|---------------|\n| `model_group_used` | string | The model group name requested by the application | `\"ResumeAgent\"` |\n| `resolved_model` | string | The model your system resolved to from the model group | `\"gpt-4-turbo\"` |\n| `model_used` | string | The actual model LiteLLM used (may differ if LiteLLM applies fallbacks) | `\"gpt-4-turbo-2024-04-09\"` |\n\n### Why Three Fields?\n\nThis three-tier tracking provides complete audit trail:\n\n**Example Scenario:**\n\n1. **Application requests** `model_group=\"ResumeAgent\"`\n2. **Your system resolves** to `resolved_model=\"gpt-4-turbo\"` (from model group configuration)\n3. **LiteLLM returns** `model=\"gpt-4-turbo-2024-04-09\"` (specific version/deployment)\n\n**Tracking All Three Allows:**\n\n- **`model_group_used`**: Track which logical model groups are most popular\n- **`resolved_model`**: Verify your resolution logic worked correctly\n- **`model_used`**: See the actual model/version LiteLLM used (important for cost tracking and debugging)\n\n### Typical Values\n\n```python\n# Example 1: Standard resolution\n{\n    \"model_group_used\": \"ResumeAgent\",      # What app requested\n    \"resolved_model\": \"gpt-4-turbo\",        # What you resolved to\n    \"model_used\": \"gpt-4-turbo-2024-04-09\"  # What LiteLLM used\n}\n\n# Example 2: Direct model request (not using model groups)\n{\n    \"model_group_used\": null,               # No group used\n    \"resolved_model\": \"gpt-4\",              # Direct model request\n    \"model_used\": \"gpt-4-0613\"              # Specific version used\n}\n\n# Example 3: Fallback scenario\n{\n    \"model_group_used\": \"ResumeAgent\",      # What app requested\n    \"resolved_model\": \"gpt-4-turbo\",        # What you resolved to\n    \"model_used\": \"gpt-4\"                   # LiteLLM fell back to this\n}\n</code></pre>"},{"location":"reference/model-resolution/#querying-model-usage","title":"Querying Model Usage","text":"<p>Track most-used model groups:</p> <pre><code>SELECT\n    model_group_used,\n    COUNT(*) as call_count,\n    AVG(cost_usd) as avg_cost,\n    AVG(latency_ms) as avg_latency\nFROM llm_calls\nWHERE model_group_used IS NOT NULL\nAND created_at &gt;= NOW() - INTERVAL '7 days'\nGROUP BY model_group_used\nORDER BY call_count DESC;\n</code></pre> <p>Verify resolution accuracy:</p> <pre><code>-- Check if resolved models match actual models used\nSELECT\n    model_group_used,\n    resolved_model,\n    model_used,\n    COUNT(*) as instances\nFROM llm_calls\nWHERE model_group_used IS NOT NULL\nAND created_at &gt;= NOW() - INTERVAL '1 day'\nGROUP BY model_group_used, resolved_model, model_used\nORDER BY instances DESC;\n</code></pre> <p>Detect unexpected fallbacks:</p> <p><pre><code>-- Find cases where LiteLLM used a different model than resolved\nSELECT\n    job_id,\n    model_group_used,\n    resolved_model,\n    model_used,\n    created_at\nFROM llm_calls\nWHERE resolved_model != model_used\nAND model_group_used IS NOT NULL\nORDER BY created_at DESC\nLIMIT 100;\n</code></pre> <pre><code>### Request Format\n\n```json\nPOST /api/jobs/{job_id}/llm-call\n{\n  \"model_group\": \"ResumeAgent\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Analyze this resume...\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 2000,\n  \"purpose\": \"Skills extraction\"\n}\n</code></pre></p>"},{"location":"reference/model-resolution/#response-format","title":"Response Format","text":"<pre><code>{\n  \"call_id\": \"call-uuid-123\",\n  \"response\": {\n    \"content\": \"This resume shows strong Python skills...\",\n    \"finish_reason\": \"stop\"\n  },\n  \"metadata\": {\n    \"tokens_used\": 1850,\n    \"latency_ms\": 2340,\n    \"model_group\": \"ResumeAgent\"\n  }\n}\n</code></pre> <p>Note: The actual resolved model is not exposed to the client (keeps implementation details hidden).</p>"},{"location":"reference/model-resolution/#error-handling","title":"Error Handling","text":""},{"location":"reference/model-resolution/#modelresolutionerror","title":"ModelResolutionError","text":"<p>Raised when resolution fails.</p> <p>Scenarios:</p> <ol> <li> <p>Team lacks access: <pre><code>ModelResolutionError: Team 'team-alpha' does not have access to model group 'ResumeAgent'\n</code></pre></p> </li> <li> <p>Model group not found: <pre><code>ModelResolutionError: Model group 'InvalidAgent' not found or inactive\n</code></pre></p> </li> <li> <p>No models configured: <pre><code>ModelResolutionError: No active models configured for group 'ResumeAgent'\n</code></pre></p> </li> </ol> <p>HTTP Response:</p> <pre><code>{\n  \"detail\": \"Team 'team-alpha' does not have access to model group 'ResumeAgent'\"\n}\n</code></pre> <p>Status code: <code>403 Forbidden</code></p>"},{"location":"reference/model-resolution/#common-operations","title":"Common Operations","text":""},{"location":"reference/model-resolution/#1-create-model-group","title":"1. Create Model Group","text":"<pre><code>-- Create group\nINSERT INTO model_groups (group_name, display_name, description)\nVALUES (\n    'ChatAgent',\n    'General Chat Agent',\n    'Conversational AI for customer support and general queries'\n)\nRETURNING model_group_id;\n-- Returns: 'xyz-uuid-789'\n\n-- Add models\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES\n    ('xyz-uuid-789', 'gpt-4-turbo', 0),\n    ('xyz-uuid-789', 'gpt-3.5-turbo', 1);\n</code></pre>"},{"location":"reference/model-resolution/#2-assign-model-group-to-team","title":"2. Assign Model Group to Team","text":"<pre><code>INSERT INTO team_model_groups (team_id, model_group_id)\nSELECT 'team-beta', model_group_id\nFROM model_groups\nWHERE group_name = 'ChatAgent';\n</code></pre>"},{"location":"reference/model-resolution/#3-update-primary-model","title":"3. Update Primary Model","text":"<pre><code>-- Swap priorities to change primary\nUPDATE model_group_models\nSET priority = CASE\n    WHEN model_name = 'gpt-4-turbo' THEN 1\n    WHEN model_name = 'gpt-4' THEN 0\nEND\nWHERE model_group_id = (\n    SELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent'\n);\n</code></pre>"},{"location":"reference/model-resolution/#4-add-fallback-model","title":"4. Add Fallback Model","text":"<pre><code>INSERT INTO model_group_models (model_group_id, model_name, priority)\nSELECT\n    model_group_id,\n    'claude-3-opus',\n    3  -- Third fallback\nFROM model_groups\nWHERE group_name = 'ResumeAgent';\n</code></pre>"},{"location":"reference/model-resolution/#5-disable-model-without-deletion","title":"5. Disable Model (without deletion)","text":"<pre><code>UPDATE model_group_models\nSET is_active = FALSE\nWHERE model_name = 'gpt-3.5-turbo'\nAND model_group_id = (\n    SELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent'\n);\n</code></pre>"},{"location":"reference/model-resolution/#6-revoke-team-access","title":"6. Revoke Team Access","text":"<pre><code>DELETE FROM team_model_groups\nWHERE team_id = 'team-alpha'\nAND model_group_id = (\n    SELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent'\n);\n</code></pre>"},{"location":"reference/model-resolution/#analytics-queries","title":"Analytics Queries","text":""},{"location":"reference/model-resolution/#model-group-usage-by-team","title":"Model Group Usage by Team","text":"<pre><code>SELECT\n    llm.model_group_used,\n    llm.resolved_model,\n    COUNT(*) AS call_count,\n    AVG(llm.latency_ms) AS avg_latency_ms,\n    SUM(llm.total_tokens) AS total_tokens,\n    SUM(llm.cost_usd) AS total_cost_usd\nFROM llm_calls llm\nJOIN jobs j ON llm.job_id = j.job_id\nWHERE j.team_id = 'team-alpha'\nAND llm.created_at &gt;= NOW() - INTERVAL '30 days'\nGROUP BY llm.model_group_used, llm.resolved_model\nORDER BY call_count DESC;\n</code></pre>"},{"location":"reference/model-resolution/#model-group-access-report","title":"Model Group Access Report","text":"<pre><code>SELECT\n    mg.group_name,\n    mg.display_name,\n    COUNT(DISTINCT tmg.team_id) AS team_count,\n    STRING_AGG(DISTINCT mgm.model_name, ', ' ORDER BY mgm.priority) AS models\nFROM model_groups mg\nLEFT JOIN team_model_groups tmg ON mg.model_group_id = tmg.model_group_id\nLEFT JOIN model_group_models mgm ON mg.model_group_id = mgm.model_group_id\nWHERE mg.status = 'active'\nAND mgm.is_active = TRUE\nGROUP BY mg.group_name, mg.display_name\nORDER BY team_count DESC;\n</code></pre>"},{"location":"reference/model-resolution/#teams-without-model-group-access","title":"Teams Without Model Group Access","text":"<pre><code>SELECT DISTINCT j.team_id\nFROM jobs j\nLEFT JOIN team_model_groups tmg ON j.team_id = tmg.team_id\nWHERE tmg.team_id IS NULL\nAND j.created_at &gt;= NOW() - INTERVAL '7 days';\n</code></pre>"},{"location":"reference/model-resolution/#best-practices","title":"Best Practices","text":""},{"location":"reference/model-resolution/#1-use-descriptive-group-names","title":"1. Use Descriptive Group Names","text":"<pre><code>-- \u2705 Good - Clear purpose\n'ResumeAgent', 'DocumentParser', 'ChatAssistant'\n\n-- \u274c Bad - Unclear\n'Agent1', 'ModelA', 'Default'\n</code></pre>"},{"location":"reference/model-resolution/#2-always-have-fallbacks","title":"2. Always Have Fallbacks","text":"<p>Configure at least 2 models per group for reliability:</p> <pre><code>INSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES\n    (group_id, 'gpt-4-turbo', 0),      -- Primary\n    (group_id, 'gpt-3.5-turbo', 1);    -- Fallback\n</code></pre>"},{"location":"reference/model-resolution/#3-test-model-changes","title":"3. Test Model Changes","text":"<p>Before switching primary models, test with a subset of teams:</p> <pre><code>-- Create test group with new model\nINSERT INTO model_groups (group_name, display_name)\nVALUES ('ResumeAgent-Beta', 'Resume Agent (Testing GPT-4.5)');\n\n-- Assign to test teams only\nINSERT INTO team_model_groups (team_id, model_group_id)\nSELECT 'team-test', model_group_id\nFROM model_groups\nWHERE group_name = 'ResumeAgent-Beta';\n</code></pre>"},{"location":"reference/model-resolution/#4-track-resolution-history","title":"4. Track Resolution History","text":"<p>The <code>llm_calls</code> table tracks both <code>model_group_used</code> and <code>resolved_model</code>, allowing you to: - See which groups are most popular - Identify when models change - Audit resolution decisions</p>"},{"location":"reference/model-resolution/#5-document-model-group-purposes","title":"5. Document Model Group Purposes","text":"<p>Use the <code>description</code> field:</p> <pre><code>UPDATE model_groups\nSET description = 'Use for: Resume parsing, skills extraction, job matching. Optimized for structured outputs. Fallback to GPT-4 if quota exceeded.'\nWHERE group_name = 'ResumeAgent';\n</code></pre>"},{"location":"reference/model-resolution/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"reference/model-resolution/#per-team-model-overrides","title":"Per-Team Model Overrides","text":"<p>Allow specific teams to use different models for the same group:</p> <pre><code>-- Create team-specific variant\nINSERT INTO model_groups (group_name, display_name)\nVALUES ('ResumeAgent-Premium', 'Resume Agent (Enterprise)');\n\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nSELECT model_group_id, 'gpt-4-turbo', 0\nFROM model_groups\nWHERE group_name = 'ResumeAgent-Premium';\n\n-- Assign premium version to enterprise team\nINSERT INTO team_model_groups (team_id, model_group_id)\nSELECT 'team-enterprise', model_group_id\nFROM model_groups\nWHERE group_name = 'ResumeAgent-Premium';\n</code></pre>"},{"location":"reference/model-resolution/#cost-based-routing","title":"Cost-Based Routing","text":"<p>Route to cheaper models for non-critical tasks:</p> <pre><code>-- Budget model group\nINSERT INTO model_groups (group_name, display_name)\nVALUES ('ChatAgent-Budget', 'Chat Agent (Economy)');\n\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nSELECT model_group_id, 'gpt-3.5-turbo', 0\nFROM model_groups\nWHERE group_name = 'ChatAgent-Budget';\n\n-- Assign to free-tier teams\nINSERT INTO team_model_groups (team_id, model_group_id)\nSELECT team_id, (SELECT model_group_id FROM model_groups WHERE group_name = 'ChatAgent-Budget')\nFROM team_credits\nWHERE credits_remaining &lt; 100;\n</code></pre>"},{"location":"reference/model-resolution/#ab-testing-models","title":"A/B Testing Models","text":"<p>Split traffic between models to compare performance:</p> <pre><code>-- Create A/B variants\nINSERT INTO model_groups (group_name, display_name)\nVALUES\n    ('ResumeAgent-A', 'Resume Agent (GPT-4 Test)'),\n    ('ResumeAgent-B', 'Resume Agent (Claude Test)');\n\n-- Configure models\nINSERT INTO model_group_models (model_group_id, model_name, priority)\nVALUES\n    ((SELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent-A'), 'gpt-4-turbo', 0),\n    ((SELECT model_group_id FROM model_groups WHERE group_name = 'ResumeAgent-B'), 'claude-3-opus', 0);\n\n-- Assign 50% of teams to each\n-- (Application logic splits teams randomly)\n</code></pre>"},{"location":"reference/model-resolution/#see-also","title":"See Also","text":"<ul> <li>Database Schema Reference - Model group table definitions</li> <li>API Reference: Model Groups - Model group management endpoints</li> <li>LiteLLM Model List - Supported model identifiers</li> </ul>"},{"location":"reference/streaming-architecture/","title":"Streaming Architecture Reference","text":"<p>Complete technical reference for Server-Sent Events (SSE) streaming implementation in SaaS LiteLLM. This system provides real-time streaming of LLM responses with zero buffering for minimal latency.</p>"},{"location":"reference/streaming-architecture/#overview","title":"Overview","text":"<p>The streaming architecture enables:</p> <ul> <li>Real-time Response Streaming: Tokens arrive as they're generated</li> <li>Zero Buffering: No intermediate buffering between hops</li> <li>Low Latency: ~300-500ms Time to First Token (TTFT)</li> <li>Transparent Forwarding: SaaS API passes chunks directly from LiteLLM</li> <li>Same Credit Model: Streaming and non-streaming use same billing (1 credit per job)</li> <li>Error Handling: Graceful stream interruption and recovery</li> </ul>"},{"location":"reference/streaming-architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502         \u2502   SaaS API   \u2502         \u2502   LiteLLM    \u2502         \u2502  OpenAI/    \u2502\n\u2502 Application \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   :8003      \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Proxy      \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Anthropic  \u2502\n\u2502             \u2502  SSE    \u2502              \u2502  SSE    \u2502   :8002      \u2502  API    \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u25b2                        \u2502                        \u2502                        \u2502\n      \u2502                        \u2502                        \u2502                        \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              Zero-buffering streaming\n</code></pre>"},{"location":"reference/streaming-architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Client \u2192 SaaS API: HTTP POST request with <code>stream=true</code> parameter</li> <li>SaaS API \u2192 LiteLLM: Forward streaming request to LiteLLM proxy</li> <li>LiteLLM \u2192 Provider: LiteLLM makes streaming API call to OpenAI/Anthropic/etc</li> <li>Provider \u2192 LiteLLM \u2192 SaaS API \u2192 Client: Tokens forwarded in real-time</li> <li>Stream Complete: Final chunk sent, metadata recorded in database</li> </ol>"},{"location":"reference/streaming-architecture/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":""},{"location":"reference/streaming-architecture/#protocol-overview","title":"Protocol Overview","text":"<p>SSE is a standard for server-to-client streaming over HTTP:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: text/event-stream\nCache-Control: no-cache\nX-Accel-Buffering: no\nConnection: keep-alive\n\ndata: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\"!\"}}]}\n\ndata: [DONE]\n</code></pre> <p>Key Characteristics:</p> <ul> <li>Content-Type: <code>text/event-stream</code></li> <li>Chunk Format: Each chunk prefixed with <code>data:</code></li> <li>Newline Separation: Double newline (<code>\\n\\n</code>) between chunks</li> <li>Completion Signal: <code>data: [DONE]</code> marks end of stream</li> <li>Keep-Alive: Connection stays open until complete</li> </ul>"},{"location":"reference/streaming-architecture/#why-sse","title":"Why SSE?","text":"<p>Advantages over WebSockets:</p> <ul> <li>Simpler protocol (HTTP-based)</li> <li>Automatic reconnection in browsers</li> <li>Works through most proxies and firewalls</li> <li>No need for handshake negotiation</li> <li>Built-in event IDs for resumption</li> </ul> <p>Advantages over HTTP polling:</p> <ul> <li>Real-time delivery (no polling delay)</li> <li>Lower bandwidth (single connection)</li> <li>Reduced server load (no repeated requests)</li> </ul>"},{"location":"reference/streaming-architecture/#streaming-endpoints","title":"Streaming Endpoints","text":""},{"location":"reference/streaming-architecture/#endpoint-apijobsjob_idllm-call-stream","title":"Endpoint: <code>/api/jobs/{job_id}/llm-call-stream</code>","text":"<p>Note: Based on documentation and client code, the streaming endpoint is expected to be implemented but not found in <code>src/saas_api.py</code>. The non-streaming endpoint exists at <code>/api/jobs/{job_id}/llm-call</code>.</p> <p>Expected Implementation:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom sse_starlette.sse import EventSourceResponse\nimport httpx\nimport json\n\n@app.post(\"/api/jobs/{job_id}/llm-call-stream\")\nasync def make_llm_call_stream(\n    job_id: str,\n    request: LLMCallRequest,\n    db: Session = Depends(get_db),\n    authenticated_team_id: str = Depends(verify_virtual_key)\n):\n    \"\"\"\n    Make a streaming LLM call within a job context.\n    Returns Server-Sent Events (SSE) stream.\n    \"\"\"\n    from .services.model_resolver import ModelResolver, ModelResolutionError\n\n    # Get job\n    job = db.query(Job).filter(Job.job_id == uuid.UUID(job_id)).first()\n    if not job:\n        raise HTTPException(404, \"Job not found\")\n\n    # Verify job belongs to authenticated team\n    if job.team_id != authenticated_team_id:\n        raise HTTPException(403, \"Job does not belong to your team\")\n\n    # Get team credentials\n    team_credits = db.query(TeamCredits).filter(\n        TeamCredits.team_id == job.team_id\n    ).first()\n\n    if not team_credits or not team_credits.virtual_key:\n        raise HTTPException(500, \"Team has no virtual key configured\")\n\n    # Update job status to in_progress\n    if job.status == JobStatus.PENDING:\n        job.status = JobStatus.IN_PROGRESS\n        job.started_at = datetime.utcnow()\n        db.commit()\n\n    # Resolve model group\n    model_resolver = ModelResolver(db)\n\n    try:\n        primary_model, _ = model_resolver.resolve_model_group(\n            team_id=job.team_id,\n            model_group_name=request.model_group\n        )\n    except ModelResolutionError as e:\n        raise HTTPException(403, str(e))\n\n    # Track model group usage\n    if not job.model_groups_used:\n        job.model_groups_used = []\n    if request.model_group not in job.model_groups_used:\n        job.model_groups_used.append(request.model_group)\n        db.commit()\n\n    # Stream generator\n    async def stream_generator():\n        litellm_url = f\"{settings.litellm_proxy_url}/chat/completions\"\n\n        headers = {\n            \"Authorization\": f\"Bearer {team_credits.virtual_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        payload = {\n            \"model\": primary_model,\n            \"messages\": request.messages,\n            \"temperature\": request.temperature,\n            \"stream\": True,  # Enable streaming\n            \"user\": job.team_id\n        }\n\n        if request.max_tokens:\n            payload[\"max_tokens\"] = request.max_tokens\n\n        # Track for database recording\n        accumulated_content = \"\"\n        start_time = datetime.utcnow()\n        prompt_tokens = 0\n        completion_tokens = 0\n\n        try:\n            async with httpx.AsyncClient(timeout=120.0) as client:\n                async with client.stream(\n                    \"POST\",\n                    litellm_url,\n                    json=payload,\n                    headers=headers\n                ) as response:\n                    response.raise_for_status()\n\n                    # Forward each chunk to client\n                    async for line in response.aiter_lines():\n                        if line.startswith(\"data: \"):\n                            chunk_data = line[6:]\n\n                            # Forward to client as-is\n                            yield f\"data: {chunk_data}\\n\\n\"\n\n                            # Parse for database recording\n                            if chunk_data == \"[DONE]\":\n                                break\n\n                            try:\n                                chunk_json = json.loads(chunk_data)\n\n                                # Accumulate content\n                                if \"choices\" in chunk_json:\n                                    delta = chunk_json[\"choices\"][0].get(\"delta\", {})\n                                    content = delta.get(\"content\", \"\")\n                                    accumulated_content += content\n\n                                # Extract token usage (in final chunk)\n                                if \"usage\" in chunk_json:\n                                    usage = chunk_json[\"usage\"]\n                                    prompt_tokens = usage.get(\"prompt_tokens\", 0)\n                                    completion_tokens = usage.get(\"completion_tokens\", 0)\n\n                            except json.JSONDecodeError:\n                                continue\n\n            # Stream complete - record in database\n            end_time = datetime.utcnow()\n            latency_ms = int((end_time - start_time).total_seconds() * 1000)\n            total_tokens = prompt_tokens + completion_tokens\n            cost_usd = (prompt_tokens * 0.0005 / 1000) + (completion_tokens * 0.0015 / 1000)\n\n            llm_call = LLMCall(\n                job_id=job.job_id,\n                model_used=primary_model,\n                model_group_used=request.model_group,\n                resolved_model=primary_model,\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n                cost_usd=cost_usd,\n                latency_ms=latency_ms,\n                purpose=request.purpose,\n                request_data={\"messages\": request.messages, \"model_group\": request.model_group},\n                response_data={\"content\": accumulated_content}\n            )\n\n            db.add(llm_call)\n            db.commit()\n\n        except Exception as e:\n            # Record failed call\n            llm_call = LLMCall(\n                job_id=job.job_id,\n                model_group_used=request.model_group,\n                purpose=request.purpose,\n                error=str(e),\n                request_data={\"messages\": request.messages, \"model_group\": request.model_group}\n            )\n            db.add(llm_call)\n            db.commit()\n\n            # Send error to client\n            yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n\n    # Return streaming response\n    return StreamingResponse(\n        stream_generator(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"X-Accel-Buffering\": \"no\",\n            \"Connection\": \"keep-alive\"\n        }\n    )\n</code></pre>"},{"location":"reference/streaming-architecture/#streaming-flow-diagram","title":"Streaming Flow Diagram","text":""},{"location":"reference/streaming-architecture/#complete-request-flow","title":"Complete Request Flow","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant SaaSAPI as SaaS API&lt;br/&gt;:8003\n    participant DB as PostgreSQL\n    participant LiteLLM as LiteLLM Proxy&lt;br/&gt;:8002\n    participant Provider as OpenAI/Anthropic\n\n    Client-&gt;&gt;SaaSAPI: POST /api/jobs/{job_id}/llm-call-stream&lt;br/&gt;model_group=\"ResumeAgent\"\n    SaaSAPI-&gt;&gt;DB: Get job, verify team\n    DB--&gt;&gt;SaaSAPI: Job details\n    SaaSAPI-&gt;&gt;DB: Update job status='in_progress'\n    SaaSAPI-&gt;&gt;DB: Resolve model group\n    DB--&gt;&gt;SaaSAPI: primary='gpt-4-turbo'\n    SaaSAPI-&gt;&gt;LiteLLM: POST /chat/completions&lt;br/&gt;stream=true, model='gpt-4-turbo'\n\n    LiteLLM-&gt;&gt;Provider: Streaming API call\n\n    loop Each token generated\n        Provider--&gt;&gt;LiteLLM: Token chunk (SSE)\n        Note over LiteLLM: Zero buffering\n        LiteLLM--&gt;&gt;SaaSAPI: Forward chunk (SSE)\n        Note over SaaSAPI: Zero buffering,&lt;br/&gt;accumulate for DB\n        SaaSAPI--&gt;&gt;Client: Forward chunk (SSE)\n    end\n\n    Provider--&gt;&gt;LiteLLM: Final chunk + usage\n    LiteLLM--&gt;&gt;SaaSAPI: Final chunk + usage\n    SaaSAPI--&gt;&gt;Client: Final chunk\n    SaaSAPI-&gt;&gt;DB: Record llm_call&lt;br/&gt;(accumulated content, tokens, cost)\n    DB--&gt;&gt;SaaSAPI: Saved\n    SaaSAPI--&gt;&gt;Client: data: [DONE]</code></pre>"},{"location":"reference/streaming-architecture/#performance-metrics","title":"Performance Metrics","text":"<p>Typical Latencies:</p> <ul> <li>Time to First Token (TTFT): 300-500ms</li> <li>Per-token Latency: 50ms</li> <li>SaaS API Overhead: 10-50ms per hop (minimal!)</li> <li>LiteLLM Overhead: 10-30ms per hop</li> </ul> <p>Total TTFT Breakdown:</p> <pre><code>Client Request \u2192 SaaS API: ~20ms (network)\nSaaS API Processing: ~30ms (auth, DB lookup, resolution)\nSaaS API \u2192 LiteLLM: ~10ms (local network)\nLiteLLM Processing: ~20ms (routing)\nLiteLLM \u2192 Provider: ~50ms (network)\nProvider Processing: ~200-300ms (model inference)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal TTFT: ~330-430ms\n</code></pre>"},{"location":"reference/streaming-architecture/#chunk-format","title":"Chunk Format","text":""},{"location":"reference/streaming-architecture/#openai-compatible-stream-chunks","title":"OpenAI-Compatible Stream Chunks","text":"<p>Each chunk is a JSON object following OpenAI's format:</p> <pre><code>{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1697896000,\n  \"model\": \"gpt-4-turbo-2024-04-09\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello\"\n      },\n      \"finish_reason\": null\n    }\n  ]\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>id</code>: Unique completion ID</li> <li><code>object</code>: Always <code>\"chat.completion.chunk\"</code> for streaming</li> <li><code>created</code>: Unix timestamp</li> <li><code>model</code>: Actual model used</li> <li><code>choices[].delta</code>: Incremental content</li> <li><code>choices[].delta.role</code>: Only present in first chunk (\"assistant\")</li> <li><code>choices[].delta.content</code>: The text chunk</li> <li><code>choices[].finish_reason</code>: <code>null</code> during streaming, <code>\"stop\"</code>/<code>\"length\"</code> at end</li> </ul>"},{"location":"reference/streaming-architecture/#final-chunk","title":"Final Chunk","text":"<p>Last chunk includes usage metadata:</p> <pre><code>{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1697896000,\n  \"model\": \"gpt-4-turbo-2024-04-09\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {},\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 125,\n    \"completion_tokens\": 450,\n    \"total_tokens\": 575\n  }\n}\n</code></pre> <p>Usage Field:</p> <ul> <li><code>prompt_tokens</code>: Input tokens</li> <li><code>completion_tokens</code>: Generated tokens</li> <li><code>total_tokens</code>: Sum of prompt + completion</li> </ul> <p>This data is extracted and stored in the <code>llm_calls</code> table.</p>"},{"location":"reference/streaming-architecture/#client-side-implementation","title":"Client-Side Implementation","text":""},{"location":"reference/streaming-architecture/#python-client-httpx","title":"Python Client (httpx)","text":"<p>From <code>examples/typed_client.py</code>:</p> <pre><code>async def chat_stream(\n    self,\n    job_id: str,\n    model_group: str,\n    messages: List[Dict[str, str]],\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None\n) -&gt; AsyncGenerator[StreamChunk, None]:\n    \"\"\"Stream LLM response\"\"\"\n\n    payload = {\n        \"model_group\": model_group,\n        \"messages\": messages,\n        \"temperature\": temperature\n    }\n\n    if max_tokens:\n        payload[\"max_tokens\"] = max_tokens\n\n    # Open streaming connection\n    async with self.client.stream(\n        \"POST\",\n        f\"{self.api_url}/api/jobs/{job_id}/llm-call-stream\",\n        json=payload\n    ) as response:\n        response.raise_for_status()\n\n        # Process SSE stream\n        async for line in response.aiter_lines():\n            if line.startswith(\"data: \"):\n                chunk_data = line[6:]  # Remove \"data: \" prefix\n\n                if chunk_data == \"[DONE]\":\n                    break\n\n                try:\n                    chunk_json = json.loads(chunk_data)\n\n                    # Handle errors\n                    if \"error\" in chunk_json:\n                        raise Exception(f\"Stream error: {chunk_json['error']}\")\n\n                    # Yield parsed chunk\n                    yield StreamChunk(**chunk_json)\n\n                except json.JSONDecodeError:\n                    continue\n</code></pre> <p>Usage:</p> <pre><code>async with SaaSLLMClient(base_url, team_id, virtual_key) as client:\n    job_id = await client.create_job(\"chat\")\n\n    async for chunk in client.chat_stream(\n        job_id=job_id,\n        model_group=\"ChatAgent\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    ):\n        if chunk.choices:\n            content = chunk.choices[0].delta.get(\"content\", \"\")\n            if content:\n                print(content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/streaming-architecture/#javascript-client","title":"JavaScript Client","text":"<pre><code>async function streamChat(jobId, messages) {\n  const response = await fetch(`${API_URL}/api/jobs/${jobId}/llm-call-stream`, {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${VIRTUAL_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model_group: 'ChatAgent',\n      messages: messages\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const {done, value} = await reader.read();\n    if (done) break;\n\n    const text = decoder.decode(value);\n    const lines = text.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.substring(6);\n\n        if (data === '[DONE]') {\n          console.log('\\nStream complete');\n          return;\n        }\n\n        try {\n          const chunk = JSON.parse(data);\n          const content = chunk.choices?.[0]?.delta?.content || '';\n          if (content) {\n            process.stdout.write(content);  // Node.js\n            // Or: document.getElementById('output').textContent += content;  // Browser\n          }\n        } catch (e) {\n          // Ignore parse errors\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reference/streaming-architecture/#error-handling","title":"Error Handling","text":""},{"location":"reference/streaming-architecture/#stream-interruption","title":"Stream Interruption","text":"<p>Scenarios:</p> <ol> <li>Network disconnection: Client or proxy drops connection</li> <li>LLM provider error: OpenAI/Anthropic returns error mid-stream</li> <li>Timeout: Request exceeds configured timeout</li> <li>Server error: Internal error in SaaS API or LiteLLM</li> </ol>"},{"location":"reference/streaming-architecture/#error-chunk-format","title":"Error Chunk Format","text":"<p>When errors occur, send error chunk before closing:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Model timeout exceeded\",\n    \"type\": \"timeout_error\",\n    \"code\": \"model_timeout\"\n  }\n}\n</code></pre> <p>Client Handling:</p> <pre><code>async for chunk in client.chat_stream(...):\n    # Check for error field\n    if hasattr(chunk, 'error') and chunk.error:\n        raise StreamError(chunk.error['message'])\n\n    # Normal processing\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/streaming-architecture/#graceful-degradation","title":"Graceful Degradation","text":"<p>If streaming fails, client can fall back to non-streaming:</p> <pre><code>async def resilient_llm_call(client, job_id, messages):\n    try:\n        # Try streaming first\n        accumulated = \"\"\n        async for chunk in client.chat_stream(job_id, messages):\n            if chunk.choices:\n                accumulated += chunk.choices[0].delta.get(\"content\", \"\")\n        return accumulated\n\n    except StreamError as e:\n        print(f\"Streaming failed: {e}. Falling back to non-streaming.\")\n        # Fall back to non-streaming\n        response = await client.chat(job_id, messages)\n        return response.choices[0].message[\"content\"]\n</code></pre>"},{"location":"reference/streaming-architecture/#timeout-configuration","title":"Timeout Configuration","text":"<p>Set appropriate timeouts at each layer:</p> <p>Client:</p> <pre><code>async with httpx.AsyncClient(timeout=120.0) as client:\n    async with client.stream(...) as response:\n        # 120 second timeout for entire stream\n        pass\n</code></pre> <p>SaaS API:</p> <pre><code>async with httpx.AsyncClient(timeout=120.0) as client:\n    async with client.stream(\"POST\", litellm_url, ...) as response:\n        # 120 second timeout to LiteLLM\n        pass\n</code></pre> <p>Application-Level:</p> <pre><code>import asyncio\n\nasync with asyncio.timeout(60):  # 60 second max for operation\n    async for chunk in client.chat_stream(...):\n        # Process chunks\n        pass\n</code></pre>"},{"location":"reference/streaming-architecture/#database-recording","title":"Database Recording","text":""},{"location":"reference/streaming-architecture/#recording-streamed-content","title":"Recording Streamed Content","text":"<p>The SaaS API accumulates content during streaming, then records to database once complete:</p> <pre><code># During streaming\naccumulated_content = \"\"\nprompt_tokens = 0\ncompletion_tokens = 0\n\nasync for line in response.aiter_lines():\n    if line.startswith(\"data: \"):\n        chunk_json = json.loads(line[6:])\n\n        # Accumulate content\n        if \"choices\" in chunk_json:\n            delta = chunk_json[\"choices\"][0].get(\"delta\", {})\n            accumulated_content += delta.get(\"content\", \"\")\n\n        # Extract final usage\n        if \"usage\" in chunk_json:\n            prompt_tokens = chunk_json[\"usage\"][\"prompt_tokens\"]\n            completion_tokens = chunk_json[\"usage\"][\"completion_tokens\"]\n\n# After stream completes, record to database\nllm_call = LLMCall(\n    job_id=job_id,\n    model_used=primary_model,\n    model_group_used=request.model_group,\n    resolved_model=primary_model,\n    prompt_tokens=prompt_tokens,\n    completion_tokens=completion_tokens,\n    total_tokens=prompt_tokens + completion_tokens,\n    cost_usd=calculate_cost(prompt_tokens, completion_tokens),\n    latency_ms=latency_ms,\n    response_data={\"content\": accumulated_content}\n)\ndb.add(llm_call)\ndb.commit()\n</code></pre> <p>Why Accumulate?</p> <ul> <li>Database stores complete response, not individual chunks</li> <li>Allows cost calculation based on final token counts</li> <li>Enables full-text search on responses</li> <li>Provides audit trail</li> </ul>"},{"location":"reference/streaming-architecture/#performance-optimization","title":"Performance Optimization","text":""},{"location":"reference/streaming-architecture/#1-disable-buffering","title":"1. Disable Buffering","text":"<p>Nginx:</p> <pre><code>location /api/jobs/ {\n    proxy_pass http://saas_api:8003;\n    proxy_buffering off;\n    proxy_cache off;\n    proxy_set_header X-Accel-Buffering no;\n}\n</code></pre> <p>FastAPI:</p> <pre><code>return StreamingResponse(\n    stream_generator(),\n    media_type=\"text/event-stream\",\n    headers={\n        \"Cache-Control\": \"no-cache\",\n        \"X-Accel-Buffering\": \"no\"  # Disable nginx buffering\n    }\n)\n</code></pre>"},{"location":"reference/streaming-architecture/#2-use-http2","title":"2. Use HTTP/2","text":"<p>HTTP/2 multiplexing improves streaming performance:</p> <pre><code>async with httpx.AsyncClient(http2=True) as client:\n    async with client.stream(...) as response:\n        # Faster streaming with HTTP/2\n        pass\n</code></pre>"},{"location":"reference/streaming-architecture/#3-async-all-the-way","title":"3. Async All The Way","text":"<p>Never block in stream processing:</p> <pre><code># \u2705 Good - Fully async\nasync def stream_generator():\n    async with httpx.AsyncClient() as client:\n        async with client.stream(...) as response:\n            async for line in response.aiter_lines():\n                yield f\"data: {line}\\n\\n\"\n\n# \u274c Bad - Blocks event loop\ndef stream_generator():\n    response = requests.get(..., stream=True)\n    for line in response.iter_lines():\n        yield f\"data: {line}\\n\\n\"\n</code></pre>"},{"location":"reference/streaming-architecture/#4-minimize-per-chunk-processing","title":"4. Minimize Per-Chunk Processing","text":"<p>Keep per-chunk processing lightweight:</p> <pre><code># \u2705 Good - Minimal processing\nasync for line in response.aiter_lines():\n    if line.startswith(\"data: \"):\n        yield f\"{line}\\n\\n\"  # Forward as-is\n\n# \u274c Bad - Heavy processing per chunk\nasync for line in response.aiter_lines():\n    chunk = json.loads(line[6:])\n    validated = validate_chunk(chunk)  # Expensive!\n    enriched = enrich_chunk(validated)  # More processing!\n    yield f\"data: {json.dumps(enriched)}\\n\\n\"\n</code></pre>"},{"location":"reference/streaming-architecture/#best-practices","title":"Best Practices","text":""},{"location":"reference/streaming-architecture/#1-always-flush-output","title":"1. Always Flush Output","text":"<pre><code># \u2705 Good - Immediate display\nprint(content, end=\"\", flush=True)\n\n# \u274c Bad - Buffered output\nprint(content, end=\"\")\n</code></pre>"},{"location":"reference/streaming-architecture/#2-handle-empty-chunks","title":"2. Handle Empty Chunks","text":"<p>Not all chunks contain content:</p> <pre><code>async for chunk in client.chat_stream(...):\n    if chunk.choices:\n        delta = chunk.choices[0].delta\n        if \"content\" in delta and delta[\"content\"]:\n            print(delta[\"content\"], end=\"\", flush=True)\n</code></pre>"},{"location":"reference/streaming-architecture/#3-monitor-finish_reason","title":"3. Monitor finish_reason","text":"<pre><code>async for chunk in client.chat_stream(...):\n    if chunk.choices:\n        choice = chunk.choices[0]\n\n        # Process content\n        if \"content\" in choice.delta:\n            print(choice.delta[\"content\"], end=\"\", flush=True)\n\n        # Check completion reason\n        if choice.finish_reason:\n            if choice.finish_reason == \"length\":\n                print(\"\\n\\nWarning: Response truncated (max_tokens reached)\")\n            elif choice.finish_reason == \"stop\":\n                print(\"\\n\\nResponse complete\")\n</code></pre>"},{"location":"reference/streaming-architecture/#4-set-appropriate-timeouts","title":"4. Set Appropriate Timeouts","text":"<pre><code># Long-form content (articles, code)\ntimeout = 180.0  # 3 minutes\n\n# Chat responses\ntimeout = 60.0  # 1 minute\n\n# Quick queries\ntimeout = 30.0  # 30 seconds\n\nasync with httpx.AsyncClient(timeout=timeout) as client:\n    async with client.stream(...) as response:\n        # Stream with appropriate timeout\n        pass\n</code></pre>"},{"location":"reference/streaming-architecture/#5-accumulate-for-post-processing","title":"5. Accumulate for Post-Processing","text":"<pre><code>accumulated = \"\"\n\nasync for chunk in client.chat_stream(...):\n    if chunk.choices:\n        content = chunk.choices[0].delta.get(\"content\", \"\")\n        accumulated += content\n        print(content, end=\"\", flush=True)\n\n# Now use accumulated for analysis, storage, etc.\nprint(f\"\\n\\nTotal length: {len(accumulated)} characters\")\n</code></pre>"},{"location":"reference/streaming-architecture/#comparison-streaming-vs-non-streaming","title":"Comparison: Streaming vs Non-Streaming","text":"Aspect Non-Streaming Streaming TTFT ~2000-3000ms ~300-500ms User Experience Wait for complete response Progressive display Implementation Simpler More complex Error Handling Single try/catch Per-chunk error handling Buffering Full response buffered Zero buffering Memory Usage Higher (full response in memory) Lower (chunk-by-chunk) Network Usage Single large payload Many small chunks Credit Cost 1 credit per completed job 1 credit per completed job Database Recording Direct response storage Accumulate then store Best For Batch processing, background jobs Interactive chat, real-time UI"},{"location":"reference/streaming-architecture/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/streaming-architecture/#issue-stream-hangs","title":"Issue: Stream Hangs","text":"<p>Symptoms: No chunks received, connection stays open</p> <p>Causes:</p> <ol> <li>Buffering enabled at proxy level</li> <li>Synchronous blocking code in stream handler</li> <li>Missing <code>flush=True</code> in client</li> </ol> <p>Solutions:</p> <pre><code># Disable nginx buffering\nproxy_buffering off;\nproxy_cache off;\n</code></pre> <pre><code># Use async streaming\nasync with client.stream(...) as response:\n    async for line in response.aiter_lines():\n        yield line\n</code></pre>"},{"location":"reference/streaming-architecture/#issue-chunks-arrive-in-bursts","title":"Issue: Chunks Arrive in Bursts","text":"<p>Symptoms: Chunks come in groups instead of continuously</p> <p>Causes:</p> <ol> <li>Intermediate proxy buffering</li> <li>TCP buffering</li> <li>Client-side buffering</li> </ol> <p>Solutions:</p> <pre><code># Disable buffering headers\nheaders = {\n    \"Cache-Control\": \"no-cache\",\n    \"X-Accel-Buffering\": \"no\",\n    \"Connection\": \"keep-alive\"\n}\n</code></pre>"},{"location":"reference/streaming-architecture/#issue-high-memory-usage","title":"Issue: High Memory Usage","text":"<p>Symptoms: Memory grows during long streams</p> <p>Causes:</p> <ol> <li>Accumulating entire response in memory</li> <li>Not releasing chunks after processing</li> </ol> <p>Solutions:</p> <pre><code># \u2705 Good - Process and discard\nasync for chunk in stream:\n    process_chunk(chunk)\n    # Chunk is garbage collected\n\n# \u274c Bad - Accumulate everything\nchunks = []\nasync for chunk in stream:\n    chunks.append(chunk)  # Memory grows!\n</code></pre>"},{"location":"reference/streaming-architecture/#see-also","title":"See Also","text":"<ul> <li>Streaming Integration Guide - How to use streaming in your app</li> <li>Streaming Examples - Code examples</li> <li>Non-Streaming Guide - Alternative approach</li> <li>Job Workflow - Understanding job lifecycle</li> </ul>"},{"location":"testing/integration-tests/","title":"Integration Tests","text":"<p>This guide covers the integration testing approach for SaasLiteLLM, including how to run tests, what scenarios are covered, and CI/CD integration.</p>"},{"location":"testing/integration-tests/#overview","title":"Overview","text":"<p>Integration tests verify that multiple components work together correctly. For SaasLiteLLM, this means testing:</p> <ul> <li>SaaS API endpoints with the database</li> <li>LiteLLM proxy integration</li> <li>Complete workflows (team creation \u2192 job tracking \u2192 credit management)</li> <li>Database persistence and retrieval</li> </ul>"},{"location":"testing/integration-tests/#test-scripts","title":"Test Scripts","text":"<p>The project includes three main integration test scripts located in the <code>scripts/</code> directory:</p>"},{"location":"testing/integration-tests/#1-test_jwt_integrationpy","title":"1. test_jwt_integration.py","text":"<p>Purpose: Comprehensive testing of JWT authentication and dual auth system.</p> <p>What It Tests: - Setup/Login flows (owner account creation) - JWT Bearer token authentication - Legacy X-Admin-Key authentication (backward compatibility) - Dual authentication on management endpoints - Role-based access control (owner, admin, user roles) - Admin user management (create, list, permissions) - Session management and logout - Audit logging system - Security features (unauthorized access protection) - Credits management with both auth methods</p> <p>Use Case: Validates that the admin authentication system is working correctly with both JWT and legacy authentication methods.</p> <p>Test Results: See Integration Test Results for detailed test results documentation.</p>"},{"location":"testing/integration-tests/#2-test_minimal_versionpy","title":"2. test_minimal_version.py","text":"<p>Purpose: Tests core SaaS API functionality without requiring full LiteLLM integration.</p> <p>What It Tests: - Health check endpoint - Organization CRUD operations - Model group creation and management - Team creation with model groups - Credit allocation and management - Credit balance tracking</p> <p>Use Case: Quick validation that the SaaS API is working correctly.</p>"},{"location":"testing/integration-tests/#2-test_full_integrationpy","title":"2. test_full_integration.py","text":"<p>Purpose: Tests complete integration with LiteLLM proxy, including virtual key generation.</p> <p>What It Tests: - Organization and model group setup - Team creation in LiteLLM - Virtual API key generation - Model group assignment to teams - Credit allocation with LiteLLM teams - Database synchronization between SaaS API and LiteLLM</p> <p>Use Case: Full system validation before deployment or after major changes.</p>"},{"location":"testing/integration-tests/#running-integration-tests","title":"Running Integration Tests","text":""},{"location":"testing/integration-tests/#prerequisites","title":"Prerequisites","text":"<p>Before running integration tests, ensure all required services are running:</p>"},{"location":"testing/integration-tests/#1-start-docker-services","title":"1. Start Docker Services","text":"<p>Start PostgreSQL and Redis containers:</p> <pre><code>./scripts/docker_setup.sh\n</code></pre> <p>This will: - Start PostgreSQL on <code>localhost:5432</code> - Start Redis on <code>localhost:6379</code> - Verify database connection - Create necessary databases</p>"},{"location":"testing/integration-tests/#2-run-database-migrations","title":"2. Run Database Migrations","text":"<p>Create the required database tables:</p> <pre><code>./scripts/run_migrations.sh\n</code></pre> <p>This creates: - Organizations table - Model groups tables - Teams table - Credits tables - Job tracking tables</p>"},{"location":"testing/integration-tests/#3-start-litellm-backend-terminal-1","title":"3. Start LiteLLM Backend (Terminal 1)","text":"<pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Start LiteLLM proxy on port 8002\npython scripts/start_local.py\n</code></pre> <p>Expected output: <pre><code>\ud83d\ude80 Starting LiteLLM proxy server...\n\ud83c\udf10 Server will be available at: http://0.0.0.0:8002\n\ud83c\udf9b\ufe0f  Admin UI will be available at: http://0.0.0.0:8002/ui\n</code></pre></p>"},{"location":"testing/integration-tests/#4-start-saas-api-terminal-2","title":"4. Start SaaS API (Terminal 2)","text":"<pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Start SaaS API on port 8003\npython scripts/start_saas_api.py\n</code></pre> <p>Expected output: <pre><code>\ud83d\ude80 Starting SaaS API wrapper service...\n\ud83c\udf10 SaaS API will be available at: http://0.0.0.0:8003\n\ud83d\udcd6 API docs: http://0.0.0.0:8003/docs\n</code></pre></p>"},{"location":"testing/integration-tests/#running-the-tests","title":"Running the Tests","text":""},{"location":"testing/integration-tests/#jwt-integration-test","title":"JWT Integration Test","text":"<p>Tests JWT authentication and dual auth system:</p> <pre><code># From project root\npython3 scripts/test_jwt_integration.py\n</code></pre> <p>Prerequisites: - SaaS API running on port 8004 - PostgreSQL database accessible - Redis running (for session management) - Admin-related tables created (admin_users, admin_sessions, admin_audit_log)</p> <p>Expected Output:</p> <pre><code>======================================================================\nJWT Authentication &amp; Dual Auth Integration Tests\n======================================================================\n\n======================================================================\n1. Health Check\n======================================================================\n\n\u2713 API health check passed: {'status': 'healthy'}\n\n======================================================================\n2. Setup Status Check\n======================================================================\n\n\u2139 Setup status: {\n  \"needs_setup\": true,\n  \"has_users\": false\n}\n\u2139 Setup is needed - will create owner account\n\n======================================================================\n3. Authentication Setup\n======================================================================\n\n\u2139 Creating owner account...\n\u2713 Owner account created successfully\n\u2139 User ID: ae89197e-e24a-4e55-a9e2-70ba9a273730\n\u2139 Role: owner\n\n======================================================================\n4. JWT Authentication Test\n======================================================================\n\n\u2713 JWT authentication successful\n\u2139 Current user: test-owner@example.com (owner)\n\n======================================================================\n5. Legacy X-Admin-Key Authentication Test\n======================================================================\n\n\u2713 Legacy X-Admin-Key authentication successful\n\u2139 Retrieved 2 model groups\n\n======================================================================\n6. Dual Authentication - Organizations\n======================================================================\n\n\u2139 Testing organizations with JWT...\n\u2713 JWT auth works for organizations endpoint\n\u2139 Testing organizations with X-Admin-Key...\n\u2713 X-Admin-Key auth works for organizations endpoint\n\n...\n\n======================================================================\nTest Summary\n======================================================================\n\nResults: 12/12 tests passed\n\n  PASS  Health Check\n  PASS  Setup/Login\n  PASS  JWT Authentication\n  PASS  Legacy X-Admin-Key Auth\n  PASS  Dual Auth - Organizations\n  PASS  Create Organization\n  PASS  Create Team\n  PASS  List Admin Users\n  PASS  Create Admin User\n  PASS  View Audit Logs\n  PASS  Unauthorized Access Protection\n  PASS  Credits Management\n\n======================================================================\n\n\u2713 All integration tests passed!\n</code></pre> <p>Test Credentials Created: - Email: test-owner@example.com - Password: TestPassword123! - Role: owner</p> <p>For detailed test results, see Integration Test Results.</p>"},{"location":"testing/integration-tests/#minimal-version-test","title":"Minimal Version Test","text":"<p>Tests core functionality without LiteLLM:</p> <pre><code># From project root\npython scripts/test_minimal_version.py\n</code></pre> <p>Expected Output:</p> <pre><code>############################################################\n#  MINIMAL VERSION TEST SUITE\n#  Testing Model Groups, Organizations, Teams &amp; Credits\n#  Base URL: http://localhost:8003\n############################################################\n\n============================================================\n  1. Health Check\n============================================================\n\nStatus: 200\nResponse: {'status': 'healthy'}\n\u2705 Health check passed\n\n============================================================\n  2. Create Organization\n============================================================\n\nStatus: 200\nResponse: {\n  \"organization_id\": \"org_test_001\",\n  \"name\": \"Test Organization\",\n  ...\n}\n\u2705 Organization created\n\n...\n\n============================================================\n  TEST SUMMARY\n============================================================\n\n\u2705 All tests passed!\n</code></pre>"},{"location":"testing/integration-tests/#full-integration-test","title":"Full Integration Test","text":"<p>Tests complete LiteLLM integration:</p> <pre><code># From project root\npython scripts/test_full_integration.py\n</code></pre> <p>Expected Output:</p> <pre><code>======================================================================\n  TESTING: Create Team with LiteLLM Integration\n======================================================================\n\n1. Creating organization...\n   Status: 200\n   Created successfully\n\n2. Creating model groups...\n   ChatAgent created\n   AnalysisAgent created\n\n3. Creating team with LiteLLM integration...\n   This will:\n   - Create team in LiteLLM\n   - Generate virtual API key\n   - Assign model groups\n   - Allocate 100 credits\n\n   Status: 200\n\n----------------------------------------------------------------------\nTEAM CREATED SUCCESSFULLY!\n----------------------------------------------------------------------\n{\n  \"team_id\": \"team_demo_engineering\",\n  \"organization_id\": \"org_demo_001\",\n  \"virtual_key\": \"sk-....\",\n  \"model_groups\": [\"ChatAgent\", \"AnalysisAgent\"],\n  \"credits_allocated\": 100\n}\n----------------------------------------------------------------------\n\nKEY INFORMATION:\n  Team ID: team_demo_engineering\n  Virtual Key: sk-xxxxxxxxxxxxxxxxxxxxxx...\n  Model Groups: ChatAgent, AnalysisAgent\n  Credits Allocated: 100\n\n...\n\n======================================================================\nSUCCESS! Team created with full LiteLLM integration\n======================================================================\n</code></pre>"},{"location":"testing/integration-tests/#exit-codes","title":"Exit Codes","text":"<p>Both test scripts use standard exit codes: - <code>0</code>: All tests passed successfully - <code>1</code>: One or more tests failed</p> <p>This makes them suitable for CI/CD pipelines:</p> <pre><code>python scripts/test_full_integration.py\nif [ $? -eq 0 ]; then\n  echo \"Tests passed!\"\nelse\n  echo \"Tests failed!\"\n  exit 1\nfi\n</code></pre>"},{"location":"testing/integration-tests/#test-scenarios-covered","title":"Test Scenarios Covered","text":""},{"location":"testing/integration-tests/#jwt-authentication-admin-user-management","title":"JWT Authentication &amp; Admin User Management","text":"<p>Endpoints: - <code>/api/admin-users/setup/status</code> - <code>/api/admin-users/setup</code> - <code>/api/admin-users/login</code> - <code>/api/admin-users/logout</code> - <code>/api/admin-users/me</code> - <code>/api/admin-users</code> (list, create, update, delete) - <code>/api/admin-users/audit-logs</code></p> <p>Test Case 1: First-time setup (owner account creation) <pre><code># Check if setup is needed\nresponse = requests.get(f\"{API_URL}/api/admin-users/setup/status\")\nstatus = response.json()\n\nif status[\"needs_setup\"]:\n    # Create owner account\n    response = requests.post(\n        f\"{API_URL}/api/admin-users/setup\",\n        json={\n            \"email\": \"admin@example.com\",\n            \"display_name\": \"Admin User\",\n            \"password\": \"SecurePassword123!\"\n        }\n    )\n    jwt_token = response.json()[\"access_token\"]\n</code></pre></p> <p>Test Case 2: Login and JWT authentication <pre><code># Login with email/password\nresponse = requests.post(\n    f\"{API_URL}/api/admin-users/login\",\n    json={\n        \"email\": \"admin@example.com\",\n        \"password\": \"SecurePassword123!\"\n    }\n)\njwt_token = response.json()[\"access_token\"]\n\n# Use JWT token for authenticated requests\nresponse = requests.get(\n    f\"{API_URL}/api/admin-users/me\",\n    headers={\"Authorization\": f\"Bearer {jwt_token}\"}\n)\nuser = response.json()\n</code></pre></p> <p>Test Case 3: Dual authentication (JWT + Legacy) <pre><code># Test with JWT Bearer token\nresponse = requests.get(\n    f\"{API_URL}/api/organizations\",\n    headers={\"Authorization\": f\"Bearer {jwt_token}\"}\n)\n\n# Test with legacy X-Admin-Key\nresponse = requests.get(\n    f\"{API_URL}/api/organizations\",\n    headers={\"X-Admin-Key\": MASTER_KEY}\n)\n</code></pre></p> <p>Test Case 4: Role-based access control <pre><code># Owner can create admin users\nresponse = requests.post(\n    f\"{API_URL}/api/admin-users\",\n    headers={\"Authorization\": f\"Bearer {owner_token}\"},\n    json={\n        \"email\": \"newadmin@example.com\",\n        \"display_name\": \"New Admin\",\n        \"password\": \"SecurePass456!\",\n        \"role\": \"admin\"\n    }\n)\n</code></pre></p> <p>Test Case 5: Security validation <pre><code># Request without authentication - should fail\nresponse = requests.get(f\"{API_URL}/api/admin-users\")\nassert response.status_code == 401\n\n# Request with invalid token - should fail\nresponse = requests.get(\n    f\"{API_URL}/api/admin-users\",\n    headers={\"Authorization\": \"Bearer invalid-token\"}\n)\nassert response.status_code == 401\n\n# Request with valid token - should succeed\nresponse = requests.get(\n    f\"{API_URL}/api/admin-users\",\n    headers={\"Authorization\": f\"Bearer {valid_token}\"}\n)\nassert response.status_code == 200\n</code></pre></p> <p>Validations: - Setup creates first owner account successfully - Login returns valid JWT token - JWT tokens authenticate properly - Legacy X-Admin-Key still works (backward compatibility) - Both auth methods work on dual-auth endpoints - JWT-only endpoints reject X-Admin-Key - Role-based permissions enforced correctly - Invalid/missing authentication rejected with 401 - Session tracking and audit logging working - Password security (hashing, minimum length)</p>"},{"location":"testing/integration-tests/#organization-management","title":"Organization Management","text":"<p>Endpoint: <code>/api/organizations/create</code></p> <p>Test Case: Create new organization <pre><code>payload = {\n    \"organization_id\": \"org_test_001\",\n    \"name\": \"Test Organization\",\n    \"metadata\": {\"plan\": \"dev\"}\n}\nresponse = requests.post(f\"{BASE_URL}/api/organizations/create\", json=payload)\n</code></pre></p> <p>Validations: - Status code 200 for success - Returns organization with correct ID - Handles duplicate organizations gracefully (400 status) - Metadata is stored correctly</p>"},{"location":"testing/integration-tests/#model-group-setup","title":"Model Group Setup","text":"<p>Endpoint: <code>/api/model-groups/create</code></p> <p>Test Case: Create model group with multiple models <pre><code>payload = {\n    \"group_name\": \"ResumeAgent\",\n    \"display_name\": \"Resume Analysis Agent\",\n    \"description\": \"Analyzes resumes and extracts structured data\",\n    \"models\": [\n        {\"model_name\": \"gpt-4-turbo\", \"priority\": 0},\n        {\"model_name\": \"gpt-3.5-turbo\", \"priority\": 1}\n    ]\n}\nresponse = requests.post(f\"{BASE_URL}/api/model-groups/create\", json=payload)\n</code></pre></p> <p>Validations: - Model group created with correct name - Models assigned with priorities - Duplicate groups handled appropriately - Can retrieve group with <code>/api/model-groups/{group_name}</code></p>"},{"location":"testing/integration-tests/#team-creation","title":"Team Creation","text":"<p>Endpoint: <code>/api/teams/create</code></p> <p>Test Case: Create team with model groups and credits <pre><code>payload = {\n    \"organization_id\": \"org_demo_001\",\n    \"team_id\": \"team_demo_engineering\",\n    \"team_alias\": \"Demo Engineering Team\",\n    \"model_groups\": [\"ChatAgent\", \"AnalysisAgent\"],\n    \"credits_allocated\": 100,\n    \"metadata\": {\"department\": \"engineering\"}\n}\nresponse = requests.post(f\"{BASE_URL}/api/teams/create\", json=payload)\n</code></pre></p> <p>Validations: - Team created in SaaS API database - Team created in LiteLLM (for full integration test) - Virtual API key generated - Model groups assigned correctly - Credits allocated - Metadata persisted</p>"},{"location":"testing/integration-tests/#credit-management","title":"Credit Management","text":"<p>Endpoints: - <code>/api/credits/teams/{team_id}/balance</code> - <code>/api/credits/teams/{team_id}/add</code></p> <p>Test Case: Check and modify credit balance <pre><code># Check balance\nresponse = requests.get(f\"{BASE_URL}/api/credits/teams/{team_id}/balance\")\nbalance = response.json()\n\n# Add credits\npayload = {\"credits\": 50, \"reason\": \"Test allocation\"}\nresponse = requests.post(f\"{BASE_URL}/api/credits/teams/{team_id}/add\", json=payload)\n</code></pre></p> <p>Validations: - Initial balance matches allocation - Credits can be added - Balance updates correctly - Transaction history maintained</p>"},{"location":"testing/integration-tests/#litellm-integration-full-test-only","title":"LiteLLM Integration (Full Test Only)","text":"<p>What's Tested:</p> <ol> <li>Virtual Key Generation:</li> <li>Team gets unique virtual key</li> <li>Key is linked to team in LiteLLM database</li> <li> <p>Key can be used for API calls</p> </li> <li> <p>Model Access:</p> </li> <li>Team can only access assigned model groups</li> <li>Model resolution works through virtual key</li> <li> <p>Fallback models used based on priority</p> </li> <li> <p>Database Synchronization:</p> </li> <li>Team exists in both SaaS and LiteLLM databases</li> <li>Credit information synchronized</li> <li>Model group assignments match</li> </ol>"},{"location":"testing/integration-tests/#using-test_full_integrationpy","title":"Using test_full_integration.py","text":""},{"location":"testing/integration-tests/#script-structure","title":"Script Structure","text":"<p>The script follows a sequential workflow:</p> <ol> <li>Setup Phase: Create organization and model groups</li> <li>Integration Phase: Create team with LiteLLM</li> <li>Verification Phase: Confirm team in both databases</li> <li>Display Phase: Show results and next steps</li> </ol>"},{"location":"testing/integration-tests/#key-features","title":"Key Features","text":""},{"location":"testing/integration-tests/#idempotency","title":"Idempotency","text":"<p>The script handles existing resources: <pre><code>if response.status_code == 400 and \"already exists\" in response.text:\n    print(\"Organization already exists (OK)\")\n</code></pre></p> <p>This allows you to run the script multiple times without errors.</p>"},{"location":"testing/integration-tests/#error-handling","title":"Error Handling","text":"<p>Comprehensive error messages for common issues: <pre><code>if response.status_code == 500:\n    print(\"\\n   ERROR: LiteLLM integration failed!\")\n    print(\"   Possible causes:\")\n    print(\"   - LiteLLM proxy not running\")\n    print(\"   - LiteLLM database not accessible\")\n    print(\"   - Master key incorrect\")\n</code></pre></p>"},{"location":"testing/integration-tests/#informative-output","title":"Informative Output","text":"<p>Provides detailed information about created resources: <pre><code>print(\"\\nKEY INFORMATION:\")\nprint(f\"  Team ID: {team_id}\")\nprint(f\"  Virtual Key: {virtual_key[:30]}...\")\nprint(f\"  Model Groups: {', '.join(model_groups_assigned)}\")\nprint(f\"  Credits Allocated: {credits}\")\n</code></pre></p>"},{"location":"testing/integration-tests/#customizing-the-test","title":"Customizing the Test","text":"<p>You can modify the script to test different scenarios:</p>"},{"location":"testing/integration-tests/#change-base-url","title":"Change Base URL","text":"<pre><code># Test against different environment\nBASE_URL = \"http://staging.example.com\"\n</code></pre>"},{"location":"testing/integration-tests/#modify-team-configuration","title":"Modify Team Configuration","text":"<pre><code>payload = {\n    \"organization_id\": \"org_demo_001\",\n    \"team_id\": \"team_custom_001\",  # Custom team ID\n    \"team_alias\": \"Custom Team Name\",\n    \"model_groups\": [\"YourModelGroup\"],  # Your model groups\n    \"credits_allocated\": 500,  # Custom credit amount\n    \"metadata\": {\"custom_field\": \"custom_value\"}\n}\n</code></pre>"},{"location":"testing/integration-tests/#add-more-model-groups","title":"Add More Model Groups","text":"<pre><code>model_groups = [\n    {\n        \"group_name\": \"CustomAgent\",\n        \"display_name\": \"Custom Agent\",\n        \"description\": \"Your custom agent\",\n        \"models\": [\n            {\"model_name\": \"gpt-4\", \"priority\": 0}\n        ]\n    }\n]\n</code></pre>"},{"location":"testing/integration-tests/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"testing/integration-tests/#github-actions-example","title":"GitHub Actions Example","text":"<p>Create <code>.github/workflows/test.yml</code>:</p> <pre><code>name: Integration Tests\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_DB: litellm\n          POSTGRES_USER: litellm_user\n          POSTGRES_PASSWORD: litellm_password\n        ports:\n          - 5432:5432\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n      redis:\n        image: redis:7-alpine\n        ports:\n          - 6379:6379\n        options: &gt;-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install uv\n      run: curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    - name: Install dependencies\n      run: |\n        uv venv\n        source .venv/bin/activate\n        uv pip install -r requirements.txt\n\n    - name: Run migrations\n      run: |\n        source .venv/bin/activate\n        ./scripts/run_migrations.sh\n      env:\n        DATABASE_URL: postgresql://litellm_user:litellm_password@localhost:5432/litellm\n\n    - name: Start LiteLLM\n      run: |\n        source .venv/bin/activate\n        python scripts/start_local.py &amp;\n        sleep 10\n      env:\n        DATABASE_URL: postgresql://litellm_user:litellm_password@localhost:5432/litellm\n        LITELLM_MASTER_KEY: ${{ secrets.LITELLM_MASTER_KEY }}\n\n    - name: Start SaaS API\n      run: |\n        source .venv/bin/activate\n        python scripts/start_saas_api.py &amp;\n        sleep 5\n\n    - name: Run minimal tests\n      run: |\n        source .venv/bin/activate\n        python scripts/test_minimal_version.py\n\n    - name: Run full integration tests\n      run: |\n        source .venv/bin/activate\n        python scripts/test_full_integration.py\n      env:\n        LITELLM_MASTER_KEY: ${{ secrets.LITELLM_MASTER_KEY }}\n</code></pre>"},{"location":"testing/integration-tests/#gitlab-ci-example","title":"GitLab CI Example","text":"<p>Create <code>.gitlab-ci.yml</code>:</p> <pre><code>variables:\n  POSTGRES_DB: litellm\n  POSTGRES_USER: litellm_user\n  POSTGRES_PASSWORD: litellm_password\n  DATABASE_URL: postgresql://litellm_user:litellm_password@postgres:5432/litellm\n\nservices:\n  - postgres:15\n  - redis:7-alpine\n\ntest:\n  image: python:3.11\n  before_script:\n    - curl -LsSf https://astral.sh/uv/install.sh | sh\n    - export PATH=\"$HOME/.local/bin:$PATH\"\n    - uv venv\n    - source .venv/bin/activate\n    - uv pip install -r requirements.txt\n    - ./scripts/run_migrations.sh\n  script:\n    - python scripts/start_local.py &amp;\n    - sleep 10\n    - python scripts/start_saas_api.py &amp;\n    - sleep 5\n    - python scripts/test_minimal_version.py\n    - python scripts/test_full_integration.py\n</code></pre>"},{"location":"testing/integration-tests/#docker-compose-for-ci","title":"Docker Compose for CI","text":"<p>For CI environments that support Docker Compose:</p> <pre><code>version: '3.8'\n\nservices:\n  test:\n    build: .\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://litellm_user:litellm_password@postgres:5432/litellm\n      REDIS_HOST: redis\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\n    command: |\n      sh -c \"\n        ./scripts/run_migrations.sh &amp;&amp;\n        python scripts/start_local.py &amp;\n        sleep 10 &amp;&amp;\n        python scripts/start_saas_api.py &amp;\n        sleep 5 &amp;&amp;\n        python scripts/test_minimal_version.py &amp;&amp;\n        python scripts/test_full_integration.py\n      \"\n\n  postgres:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: litellm_user\n      POSTGRES_PASSWORD: litellm_password\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U litellm_user\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n</code></pre> <p>Run in CI: <pre><code>docker compose -f docker-compose.test.yml up --abort-on-container-exit\n</code></pre></p>"},{"location":"testing/integration-tests/#test-data-cleanup","title":"Test Data Cleanup","text":""},{"location":"testing/integration-tests/#handling-test-data","title":"Handling Test Data","text":"<p>Integration tests create data in the database. Options for cleanup:</p>"},{"location":"testing/integration-tests/#1-use-test-specific-ids","title":"1. Use Test-Specific IDs","text":"<p>Prefix all test entities with <code>test_</code>: <pre><code>\"organization_id\": \"org_test_001\"\n\"team_id\": \"team_test_hr\"\n</code></pre></p> <p>This makes it easy to identify and remove test data.</p>"},{"location":"testing/integration-tests/#2-cleanup-script","title":"2. Cleanup Script","text":"<p>Create <code>scripts/cleanup_test_data.py</code>: <pre><code>import psycopg2\nfrom config.settings import settings\n\ndef cleanup():\n    conn = psycopg2.connect(settings.database_url)\n    cur = conn.cursor()\n\n    # Delete test organizations and cascade\n    cur.execute(\"DELETE FROM organizations WHERE organization_id LIKE 'org_test_%'\")\n\n    conn.commit()\n    conn.close()\n\nif __name__ == \"__main__\":\n    cleanup()\n</code></pre></p>"},{"location":"testing/integration-tests/#3-use-transactions-future-enhancement","title":"3. Use Transactions (Future Enhancement)","text":"<p>Wrap tests in database transactions that rollback: <pre><code>@pytest.fixture(scope=\"function\")\ndef db_transaction():\n    conn = get_connection()\n    conn.autocommit = False\n    yield conn\n    conn.rollback()\n    conn.close()\n</code></pre></p>"},{"location":"testing/integration-tests/#debugging-failed-tests","title":"Debugging Failed Tests","text":""},{"location":"testing/integration-tests/#enable-verbose-output","title":"Enable Verbose Output","text":"<pre><code># Run with detailed output\npython scripts/test_full_integration.py 2&gt;&amp;1 | tee test_output.log\n</code></pre>"},{"location":"testing/integration-tests/#check-service-status","title":"Check Service Status","text":"<pre><code># Check if services are running\ncurl http://localhost:8002/health  # LiteLLM\ncurl http://localhost:8003/health  # SaaS API\n\n# Check Docker services\ndocker compose ps\n</code></pre>"},{"location":"testing/integration-tests/#review-logs","title":"Review Logs","text":"<pre><code># LiteLLM logs\ndocker compose logs litellm\n\n# Database logs\ndocker compose logs postgres\n\n# Redis logs\ndocker compose logs redis\n</code></pre>"},{"location":"testing/integration-tests/#common-issues","title":"Common Issues","text":"<p>See Troubleshooting for detailed solutions to common test failures.</p>"},{"location":"testing/integration-tests/#best-practices","title":"Best Practices","text":""},{"location":"testing/integration-tests/#1-run-tests-before-commits","title":"1. Run Tests Before Commits","text":"<pre><code># Quick check\npython scripts/test_minimal_version.py\n\n# Full validation\npython scripts/test_full_integration.py\n</code></pre>"},{"location":"testing/integration-tests/#2-test-in-clean-environment","title":"2. Test in Clean Environment","text":"<pre><code># Reset database\ndocker compose down -v\n./scripts/docker_setup.sh\n./scripts/run_migrations.sh\n</code></pre>"},{"location":"testing/integration-tests/#3-verify-service-health-first","title":"3. Verify Service Health First","text":"<pre><code># Check all services before testing\ncurl http://localhost:8002/health\ncurl http://localhost:8003/health\ndocker compose ps\n</code></pre>"},{"location":"testing/integration-tests/#4-use-consistent-test-data","title":"4. Use Consistent Test Data","text":"<p>Maintain a set of standard test IDs and configurations for reproducible tests.</p>"},{"location":"testing/integration-tests/#5-document-test-changes","title":"5. Document Test Changes","text":"<p>When modifying integration tests, update this documentation with: - New scenarios covered - Changed endpoints - Updated prerequisites - New expected outputs</p>"},{"location":"testing/integration-tests/#related-documentation","title":"Related Documentation","text":"<ul> <li>Testing Overview - General testing strategy</li> <li>Troubleshooting - Debug failed tests</li> <li>API Reference - API endpoint details</li> <li>Getting Started - Initial setup</li> </ul>"},{"location":"testing/overview/","title":"Testing Overview","text":""},{"location":"testing/overview/#testing-philosophy","title":"Testing Philosophy","text":"<p>The SaasLiteLLM platform follows a pragmatic testing approach that balances comprehensive coverage with practical development needs:</p> <ul> <li>Functional Testing First: Focus on integration tests that verify end-to-end functionality</li> <li>Real-World Scenarios: Tests reflect actual use cases (team creation, job tracking, credit management)</li> <li>Database-Centric: Most tests validate database interactions and API responses</li> <li>Developer-Friendly: Tests are easy to run and provide clear feedback</li> </ul>"},{"location":"testing/overview/#testing-strategy","title":"Testing Strategy","text":""},{"location":"testing/overview/#current-testing-approach","title":"Current Testing Approach","text":"<p>The project currently emphasizes integration and functional testing over unit testing. This approach is well-suited for a SaaS platform where:</p> <ol> <li>API Contracts Matter: Testing complete request/response flows is more valuable than isolated unit tests</li> <li>Database Interactions: Most business logic involves database operations</li> <li>Multi-Service Architecture: Testing the interaction between LiteLLM proxy and SaaS API wrapper</li> </ol>"},{"location":"testing/overview/#what-gets-tested","title":"What Gets Tested","text":"<ul> <li>API Endpoints: All REST API endpoints for teams, organizations, model groups, and credits</li> <li>Database Operations: CRUD operations and data integrity</li> <li>LiteLLM Integration: Team creation, virtual key generation, and model assignment</li> <li>Credit System: Credit allocation, deduction, and balance tracking</li> <li>Job Tracking: Job creation, LLM call tracking, and cost aggregation</li> </ul>"},{"location":"testing/overview/#test-types","title":"Test Types","text":""},{"location":"testing/overview/#1-unit-tests","title":"1. Unit Tests","text":"<p>Location: <code>/tests/</code> Purpose: Test individual components and modules in isolation</p> <p>Current Coverage: - Basic import tests (<code>test_main.py</code>) - Settings and configuration loading</p> <p>Running Unit Tests: <pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Install test dependencies\nuv pip install pytest pytest-asyncio\n\n# Run all unit tests\npytest tests/\n\n# Run with verbose output\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_main.py\n\n# Run with coverage\npytest tests/ --cov=src --cov-report=html\n</code></pre></p>"},{"location":"testing/overview/#2-integration-tests","title":"2. Integration Tests","text":"<p>Location: <code>/scripts/</code> Purpose: Test complete workflows across multiple services</p> <p>Test Scripts:</p>"},{"location":"testing/overview/#test_minimal_versionpy","title":"<code>test_minimal_version.py</code>","text":"<p>Tests core functionality without LiteLLM integration: - Health checks - Organization creation - Model group setup - Team creation with credits - Credit management operations</p> <pre><code># Prerequisites: SaaS API must be running\npython scripts/test_minimal_version.py\n</code></pre>"},{"location":"testing/overview/#test_full_integrationpy","title":"<code>test_full_integration.py</code>","text":"<p>Tests complete LiteLLM integration: - Team creation in LiteLLM - Virtual key generation - Model group assignment - Credit allocation - Database synchronization</p> <pre><code># Prerequisites: Both LiteLLM and SaaS API must be running\npython scripts/test_full_integration.py\n</code></pre> <p>See Integration Tests for detailed documentation.</p>"},{"location":"testing/overview/#3-end-to-end-tests","title":"3. End-to-End Tests","text":"<p>Purpose: Simulate real user workflows from start to finish</p> <p>Scenarios: - Complete job lifecycle (create \u2192 execute \u2192 complete) - Team onboarding workflow - Multi-call job with cost tracking - Credit exhaustion scenarios</p> <p>Running E2E Tests: <pre><code># Ensure all services are running\n./scripts/docker_setup.sh\npython scripts/start_local.py  # Terminal 1\npython scripts/start_saas_api.py  # Terminal 2\n\n# Run integration test (acts as E2E test)\npython scripts/test_full_integration.py\n</code></pre></p>"},{"location":"testing/overview/#testing-tools","title":"Testing Tools","text":""},{"location":"testing/overview/#primary-tools","title":"Primary Tools","text":""},{"location":"testing/overview/#pytest","title":"pytest","text":"<ul> <li>Version: 7.4.0+</li> <li>Purpose: Test framework and runner</li> <li>Configuration: No pytest.ini currently; uses defaults</li> <li>Key Features:</li> <li>Simple assertion syntax</li> <li>Automatic test discovery</li> <li>Rich failure reporting</li> <li>Fixture support for setup/teardown</li> </ul>"},{"location":"testing/overview/#pytest-asyncio","title":"pytest-asyncio","text":"<ul> <li>Purpose: Testing async FastAPI endpoints</li> <li>Usage: Handles async test functions and fixtures</li> </ul>"},{"location":"testing/overview/#supporting-tools","title":"Supporting Tools","text":""},{"location":"testing/overview/#requests","title":"requests","text":"<ul> <li>Purpose: HTTP API testing</li> <li>Usage: All integration tests use requests library to call API endpoints</li> <li>Example:   <pre><code>response = requests.post(f\"{BASE_URL}/api/teams/create\", json=payload)\nassert response.status_code == 200\n</code></pre></li> </ul>"},{"location":"testing/overview/#docker-docker-compose","title":"Docker &amp; Docker Compose","text":"<ul> <li>Purpose: Test environment setup</li> <li>Services:</li> <li>PostgreSQL (database)</li> <li>Redis (caching)</li> <li>LiteLLM proxy</li> <li>SaaS API</li> </ul>"},{"location":"testing/overview/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"testing/overview/#black","title":"black","text":"<ul> <li>Purpose: Code formatting</li> <li>Configuration: Line length 88 (Python standard)</li> <li>Usage:   <pre><code>black src/ tests/\n</code></pre></li> </ul>"},{"location":"testing/overview/#ruff","title":"ruff","text":"<ul> <li>Purpose: Fast Python linter</li> <li>Usage:   <pre><code>ruff check src/ tests/\n</code></pre></li> </ul>"},{"location":"testing/overview/#mypy-optional","title":"mypy (optional)","text":"<ul> <li>Purpose: Static type checking</li> <li>Usage:   <pre><code>mypy src/\n</code></pre></li> </ul>"},{"location":"testing/overview/#running-tests","title":"Running Tests","text":""},{"location":"testing/overview/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install Dependencies:    <pre><code>source .venv/bin/activate\nuv pip install pytest pytest-asyncio\n</code></pre></p> </li> <li> <p>Start Docker Services:    <pre><code>./scripts/docker_setup.sh\n</code></pre></p> </li> <li> <p>Start Application Services:    <pre><code># Terminal 1: LiteLLM Backend\npython scripts/start_local.py\n\n# Terminal 2: SaaS API\npython scripts/start_saas_api.py\n</code></pre></p> </li> </ol>"},{"location":"testing/overview/#quick-test-commands","title":"Quick Test Commands","text":"<pre><code># Run all unit tests\npytest tests/\n\n# Run unit tests with verbose output\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_main.py\n\n# Run integration tests\npython scripts/test_minimal_version.py\npython scripts/test_full_integration.py\n\n# Run with pytest verbose mode\npytest tests/ -vv\n\n# Run tests matching pattern\npytest tests/ -k \"test_import\"\n\n# Show print statements during tests\npytest tests/ -s\n</code></pre>"},{"location":"testing/overview/#test-output","title":"Test Output","text":"<p>Successful test output example: <pre><code>============================= test session starts ==============================\nplatform darwin -- Python 3.12.0, pytest-7.4.0, pluggy-1.0.0\nrootdir: /Users/keithelliott/repos/SaasLiteLLM\ncollected 2 items\n\ntests/test_main.py ..                                                    [100%]\n\n============================== 2 passed in 0.45s ===============================\n</code></pre></p>"},{"location":"testing/overview/#test-organization","title":"Test Organization","text":""},{"location":"testing/overview/#directory-structure","title":"Directory Structure","text":"<pre><code>SaasLiteLLM/\n\u251c\u2500\u2500 tests/                          # Unit tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_main.py               # Basic import and settings tests\n\u2502\n\u251c\u2500\u2500 scripts/                        # Integration test scripts\n\u2502   \u251c\u2500\u2500 test_minimal_version.py    # Core functionality tests\n\u2502   \u2514\u2500\u2500 test_full_integration.py   # Full LiteLLM integration tests\n\u2502\n\u2514\u2500\u2500 src/                           # Application code\n    \u251c\u2500\u2500 saas_api.py                # Main SaaS API\n    \u251c\u2500\u2500 models/                    # Database models\n    \u2514\u2500\u2500 api/                       # API endpoints\n</code></pre>"},{"location":"testing/overview/#test-naming-conventions","title":"Test Naming Conventions","text":"<ul> <li>Unit test files: <code>test_*.py</code> in <code>tests/</code> directory</li> <li>Unit test functions: <code>test_*</code> prefix (e.g., <code>test_import_main</code>)</li> <li>Integration scripts: <code>test_*.py</code> in <code>scripts/</code> directory</li> <li>Test descriptions: Clear docstrings explaining what is tested</li> </ul>"},{"location":"testing/overview/#best-practices","title":"Best Practices","text":""},{"location":"testing/overview/#writing-tests","title":"Writing Tests","text":"<ol> <li> <p>Clear Test Names: Use descriptive names that explain what is being tested    <pre><code>def test_create_organization():\n    \"\"\"Create a test organization\"\"\"\n</code></pre></p> </li> <li> <p>Arrange-Act-Assert Pattern:    <pre><code># Arrange\npayload = {\"organization_id\": \"org_test\", \"name\": \"Test Org\"}\n\n# Act\nresponse = requests.post(f\"{BASE_URL}/api/organizations/create\", json=payload)\n\n# Assert\nassert response.status_code == 200\nassert response.json()[\"organization_id\"] == \"org_test\"\n</code></pre></p> </li> <li> <p>Handle Edge Cases: Test both success and failure scenarios    <pre><code>if response.status_code == 400 and \"already exists\" in response.text:\n    print(\"Organization already exists (OK)\")\n    return\n</code></pre></p> </li> <li> <p>Use Meaningful Assertions: Provide helpful error messages    <pre><code>assert response.status_code == 200, f\"Failed to create org: {response.text}\"\n</code></pre></p> </li> </ol>"},{"location":"testing/overview/#test-data-management","title":"Test Data Management","text":"<ol> <li> <p>Use Consistent Test IDs: Prefix test entities with <code>test_</code> or use unique identifiers    <pre><code>\"organization_id\": \"org_test_001\"\n\"team_id\": \"team_test_hr\"\n</code></pre></p> </li> <li> <p>Clean Up Test Data: Handle cases where test data already exists</p> </li> <li>Isolate Tests: Each test should be independent and not rely on others</li> </ol>"},{"location":"testing/overview/#database-testing","title":"Database Testing","text":"<ol> <li>Use Test Database: Ensure tests run against local development database</li> <li>Check Data Persistence: Verify data is correctly saved and retrievable</li> <li>Test Transactions: Ensure database operations are atomic</li> </ol>"},{"location":"testing/overview/#coverage-goals","title":"Coverage Goals","text":"<p>While the project doesn't currently enforce strict coverage metrics, aim for:</p> <ul> <li>Critical Paths: 100% coverage of core functionality</li> <li>API Endpoints: All endpoints should have integration tests</li> <li>Business Logic: Key operations (credits, jobs, teams) fully tested</li> <li>Error Handling: Test failure scenarios and edge cases</li> </ul>"},{"location":"testing/overview/#next-steps","title":"Next Steps","text":"<p>To improve the testing infrastructure:</p> <ol> <li>Add More Unit Tests:</li> <li>Test individual model classes</li> <li>Test utility functions</li> <li> <p>Test API endpoint handlers</p> </li> <li> <p>Create Test Fixtures:</p> </li> <li>Add <code>conftest.py</code> with reusable fixtures</li> <li>Create database setup/teardown fixtures</li> <li> <p>Add test data generators</p> </li> <li> <p>Add Coverage Reporting:    <pre><code>pytest --cov=src --cov-report=html tests/\n</code></pre></p> </li> <li> <p>Set Up CI/CD:</p> </li> <li>Automated test runs on push/PR</li> <li>Coverage reporting in CI</li> <li> <p>Test status badges</p> </li> <li> <p>Add Performance Tests:</p> </li> <li>Load testing for API endpoints</li> <li>Database query performance</li> <li>Concurrent request handling</li> </ol>"},{"location":"testing/overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Integration Tests - Detailed integration testing guide</li> <li>Troubleshooting - Common issues and solutions</li> <li>API Reference - API endpoint documentation</li> </ul>"},{"location":"testing/troubleshooting/","title":"Testing Troubleshooting Guide","text":"<p>This guide helps you diagnose and resolve common issues when running tests in SaasLiteLLM.</p>"},{"location":"testing/troubleshooting/#quick-diagnostics-checklist","title":"Quick Diagnostics Checklist","text":"<p>Before diving into specific issues, run through this checklist:</p> <pre><code># 1. Check Docker services\ndocker compose ps\n\n# 2. Check service health\ncurl http://localhost:8002/health  # LiteLLM\ncurl http://localhost:8003/health  # SaaS API\n\n# 3. Check database connection\ndocker exec -it litellm-postgres pg_isready -U litellm_user -d litellm\n\n# 4. Check Redis connection\ndocker exec -it litellm-redis redis-cli ping\n\n# 5. Review recent logs\ndocker compose logs --tail=50 postgres\ndocker compose logs --tail=50 redis\n</code></pre> <p>If any of these fail, see the relevant section below.</p>"},{"location":"testing/troubleshooting/#common-test-failures","title":"Common Test Failures","text":""},{"location":"testing/troubleshooting/#1-connection-refused-errors","title":"1. Connection Refused Errors","text":""},{"location":"testing/troubleshooting/#symptom","title":"Symptom","text":"<pre><code>Connection Error: Could not connect to http://localhost:8003\nrequests.exceptions.ConnectionError: Connection refused\n</code></pre>"},{"location":"testing/troubleshooting/#cause","title":"Cause","text":"<p>The SaaS API is not running or not accessible on port 8003.</p>"},{"location":"testing/troubleshooting/#solution","title":"Solution","text":"<p>Step 1: Check if the process is running <pre><code># Check for Python processes on port 8003\nlsof -i :8003\n\n# Check for Python processes on port 8002 (LiteLLM)\nlsof -i :8002\n</code></pre></p> <p>Step 2: Start the required services <pre><code># Terminal 1: Start LiteLLM backend\nsource .venv/bin/activate\npython scripts/start_local.py\n\n# Terminal 2: Start SaaS API\nsource .venv/bin/activate\npython scripts/start_saas_api.py\n</code></pre></p> <p>Step 3: Verify services are accessible <pre><code># Should return {\"status\": \"healthy\"}\ncurl http://localhost:8003/health\ncurl http://localhost:8002/health\n</code></pre></p>"},{"location":"testing/troubleshooting/#prevention","title":"Prevention","text":"<p>Create a startup script that checks prerequisites: <pre><code>#!/bin/bash\n# scripts/start_all_services.sh\n\necho \"Starting all services for testing...\"\n\n# Check Docker services\nif ! docker compose ps postgres | grep -q \"Up\"; then\n    echo \"Starting Docker services...\"\n    ./scripts/docker_setup.sh\nfi\n\n# Start LiteLLM in background\necho \"Starting LiteLLM...\"\npython scripts/start_local.py &amp;\nLITELLM_PID=$!\n\n# Wait for LiteLLM\nsleep 10\nif ! curl -s http://localhost:8002/health &gt; /dev/null; then\n    echo \"Failed to start LiteLLM\"\n    kill $LITELLM_PID\n    exit 1\nfi\n\n# Start SaaS API in background\necho \"Starting SaaS API...\"\npython scripts/start_saas_api.py &amp;\nSAAS_PID=$!\n\n# Wait for SaaS API\nsleep 5\nif ! curl -s http://localhost:8003/health &gt; /dev/null; then\n    echo \"Failed to start SaaS API\"\n    kill $LITELLM_PID $SAAS_PID\n    exit 1\nfi\n\necho \"All services running!\"\necho \"LiteLLM PID: $LITELLM_PID\"\necho \"SaaS API PID: $SAAS_PID\"\n</code></pre></p>"},{"location":"testing/troubleshooting/#2-database-connection-issues","title":"2. Database Connection Issues","text":""},{"location":"testing/troubleshooting/#symptom-a-database-not-running","title":"Symptom A: Database Not Running","text":"<pre><code>Failed to connect to database\npsycopg2.OperationalError: could not connect to server\nconnection refused\n</code></pre>"},{"location":"testing/troubleshooting/#solution-for-symptom-a","title":"Solution for Symptom A","text":"<p>Step 1: Check if PostgreSQL container is running <pre><code>docker compose ps postgres\n</code></pre></p> <p>Step 2: If not running, start Docker services <pre><code>./scripts/docker_setup.sh\n</code></pre></p> <p>Step 3: Verify PostgreSQL is accepting connections <pre><code># Should output \"accepting connections\"\ndocker exec litellm-postgres pg_isready -U litellm_user -d litellm\n\n# Test connection with psql\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm -c \"SELECT version();\"\n</code></pre></p> <p>Step 4: Check PostgreSQL logs for errors <pre><code>docker compose logs postgres | tail -50\n</code></pre></p>"},{"location":"testing/troubleshooting/#symptom-b-wrong-database-credentials","title":"Symptom B: Wrong Database Credentials","text":"<pre><code>FATAL: password authentication failed for user \"litellm_user\"\n</code></pre>"},{"location":"testing/troubleshooting/#solution-for-symptom-b","title":"Solution for Symptom B","text":"<p>Step 1: Verify environment variables <pre><code># Check .env file\ncat .env | grep DATABASE_URL\ncat .env | grep POSTGRES_\n</code></pre></p> <p>Step 2: Ensure credentials match docker-compose.yml <pre><code># Check docker-compose.yml settings\ncat docker-compose.yml | grep -A 5 \"postgres:\"\n</code></pre></p> <p>Step 3: Recreate database with correct credentials <pre><code># Stop and remove volumes\ndocker compose down -v\n\n# Restart with fresh database\n./scripts/docker_setup.sh\n</code></pre></p>"},{"location":"testing/troubleshooting/#symptom-c-database-missing-tables","title":"Symptom C: Database Missing Tables","text":"<pre><code>relation \"organizations\" does not exist\npsycopg2.errors.UndefinedTable\n</code></pre>"},{"location":"testing/troubleshooting/#solution-for-symptom-c","title":"Solution for Symptom C","text":"<p>Step 1: Run database migrations <pre><code>./scripts/run_migrations.sh\n</code></pre></p> <p>Step 2: Verify tables exist <pre><code>docker exec -it litellm-postgres psql -U litellm_user -d litellm -c \"\\dt\"\n</code></pre></p> <p>Expected tables: - organizations - model_groups - model_group_models - teams - team_model_groups - team_credits - team_credit_transactions - jobs - llm_calls</p> <p>Step 3: If migrations fail, check migration files <pre><code>ls -la scripts/migrations/\n</code></pre></p> <p>Step 4: Manually run migrations if needed <pre><code>for file in scripts/migrations/*.sql; do\n    echo \"Running $file...\"\n    docker exec -i litellm-postgres psql -U litellm_user -d litellm &lt; \"$file\"\ndone\n</code></pre></p>"},{"location":"testing/troubleshooting/#3-litellm-integration-failures","title":"3. LiteLLM Integration Failures","text":""},{"location":"testing/troubleshooting/#symptom_1","title":"Symptom","text":"<pre><code>ERROR: LiteLLM integration failed!\nResponse: 500 Internal Server Error\n\nPossible causes:\n- LiteLLM proxy not running\n- LiteLLM database not accessible\n- Master key incorrect\n</code></pre>"},{"location":"testing/troubleshooting/#cause_1","title":"Cause","text":"<p>Communication between SaaS API and LiteLLM proxy is broken.</p>"},{"location":"testing/troubleshooting/#solution_1","title":"Solution","text":"<p>Step 1: Verify LiteLLM is running <pre><code>curl http://localhost:8002/health\n</code></pre></p> <p>Step 2: Check LiteLLM can access database <pre><code># View LiteLLM startup logs\ndocker compose logs litellm 2&gt;&amp;1 | grep -i \"database\"\n</code></pre></p> <p>Step 3: Verify master key configuration <pre><code># Check .env file\ncat .env | grep LITELLM_MASTER_KEY\n\n# Ensure it's set in environment\necho $LITELLM_MASTER_KEY\n</code></pre></p> <p>Step 4: Test LiteLLM API directly <pre><code># Should return information about the key endpoint\ncurl -X POST http://localhost:8002/key/info \\\n  -H \"Authorization: Bearer sk-local-dev-master-key-change-me\" \\\n  -H \"Content-Type: application/json\"\n</code></pre></p> <p>Step 5: Check LiteLLM configuration <pre><code># Verify config file exists\ncat src/config/litellm_config.yaml\n\n# Check for syntax errors\npython -c \"import yaml; yaml.safe_load(open('src/config/litellm_config.yaml'))\"\n</code></pre></p> <p>Step 6: Restart LiteLLM with verbose logging <pre><code># Stop current instance\npkill -f \"litellm\"\n\n# Start with debug mode\nsource .venv/bin/activate\nlitellm --config src/config/litellm_config.yaml --port 8002 --detailed_debug\n</code></pre></p>"},{"location":"testing/troubleshooting/#4-already-exists-errors","title":"4. \"Already Exists\" Errors","text":""},{"location":"testing/troubleshooting/#symptom_2","title":"Symptom","text":"<pre><code>Status: 400\nResponse: {\"detail\": \"Organization with id 'org_test_001' already exists\"}\n</code></pre>"},{"location":"testing/troubleshooting/#cause_2","title":"Cause","text":"<p>Test data from previous runs still exists in the database.</p>"},{"location":"testing/troubleshooting/#is-this-a-problem","title":"Is This a Problem?","text":"<p>Usually No: The test scripts are designed to handle existing data: <pre><code>if response.status_code == 400 and \"already exists\" in response.text:\n    print(\"Organization already exists (OK)\")\n</code></pre></p>"},{"location":"testing/troubleshooting/#when-its-a-problem","title":"When It's a Problem","text":"<p>If you're testing creation logic specifically, or tests fail due to stale data:</p>"},{"location":"testing/troubleshooting/#solution_2","title":"Solution","text":"<p>Option 1: Clean up test data manually <pre><code>docker exec -it litellm-postgres psql -U litellm_user -d litellm &lt;&lt; EOF\nDELETE FROM team_credit_transactions WHERE team_id LIKE 'team_test%';\nDELETE FROM team_credits WHERE team_id LIKE 'team_test%';\nDELETE FROM team_model_groups WHERE team_id LIKE 'team_test%';\nDELETE FROM teams WHERE team_id LIKE 'team_test%';\nDELETE FROM model_group_models WHERE group_id IN (SELECT id FROM model_groups WHERE group_name LIKE '%Test%');\nDELETE FROM model_groups WHERE group_name LIKE '%Test%';\nDELETE FROM organizations WHERE organization_id LIKE 'org_test%';\nEOF\n</code></pre></p> <p>Option 2: Reset entire database (nuclear option) <pre><code># Stop containers and remove volumes\ndocker compose down -v\n\n# Restart fresh\n./scripts/docker_setup.sh\n\n# Recreate schema\n./scripts/run_migrations.sh\n</code></pre></p> <p>Option 3: Create cleanup script <pre><code># scripts/cleanup_test_data.py\n#!/usr/bin/env python3\nimport psycopg2\nfrom config.settings import settings\n\ndef cleanup():\n    \"\"\"Remove all test data\"\"\"\n    conn = psycopg2.connect(settings.database_url)\n    cur = conn.cursor()\n\n    print(\"Cleaning up test data...\")\n\n    # Delete in correct order (handle foreign keys)\n    tables_and_conditions = [\n        (\"team_credit_transactions\", \"team_id LIKE 'team_test%' OR team_id LIKE 'team_demo%'\"),\n        (\"team_credits\", \"team_id LIKE 'team_test%' OR team_id LIKE 'team_demo%'\"),\n        (\"team_model_groups\", \"team_id LIKE 'team_test%' OR team_id LIKE 'team_demo%'\"),\n        (\"teams\", \"team_id LIKE 'team_test%' OR team_id LIKE 'team_demo%'\"),\n        (\"organizations\", \"organization_id LIKE 'org_test%' OR organization_id LIKE 'org_demo%'\"),\n    ]\n\n    for table, condition in tables_and_conditions:\n        cur.execute(f\"DELETE FROM {table} WHERE {condition}\")\n        print(f\"  Deleted {cur.rowcount} rows from {table}\")\n\n    conn.commit()\n    conn.close()\n    print(\"Cleanup complete!\")\n\nif __name__ == \"__main__\":\n    cleanup()\n</code></pre></p> <p>Run before tests: <pre><code>python scripts/cleanup_test_data.py\npython scripts/test_full_integration.py\n</code></pre></p>"},{"location":"testing/troubleshooting/#5-import-errors","title":"5. Import Errors","text":""},{"location":"testing/troubleshooting/#symptom_3","title":"Symptom","text":"<pre><code>ImportError: No module named 'fastapi'\nModuleNotFoundError: No module named 'litellm'\n</code></pre>"},{"location":"testing/troubleshooting/#cause_3","title":"Cause","text":"<p>Dependencies not installed or virtual environment not activated.</p>"},{"location":"testing/troubleshooting/#solution_3","title":"Solution","text":"<p>Step 1: Activate virtual environment <pre><code>source .venv/bin/activate\n</code></pre></p> <p>Step 2: Verify Python version <pre><code>python --version  # Should be 3.11 or higher\n</code></pre></p> <p>Step 3: Install dependencies <pre><code># Install core dependencies\nuv pip install litellm[proxy] fastapi uvicorn[standard] psycopg2-binary sqlalchemy\n\n# Install test dependencies\nuv pip install pytest pytest-asyncio\n\n# Or install all from pyproject.toml\nuv pip install -e \".[dev]\"\n</code></pre></p> <p>Step 4: Verify installation <pre><code>python -c \"import litellm; print(litellm.__version__)\"\npython -c \"import fastapi; print(fastapi.__version__)\"\n</code></pre></p>"},{"location":"testing/troubleshooting/#6-port-already-in-use","title":"6. Port Already in Use","text":""},{"location":"testing/troubleshooting/#symptom_4","title":"Symptom","text":"<pre><code>ERROR: Address already in use\nOSError: [Errno 48] Address already in use\n</code></pre>"},{"location":"testing/troubleshooting/#cause_4","title":"Cause","text":"<p>Another process is using port 8002 or 8003.</p>"},{"location":"testing/troubleshooting/#solution_4","title":"Solution","text":"<p>Step 1: Find the process using the port <pre><code># Check port 8002 (LiteLLM)\nlsof -i :8002\n\n# Check port 8003 (SaaS API)\nlsof -i :8003\n</code></pre></p> <p>Step 2: Kill the process <pre><code># Kill by PID\nkill -9 &lt;PID&gt;\n\n# Or kill all Python processes using these ports\npkill -f \"start_local.py\"\npkill -f \"start_saas_api.py\"\n</code></pre></p> <p>Step 3: Verify ports are free <pre><code>lsof -i :8002  # Should return nothing\nlsof -i :8003  # Should return nothing\n</code></pre></p> <p>Step 4: Restart services <pre><code>python scripts/start_local.py &amp;\nsleep 10\npython scripts/start_saas_api.py &amp;\n</code></pre></p>"},{"location":"testing/troubleshooting/#7-redis-connection-failures","title":"7. Redis Connection Failures","text":""},{"location":"testing/troubleshooting/#symptom_5","title":"Symptom","text":"<pre><code>Redis connection failed\nredis.exceptions.ConnectionError: Error 61 connecting to localhost:6379\n</code></pre>"},{"location":"testing/troubleshooting/#cause_5","title":"Cause","text":"<p>Redis container not running or not accessible.</p>"},{"location":"testing/troubleshooting/#solution_5","title":"Solution","text":"<p>Step 1: Check Redis container <pre><code>docker compose ps redis\n</code></pre></p> <p>Step 2: If not running, start it <pre><code>docker compose up -d redis\n</code></pre></p> <p>Step 3: Test Redis connection <pre><code># Should return PONG\ndocker exec litellm-redis redis-cli ping\n</code></pre></p> <p>Step 4: Check Redis logs <pre><code>docker compose logs redis\n</code></pre></p> <p>Note: Redis is optional for basic functionality. If you don't need caching, you can disable it in configuration.</p>"},{"location":"testing/troubleshooting/#8-test-timeouts","title":"8. Test Timeouts","text":""},{"location":"testing/troubleshooting/#symptom_6","title":"Symptom","text":"<pre><code>Test timed out after 30 seconds\nTimeoutError: Operation timed out\n</code></pre>"},{"location":"testing/troubleshooting/#cause_6","title":"Cause","text":"<ul> <li>Services taking too long to start</li> <li>Database query hanging</li> <li>Network issues</li> </ul>"},{"location":"testing/troubleshooting/#solution_6","title":"Solution","text":"<p>Step 1: Increase wait times in test scripts <pre><code># In test script, increase sleep duration\nsleep 15  # Instead of sleep 10\n</code></pre></p> <p>Step 2: Check service resource usage <pre><code># Check Docker container resources\ndocker stats\n\n# Check system resources\ntop\n</code></pre></p> <p>Step 3: Restart Docker services <pre><code>docker compose restart postgres redis\n</code></pre></p> <p>Step 4: Check for blocking queries <pre><code># View active PostgreSQL connections\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm -c \"\nSELECT pid, usename, application_name, state, query\nFROM pg_stat_activity\nWHERE state != 'idle';\n\"\n</code></pre></p>"},{"location":"testing/troubleshooting/#9-authentication-failures","title":"9. Authentication Failures","text":""},{"location":"testing/troubleshooting/#symptom_7","title":"Symptom","text":"<pre><code>401 Unauthorized\nInvalid API key\n</code></pre>"},{"location":"testing/troubleshooting/#cause_7","title":"Cause","text":"<p>Incorrect or missing master key configuration.</p>"},{"location":"testing/troubleshooting/#solution_7","title":"Solution","text":"<p>Step 1: Verify master key in .env <pre><code>cat .env | grep LITELLM_MASTER_KEY\n</code></pre></p> <p>Step 2: Ensure key format is correct <pre><code># Should start with \"sk-\"\n# Example: sk-local-dev-master-key-change-me\n</code></pre></p> <p>Step 3: Update .env if needed <pre><code>echo \"LITELLM_MASTER_KEY=sk-local-dev-master-key-change-me\" &gt;&gt; .env\n</code></pre></p> <p>Step 4: Restart services to pick up new key <pre><code>pkill -f \"start_local.py\"\npython scripts/start_local.py\n</code></pre></p> <p>Step 5: Test authentication <pre><code>curl -X GET http://localhost:8002/health \\\n  -H \"Authorization: Bearer sk-local-dev-master-key-change-me\"\n</code></pre></p>"},{"location":"testing/troubleshooting/#debugging-failed-tests","title":"Debugging Failed Tests","text":""},{"location":"testing/troubleshooting/#enable-detailed-logging","title":"Enable Detailed Logging","text":""},{"location":"testing/troubleshooting/#for-test-scripts","title":"For Test Scripts","text":"<pre><code># Run with output redirection to capture all logs\npython scripts/test_full_integration.py 2&gt;&amp;1 | tee test_output.log\n</code></pre>"},{"location":"testing/troubleshooting/#for-pytest","title":"For pytest","text":"<pre><code># Verbose output with print statements\npytest tests/ -vv -s\n\n# Show local variables on failure\npytest tests/ -l\n\n# Stop on first failure\npytest tests/ -x\n</code></pre>"},{"location":"testing/troubleshooting/#for-litellm","title":"For LiteLLM","text":"<pre><code># Start with detailed debug mode\nlitellm --config src/config/litellm_config.yaml \\\n        --port 8002 \\\n        --detailed_debug\n</code></pre>"},{"location":"testing/troubleshooting/#for-saas-api","title":"For SaaS API","text":"<pre><code># Run with debug logging\nuvicorn src.saas_api:app \\\n  --host 0.0.0.0 \\\n  --port 8003 \\\n  --log-level debug\n</code></pre>"},{"location":"testing/troubleshooting/#inspect-database-state","title":"Inspect Database State","text":"<pre><code># Connect to database\ndocker exec -it litellm-postgres psql -U litellm_user -d litellm\n\n# Check organizations\nSELECT * FROM organizations;\n\n# Check teams\nSELECT * FROM teams;\n\n# Check credits\nSELECT * FROM team_credits;\n\n# Check model groups\nSELECT * FROM model_groups;\n\n# Check team-model assignments\nSELECT t.team_id, mg.group_name\nFROM teams t\nJOIN team_model_groups tmg ON t.id = tmg.team_id\nJOIN model_groups mg ON tmg.group_id = mg.id;\n</code></pre>"},{"location":"testing/troubleshooting/#check-api-endpoints-manually","title":"Check API Endpoints Manually","text":"<pre><code># Health check\ncurl http://localhost:8003/health\n\n# List organizations\ncurl http://localhost:8003/api/organizations\n\n# Get specific team\ncurl http://localhost:8003/api/teams/team_test_hr\n\n# Check team credits\ncurl http://localhost:8003/api/credits/teams/team_test_hr/balance\n\n# View API documentation\nopen http://localhost:8003/docs\n</code></pre>"},{"location":"testing/troubleshooting/#monitor-service-logs-in-real-time","title":"Monitor Service Logs in Real-Time","text":"<pre><code># Terminal 1: PostgreSQL logs\ndocker compose logs -f postgres\n\n# Terminal 2: Redis logs\ndocker compose logs -f redis\n\n# Terminal 3: All logs\ndocker compose logs -f\n</code></pre>"},{"location":"testing/troubleshooting/#environment-specific-issues","title":"Environment-Specific Issues","text":""},{"location":"testing/troubleshooting/#macos-issues","title":"macOS Issues","text":""},{"location":"testing/troubleshooting/#docker-desktop-not-running","title":"Docker Desktop Not Running","text":"<pre><code># Check Docker Desktop status\ndocker info\n\n# If not running, start Docker Desktop app\nopen -a Docker\n</code></pre>"},{"location":"testing/troubleshooting/#port-conflicts-on-macos","title":"Port Conflicts on macOS","text":"<pre><code># Check what's using the port\nlsof -i :8002 -i :8003 -i :5432 -i :6379\n\n# Kill conflicting processes\nsudo lsof -ti:8002 | xargs kill -9\n</code></pre>"},{"location":"testing/troubleshooting/#linux-issues","title":"Linux Issues","text":""},{"location":"testing/troubleshooting/#permission-denied-on-docker","title":"Permission Denied on Docker","text":"<pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and log back in, or run:\nnewgrp docker\n</code></pre>"},{"location":"testing/troubleshooting/#postgresql-port-conflict","title":"PostgreSQL Port Conflict","text":"<pre><code># If system PostgreSQL is running on 5432\nsudo systemctl stop postgresql\n\n# Or change port in docker-compose.yml\n</code></pre>"},{"location":"testing/troubleshooting/#windows-issues","title":"Windows Issues","text":""},{"location":"testing/troubleshooting/#wsl2-docker-integration","title":"WSL2 Docker Integration","text":"<pre><code># Ensure WSL2 integration is enabled in Docker Desktop\n# Settings &gt; Resources &gt; WSL Integration\n\n# Restart Docker Desktop\nwsl --shutdown\n# Start Docker Desktop again\n</code></pre>"},{"location":"testing/troubleshooting/#path-issues-in-wsl","title":"Path Issues in WSL","text":"<pre><code># Use WSL paths, not Windows paths\ncd /mnt/c/Users/YourName/repos/SaasLiteLLM  # Instead of C:\\Users\\...\n</code></pre>"},{"location":"testing/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"testing/troubleshooting/#slow-test-execution","title":"Slow Test Execution","text":""},{"location":"testing/troubleshooting/#cause_8","title":"Cause","text":"<ul> <li>Database performance</li> <li>Network latency</li> <li>Resource constraints</li> </ul>"},{"location":"testing/troubleshooting/#solution_8","title":"Solution","text":"<p>Step 1: Optimize database <pre><code># Analyze and vacuum database\ndocker exec litellm-postgres psql -U litellm_user -d litellm -c \"VACUUM ANALYZE;\"\n</code></pre></p> <p>Step 2: Check Docker resource allocation <pre><code># Docker Desktop &gt; Settings &gt; Resources\n# Increase CPU and Memory if available\n</code></pre></p> <p>Step 3: Use connection pooling Edit <code>src/models/database.py</code> to add connection pooling.</p> <p>Step 4: Profile slow queries <pre><code>-- Enable query logging in PostgreSQL\nALTER DATABASE litellm SET log_statement = 'all';\nALTER DATABASE litellm SET log_duration = on;\n</code></pre></p>"},{"location":"testing/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"testing/troubleshooting/#collecting-debug-information","title":"Collecting Debug Information","text":"<p>When reporting issues, include:</p> <ol> <li> <p>System Information:    <pre><code>uname -a\npython --version\ndocker --version\ndocker compose version\n</code></pre></p> </li> <li> <p>Service Status:    <pre><code>docker compose ps\ncurl http://localhost:8002/health\ncurl http://localhost:8003/health\n</code></pre></p> </li> <li> <p>Recent Logs:    <pre><code>docker compose logs --tail=100 &gt; docker_logs.txt\n</code></pre></p> </li> <li> <p>Environment Configuration:    <pre><code>cat .env | sed 's/=.*/=***/' &gt; env_sanitized.txt\n</code></pre></p> </li> <li> <p>Test Output:    <pre><code>python scripts/test_full_integration.py 2&gt;&amp;1 | tee test_output.txt\n</code></pre></p> </li> </ol>"},{"location":"testing/troubleshooting/#resources","title":"Resources","text":"<ul> <li>Project Documentation: Check other docs in <code>/docs</code></li> <li>LiteLLM Documentation: https://docs.litellm.ai</li> <li>FastAPI Documentation: https://fastapi.tiangolo.com</li> <li>PostgreSQL Documentation: https://www.postgresql.org/docs</li> </ul>"},{"location":"testing/troubleshooting/#creating-an-issue","title":"Creating an Issue","text":"<p>When creating an issue on GitHub:</p> <ol> <li>Use a descriptive title: \"Test fails: Connection refused on port 8003\"</li> <li>Describe the problem: What were you trying to do?</li> <li>Steps to reproduce: What commands did you run?</li> <li>Expected behavior: What should have happened?</li> <li>Actual behavior: What actually happened?</li> <li>Environment: OS, Python version, Docker version</li> <li>Logs: Include relevant error messages and logs</li> </ol>"},{"location":"testing/troubleshooting/#preventive-measures","title":"Preventive Measures","text":""},{"location":"testing/troubleshooting/#pre-test-checklist","title":"Pre-Test Checklist","text":"<p>Create a checklist to run before testing:</p> <pre><code>#!/bin/bash\n# scripts/pre_test_check.sh\n\necho \"Running pre-test checks...\"\n\n# Check Docker\nif ! docker info &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u274c Docker is not running\"\n    exit 1\nfi\necho \"\u2705 Docker is running\"\n\n# Check containers\nif ! docker compose ps postgres | grep -q \"Up\"; then\n    echo \"\u274c PostgreSQL is not running\"\n    exit 1\nfi\necho \"\u2705 PostgreSQL is running\"\n\nif ! docker compose ps redis | grep -q \"Up\"; then\n    echo \"\u26a0\ufe0f  Redis is not running (optional)\"\nfi\n\n# Check database connection\nif ! docker exec litellm-postgres pg_isready -U litellm_user -d litellm &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u274c Database connection failed\"\n    exit 1\nfi\necho \"\u2705 Database connection successful\"\n\n# Check virtual environment\nif [ -z \"$VIRTUAL_ENV\" ]; then\n    echo \"\u26a0\ufe0f  Virtual environment not activated\"\n    echo \"   Run: source .venv/bin/activate\"\nfi\n\n# Check services\nif ! curl -s http://localhost:8002/health &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  LiteLLM is not running\"\n    echo \"   Run: python scripts/start_local.py\"\nfi\n\nif ! curl -s http://localhost:8003/health &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  SaaS API is not running\"\n    echo \"   Run: python scripts/start_saas_api.py\"\nfi\n\necho \"\"\necho \"\u2705 Pre-test checks complete\"\n</code></pre>"},{"location":"testing/troubleshooting/#regular-maintenance","title":"Regular Maintenance","text":"<pre><code># Clean up old containers and volumes (monthly)\ndocker system prune -af --volumes\n\n# Update dependencies (weekly)\nuv pip list --outdated\n\n# Vacuum database (weekly)\ndocker exec litellm-postgres psql -U litellm_user -d litellm -c \"VACUUM ANALYZE;\"\n\n# Clear test data (after testing)\npython scripts/cleanup_test_data.py\n</code></pre>"},{"location":"testing/troubleshooting/#related-documentation","title":"Related Documentation","text":"<ul> <li>Testing Overview - Testing strategy and philosophy</li> <li>Integration Tests - Running integration tests</li> <li>Getting Started - Initial setup instructions</li> <li>Deployment - Production deployment guide</li> </ul>"}]}